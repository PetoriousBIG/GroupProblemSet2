Title,Abstract,Authors,Year,URL
Lecture Notes: Optimization for Machine Learning,"Lecture notes on optimization for machine learning, derived from a course at
Princeton University and tutorials given in MLSS, Buenos Aires, as well as
Simons Foundation, Berkeley.",Elad Hazan,2019,http://arxiv.org/abs/1909.03550v1
An Optimal Control View of Adversarial Machine Learning,"I describe an optimal control view of adversarial machine learning, where the
dynamical system is the machine learner, the input are adversarial actions, and
the control costs are defined by the adversary's goals to do harm and be hard
to detect. This view encompasses many types of adversarial machine learning,
including test-item attacks, training-data poisoning, and adversarial reward
shaping. The view encourages adversarial machine learning researcher to utilize
advances in control theory and reinforcement learning.",Xiaojin Zhu,2018,http://arxiv.org/abs/1811.04422v1
"Minimax deviation strategies for machine learning and recognition with
  short learning samples","The article is devoted to the problem of small learning samples in machine
learning. The flaws of maximum likelihood learning and minimax learning are
looked into and the concept of minimax deviation learning is introduced that is
free of those flaws.","Michail Schlesinger, Evgeniy Vodolazskiy",2017,http://arxiv.org/abs/1707.04849v1
Machine Learning for Clinical Predictive Analytics,"In this chapter, we provide a brief overview of applying machine learning
techniques for clinical prediction tasks. We begin with a quick introduction to
the concepts of machine learning and outline some of the most common machine
learning algorithms. Next, we demonstrate how to apply the algorithms with
appropriate toolkits to conduct machine learning experiments for clinical
prediction tasks. The objectives of this chapter are to (1) understand the
basics of machine learning techniques and the reasons behind why they are
useful for solving clinical prediction problems, (2) understand the intuition
behind some machine learning models, including regression, decision trees, and
support vector machines, and (3) understand how to apply these models to
clinical prediction problems using publicly available datasets via case
studies.",Wei-Hung Weng,2019,http://arxiv.org/abs/1909.09246v1
"Towards Modular Machine Learning Solution Development: Benefits and
  Trade-offs","Machine learning technologies have demonstrated immense capabilities in
various domains. They play a key role in the success of modern businesses.
However, adoption of machine learning technologies has a lot of untouched
potential. Cost of developing custom machine learning solutions that solve
unique business problems is a major inhibitor to far-reaching adoption of
machine learning technologies. We recognize that the monolithic nature
prevalent in today's machine learning applications stands in the way of
efficient and cost effective customized machine learning solution development.
In this work we explore the benefits of modular machine learning solutions and
discuss how modular machine learning solutions can overcome some of the major
solution engineering limitations of monolithic machine learning solutions. We
analyze the trade-offs between modular and monolithic machine learning
solutions through three deep learning problems; one text based and the two
image based. Our experimental results show that modular machine learning
solutions have a promising potential to reap the solution engineering
advantages of modularity while gaining performance and data advantages in a way
the monolithic machine learning solutions do not permit.","Samiyuru Menik, Lakshmish Ramaswamy",2023,http://arxiv.org/abs/2301.09753v1
The Tribes of Machine Learning and the Realm of Computer Architecture,"Machine learning techniques have influenced the field of computer
architecture like many other fields. This paper studies how the fundamental
machine learning techniques can be applied towards computer architecture
problems. We also provide a detailed survey of computer architecture research
that employs different machine learning methods. Finally, we present some
future opportunities and the outstanding challenges that need to be overcome to
exploit full potential of machine learning for computer architecture.","Ayaz Akram, Jason Lowe-Power",2020,http://arxiv.org/abs/2012.04105v1
"A Machine Learning Tutorial for Operational Meteorology, Part I:
  Traditional Machine Learning","Recently, the use of machine learning in meteorology has increased greatly.
While many machine learning methods are not new, university classes on machine
learning are largely unavailable to meteorology students and are not required
to become a meteorologist. The lack of formal instruction has contributed to
perception that machine learning methods are 'black boxes' and thus end-users
are hesitant to apply the machine learning methods in their every day workflow.
To reduce the opaqueness of machine learning methods and lower hesitancy
towards machine learning in meteorology, this paper provides a survey of some
of the most common machine learning methods. A familiar meteorological example
is used to contextualize the machine learning methods while also discussing
machine learning topics using plain language. The following machine learning
methods are demonstrated: linear regression; logistic regression; decision
trees; random forest; gradient boosted decision trees; naive Bayes; and support
vector machines. Beyond discussing the different methods, the paper also
contains discussions on the general machine learning process as well as best
practices to enable readers to apply machine learning to their own datasets.
Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory
notebooks) used to make the examples in the paper is provided in an effort to
catalyse the use of machine learning in meteorology.","Randy J. Chase, David R. Harrison, Amanda Burke, Gary M. Lackmann, Amy McGovern",2022,http://arxiv.org/abs/2204.07492v2
Position Paper: Towards Transparent Machine Learning,"Transparent machine learning is introduced as an alternative form of machine
learning, where both the model and the learning system are represented in
source code form. The goal of this project is to enable direct human
understanding of machine learning models, giving us the ability to learn,
verify, and refine them as programs. If solved, this technology could represent
a best-case scenario for the safety and security of AI systems going forward.",Dustin Juliano,2019,http://arxiv.org/abs/1911.06612v1
Understanding Bias in Machine Learning,"Bias is known to be an impediment to fair decisions in many domains such as
human resources, the public sector, health care etc. Recently, hope has been
expressed that the use of machine learning methods for taking such decisions
would diminish or even resolve the problem. At the same time, machine learning
experts warn that machine learning models can be biased as well. In this
article, our goal is to explain the issue of bias in machine learning from a
technical perspective and to illustrate the impact that biased data can have on
a machine learning model. To reach such a goal, we develop interactive plots to
visualizing the bias learned from synthetic data.","Jindong Gu, Daniela Oelke",2019,http://arxiv.org/abs/1909.01866v1
"A Unified Analytical Framework for Trustable Machine Learning and
  Automation Running with Blockchain","Traditional machine learning algorithms use data from databases that are
mutable, and therefore the data cannot be fully trusted. Also, the machine
learning process is difficult to automate. This paper proposes building a
trustable machine learning system by using blockchain technology, which can
store data in a permanent and immutable way. In addition, smart contracts are
used to automate the machine learning process. This paper makes three
contributions. First, it establishes a link between machine learning technology
and blockchain technology. Previously, machine learning and blockchain have
been considered two independent technologies without an obvious link. Second,
it proposes a unified analytical framework for trustable machine learning by
using blockchain technology. This unified framework solves both the
trustability and automation issues in machine learning. Third, it enables a
computer to translate core machine learning implementation from a single thread
on a single machine to multiple threads on multiple machines running with
blockchain by using a unified approach. The paper uses association rule mining
as an example to demonstrate how trustable machine learning can be implemented
with blockchain, and it shows how this approach can be used to analyze opioid
prescriptions to help combat the opioid crisis.",Tao Wang,2019,http://arxiv.org/abs/1903.08801v1
"MLBench: How Good Are Machine Learning Clouds for Binary Classification
  Tasks on Structured Data?","We conduct an empirical study of machine learning functionalities provided by
major cloud service providers, which we call machine learning clouds. Machine
learning clouds hold the promise of hiding all the sophistication of running
large-scale machine learning: Instead of specifying how to run a machine
learning task, users only specify what machine learning task to run and the
cloud figures out the rest. Raising the level of abstraction, however, rarely
comes free - a performance penalty is possible. How good, then, are current
machine learning clouds on real-world machine learning workloads?
  We study this question with a focus on binary classication problems. We
present mlbench, a novel benchmark constructed by harvesting datasets from
Kaggle competitions. We then compare the performance of the top winning code
available from Kaggle with that of running machine learning clouds from both
Azure and Amazon on mlbench. Our comparative study reveals the strength and
weakness of existing machine learning clouds and points out potential future
directions for improvement.","Yu Liu, Hantian Zhang, Luyuan Zeng, Wentao Wu, Ce Zhang",2017,http://arxiv.org/abs/1707.09562v3
Data Pricing in Machine Learning Pipelines,"Machine learning is disruptive. At the same time, machine learning can only
succeed by collaboration among many parties in multiple steps naturally as
pipelines in an eco-system, such as collecting data for possible machine
learning applications, collaboratively training models by multiple parties and
delivering machine learning services to end users. Data is critical and
penetrating in the whole machine learning pipelines. As machine learning
pipelines involve many parties and, in order to be successful, have to form a
constructive and dynamic eco-system, marketplaces and data pricing are
fundamental in connecting and facilitating those many parties. In this article,
we survey the principles and the latest research development of data pricing in
machine learning pipelines. We start with a brief review of data marketplaces
and pricing desiderata. Then, we focus on pricing in three important steps in
machine learning pipelines. To understand pricing in the step of training data
collection, we review pricing raw data sets and data labels. We also
investigate pricing in the step of collaborative training of machine learning
models, and overview pricing machine learning models for end users in the step
of machine learning deployment. We also discuss a series of possible future
directions.","Zicun Cong, Xuan Luo, Pei Jian, Feida Zhu, Yong Zhang",2021,http://arxiv.org/abs/2108.07915v1
Techniques for Automated Machine Learning,"Automated machine learning (AutoML) aims to find optimal machine learning
solutions automatically given a machine learning problem. It could release the
burden of data scientists from the multifarious manual tuning process and
enable the access of domain experts to the off-the-shelf machine learning
solutions without extensive experience. In this paper, we review the current
developments of AutoML in terms of three categories, automated feature
engineering (AutoFE), automated model and hyperparameter learning (AutoMHL),
and automated deep learning (AutoDL). State-of-the-art techniques adopted in
the three categories are presented, including Bayesian optimization,
reinforcement learning, evolutionary algorithm, and gradient-based approaches.
We summarize popular AutoML frameworks and conclude with current open
challenges of AutoML.","Yi-Wei Chen, Qingquan Song, Xia Hu",2019,http://arxiv.org/abs/1907.08908v1
"The Landscape of Modern Machine Learning: A Review of Machine,
  Distributed and Federated Learning","With the advance of the powerful heterogeneous, parallel and distributed
computing systems and ever increasing immense amount of data, machine learning
has become an indispensable part of cutting-edge technology, scientific
research and consumer products. In this study, we present a review of modern
machine and deep learning. We provide a high-level overview for the latest
advanced machine learning algorithms, applications, and frameworks. Our
discussion encompasses parallel distributed learning, deep learning as well as
federated learning. As a result, our work serves as an introductory text to the
vast field of modern machine learning.","Omer Subasi, Oceane Bel, Joseph Manzano, Kevin Barker",2023,http://arxiv.org/abs/2312.03120v1
"Parallelization of Machine Learning Algorithms Respectively on Single
  Machine and Spark","With the rapid development of big data technologies, how to dig out useful
information from massive data becomes an essential problem. However, using
machine learning algorithms to analyze large data may be time-consuming and
inefficient on the traditional single machine. To solve these problems, this
paper has made some research on the parallelization of several classic machine
learning algorithms respectively on the single machine and the big data
platform Spark. We compare the runtime and efficiency of traditional machine
learning algorithms with parallelized machine learning algorithms respectively
on the single machine and Spark platform. The research results have shown
significant improvement in runtime and efficiency of parallelized machine
learning algorithms.",Jiajun Shen,2022,http://arxiv.org/abs/2206.07090v2
AutoCompete: A Framework for Machine Learning Competition,"In this paper, we propose AutoCompete, a highly automated machine learning
framework for tackling machine learning competitions. This framework has been
learned by us, validated and improved over a period of more than two years by
participating in online machine learning competitions. It aims at minimizing
human interference required to build a first useful predictive model and to
assess the practical difficulty of a given machine learning challenge. The
proposed system helps in identifying data types, choosing a machine learn- ing
model, tuning hyper-parameters, avoiding over-fitting and optimization for a
provided evaluation metric. We also observe that the proposed system produces
better (or comparable) results with less runtime as compared to other
approaches.","Abhishek Thakur, Artus Krohn-Grimberghe",2015,http://arxiv.org/abs/1507.02188v1
"Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in
  Social Good Applications","This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning
in Social Good Applications, which was held on June 24, 2016 in New York.",Kush R. Varshney,2016,http://arxiv.org/abs/1607.02450v2
Mathematical Perspective of Machine Learning,"We take a closer look at some theoretical challenges of Machine Learning as a
function approximation, gradient descent as the default optimization algorithm,
limitations of fixed length and width networks and a different approach to RNNs
from a mathematical perspective.",Yarema Boryshchak,2020,http://arxiv.org/abs/2007.01503v1
Private Machine Learning via Randomised Response,"We introduce a general learning framework for private machine learning based
on randomised response. Our assumption is that all actors are potentially
adversarial and as such we trust only to release a single noisy version of an
individual's datapoint. We discuss a general approach that forms a consistent
way to estimate the true underlying machine learning model and demonstrate this
in the case of logistic regression.",David Barber,2020,http://arxiv.org/abs/2001.04942v2
A Survey of Optimization Methods from a Machine Learning Perspective,"Machine learning develops rapidly, which has made many theoretical
breakthroughs and is widely applied in various fields. Optimization, as an
important part of machine learning, has attracted much attention of
researchers. With the exponential growth of data amount and the increase of
model complexity, optimization methods in machine learning face more and more
challenges. A lot of work on solving optimization problems or improving
optimization methods in machine learning has been proposed successively. The
systematic retrospect and summary of the optimization methods from the
perspective of machine learning are of great significance, which can offer
guidance for both developments of optimization and machine learning research.
In this paper, we first describe the optimization problems in machine learning.
Then, we introduce the principles and progresses of commonly used optimization
methods. Next, we summarize the applications and developments of optimization
methods in some popular machine learning fields. Finally, we explore and give
some challenges and open problems for the optimization in machine learning.","Shiliang Sun, Zehui Cao, Han Zhu, Jing Zhao",2019,http://arxiv.org/abs/1906.06821v2
Ten-year Survival Prediction for Breast Cancer Patients,"This report assesses different machine learning approaches to 10-year
survival prediction of breast cancer patients.","Changmao Li, Han He, Yunze Hao, Caleb Ziems",2019,http://arxiv.org/abs/1911.00776v1
When Machine Learning Meets Privacy: A Survey and Outlook,"The newly emerged machine learning (e.g. deep learning) methods have become a
strong driving force to revolutionize a wide range of industries, such as smart
healthcare, financial technology, and surveillance systems. Meanwhile, privacy
has emerged as a big concern in this machine learning-based artificial
intelligence era. It is important to note that the problem of privacy
preservation in the context of machine learning is quite different from that in
traditional data privacy protection, as machine learning can act as both friend
and foe. Currently, the work on the preservation of privacy and machine
learning (ML) is still in an infancy stage, as most existing solutions only
focus on privacy problems during the machine learning process. Therefore, a
comprehensive study on the privacy preservation problems and machine learning
is required. This paper surveys the state of the art in privacy issues and
solutions for machine learning. The survey covers three categories of
interactions between privacy and machine learning: (i) private machine
learning, (ii) machine learning aided privacy protection, and (iii) machine
learning-based privacy attack and corresponding protection schemes. The current
research progress in each category is reviewed and the key challenges are
identified. Finally, based on our in-depth analysis of the area of privacy and
machine learning, we point out future research directions in this field.","Bo Liu, Ming Ding, Sina Shaham, Wenny Rahayu, Farhad Farokhi, Zihuai Lin",2020,http://arxiv.org/abs/2011.11819v1
Augmented Q Imitation Learning (AQIL),"The study of unsupervised learning can be generally divided into two
categories: imitation learning and reinforcement learning. In imitation
learning the machine learns by mimicking the behavior of an expert system
whereas in reinforcement learning the machine learns via direct environment
feedback. Traditional deep reinforcement learning takes a significant time
before the machine starts to converge to an optimal policy. This paper proposes
Augmented Q-Imitation-Learning, a method by which deep reinforcement learning
convergence can be accelerated by applying Q-imitation-learning as the initial
training process in traditional Deep Q-learning.","Xiao Lei Zhang, Anish Agarwal",2020,http://arxiv.org/abs/2004.00993v2
Probabilistic Machine Learning for Healthcare,"Machine learning can be used to make sense of healthcare data. Probabilistic
machine learning models help provide a complete picture of observed data in
healthcare. In this review, we examine how probabilistic machine learning can
advance healthcare. We consider challenges in the predictive model building
pipeline where probabilistic models can be beneficial including calibration and
missing data. Beyond predictive models, we also investigate the utility of
probabilistic machine learning models in phenotyping, in generative models for
clinical use cases, and in reinforcement learning.","Irene Y. Chen, Shalmali Joshi, Marzyeh Ghassemi, Rajesh Ranganath",2020,http://arxiv.org/abs/2009.11087v1
Evaluation Challenges for Geospatial ML,"As geospatial machine learning models and maps derived from their predictions
are increasingly used for downstream analyses in science and policy, it is
imperative to evaluate their accuracy and applicability. Geospatial machine
learning has key distinctions from other learning paradigms, and as such, the
correct way to measure performance of spatial machine learning outputs has been
a topic of debate. In this paper, I delineate unique challenges of model
evaluation for geospatial machine learning with global or remotely sensed
datasets, culminating in concrete takeaways to improve evaluations of
geospatial model performance.",Esther Rolf,2023,http://arxiv.org/abs/2303.18087v1
"A comprehensive review of Quantum Machine Learning: from NISQ to Fault
  Tolerance","Quantum machine learning, which involves running machine learning algorithms
on quantum devices, has garnered significant attention in both academic and
business circles. In this paper, we offer a comprehensive and unbiased review
of the various concepts that have emerged in the field of quantum machine
learning. This includes techniques used in Noisy Intermediate-Scale Quantum
(NISQ) technologies and approaches for algorithms compatible with
fault-tolerant quantum computing hardware. Our review covers fundamental
concepts, algorithms, and the statistical learning theory pertinent to quantum
machine learning.","Yunfei Wang, Junyu Liu",2024,http://arxiv.org/abs/2401.11351v2
"Towards CRISP-ML(Q): A Machine Learning Process Model with Quality
  Assurance Methodology","Machine learning is an established and frequently used technique in industry
and academia but a standard process model to improve success and efficiency of
machine learning applications is still missing. Project organizations and
machine learning practitioners have a need for guidance throughout the life
cycle of a machine learning application to meet business expectations. We
therefore propose a process model for the development of machine learning
applications, that covers six phases from defining the scope to maintaining the
deployed machine learning application. The first phase combines business and
data understanding as data availability oftentimes affects the feasibility of
the project. The sixth phase covers state-of-the-art approaches for monitoring
and maintenance of a machine learning applications, as the risk of model
degradation in a changing environment is eminent. With each task of the
process, we propose quality assurance methodology that is suitable to adress
challenges in machine learning development that we identify in form of risks.
The methodology is drawn from practical experience and scientific literature
and has proven to be general and stable. The process model expands on CRISP-DM,
a data mining process model that enjoys strong industry support but lacks to
address machine learning specific tasks. Our work proposes an industry and
application neutral process model tailored for machine learning applications
with focus on technical tasks for quality assurance.","Stefan Studer, Thanh Binh Bui, Christian Drescher, Alexander Hanuschkin, Ludwig Winkler, Steven Peters, Klaus-Robert Mueller",2020,http://arxiv.org/abs/2003.05155v2
"Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of
  learning relational order via reinforcement learning procedure?","In this article, we extend the conventional framework of
convolutional-Restricted-Boltzmann-Machine to learn highly abstract features
among abitrary number of time related input maps by constructing a layer of
multiplicative units, which capture the relations among inputs. In many cases,
more than two maps are strongly related, so it is wise to make multiplicative
unit learn relations among more input maps, in other words, to find the optimal
relational-order of each unit. In order to enable our machine to learn
relational order, we developed a reinforcement-learning method whose optimality
is proven to train the network.",Zizhuang Wang,2017,http://arxiv.org/abs/1706.08001v1
Spatial Transfer Learning with Simple MLP,"First step to investigate the potential of transfer learning applied to the
field of spatial statistics",Hongjian Yang,2024,http://arxiv.org/abs/2405.03720v1
Distributed Multi-Task Learning with Shared Representation,"We study the problem of distributed multi-task learning with shared
representation, where each machine aims to learn a separate, but related, task
in an unknown shared low-dimensional subspaces, i.e. when the predictor matrix
has low rank. We consider a setting where each task is handled by a different
machine, with samples for the task available locally on the machine, and study
communication-efficient methods for exploiting the shared structure.","Jialei Wang, Mladen Kolar, Nathan Srebro",2016,http://arxiv.org/abs/1603.02185v1
Components of Machine Learning: Binding Bits and FLOPS,"Many machine learning problems and methods are combinations of three
components: data, hypothesis space and loss function. Different machine
learning methods are obtained as combinations of different choices for the
representation of data, hypothesis space and loss function. After reviewing the
mathematical structure of these three components, we discuss intrinsic
trade-offs between statistical and computational properties of machine learning
methods.",Alexander Jung,2019,http://arxiv.org/abs/1910.12387v2
Impact of Legal Requirements on Explainability in Machine Learning,"The requirements on explainability imposed by European laws and their
implications for machine learning (ML) models are not always clear. In that
perspective, our research analyzes explanation obligations imposed for private
and public decision-making, and how they can be implemented by machine learning
techniques.","Adrien Bibal, Michael Lognoul, Alexandre de Streel, Benoît Frénay",2020,http://arxiv.org/abs/2007.05479v1
Machine Learning Potential Repository,"This paper introduces a machine learning potential repository that includes
Pareto optimal machine learning potentials. It also shows the systematic
development of accurate and fast machine learning potentials for a wide range
of elemental systems. As a result, many Pareto optimal machine learning
potentials are available in the repository from a website. Therefore, the
repository will help many scientists to perform accurate and fast atomistic
simulations.",Atsuto Seko,2020,http://arxiv.org/abs/2007.14206v1
Quantum memristors for neuromorphic quantum machine learning,"Quantum machine learning may permit to realize more efficient machine
learning calculations with near-term quantum devices. Among the diverse quantum
machine learning paradigms which are currently being considered, quantum
memristors are promising as a way of combining, in the same quantum hardware, a
unitary evolution with the nonlinearity provided by the measurement and
feedforward. Thus, an efficient way of deploying neuromorphic quantum computing
for quantum machine learning may be enabled.",Lucas Lamata,2024,http://arxiv.org/abs/2412.18979v1
metric-learn: Metric Learning Algorithms in Python,"metric-learn is an open source Python package implementing supervised and
weakly-supervised distance metric learning algorithms. As part of
scikit-learn-contrib, it provides a unified interface compatible with
scikit-learn which allows to easily perform cross-validation, model selection,
and pipelining with other machine learning estimators. metric-learn is
thoroughly tested and available on PyPi under the MIT licence.","William de Vazelhes, CJ Carey, Yuan Tang, Nathalie Vauquier, Aurélien Bellet",2019,http://arxiv.org/abs/1908.04710v3
Theoretical Models of Learning to Learn,"A Machine can only learn if it is biased in some way. Typically the bias is
supplied by hand, for example through the choice of an appropriate set of
features. However, if the learning machine is embedded within an {\em
environment} of related tasks, then it can {\em learn} its own bias by learning
sufficiently many tasks from the environment. In this paper two models of bias
learning (or equivalently, learning to learn) are introduced and the main
theoretical results presented. The first model is a PAC-type model based on
empirical process theory, while the second is a hierarchical Bayes model.",Jonathan Baxter,2020,http://arxiv.org/abs/2002.12364v1
On-the-Fly Learning in a Perpetual Learning Machine,"Despite the promise of brain-inspired machine learning, deep neural networks
(DNN) have frustratingly failed to bridge the deceptively large gap between
learning and memory. Here, we introduce a Perpetual Learning Machine; a new
type of DNN that is capable of brain-like dynamic 'on the fly' learning because
it exists in a self-supervised state of Perpetual Stochastic Gradient Descent.
Thus, we provide the means to unify learning and memory within a machine
learning framework. We also explore the elegant duality of abstraction and
synthesis: the Yin and Yang of deep learning.",Andrew J. R. Simpson,2015,http://arxiv.org/abs/1509.00913v3
"An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality
  in Machine Learning","We propose a clustering-based iterative algorithm to solve certain
optimization problems in machine learning, where we start the algorithm by
aggregating the original data, solving the problem on aggregated data, and then
in subsequent steps gradually disaggregate the aggregated data. We apply the
algorithm to common machine learning problems such as the least absolute
deviation regression problem, support vector machines, and semi-supervised
support vector machines. We derive model-specific data aggregation and
disaggregation procedures. We also show optimality, convergence, and the
optimality gap of the approximated solution in each iteration. A computational
study is provided.","Young Woong Park, Diego Klabjan",2016,http://arxiv.org/abs/1607.01400v1
Human-in-the-loop Machine Learning: A Macro-Micro Perspective,"Though technical advance of artificial intelligence and machine learning has
enabled many promising intelligent systems, many computing tasks are still not
able to be fully accomplished by machine intelligence. Motivated by the
complementary nature of human and machine intelligence, an emerging trend is to
involve humans in the loop of machine learning and decision-making. In this
paper, we provide a macro-micro review of human-in-the-loop machine learning.
We first describe major machine learning challenges which can be addressed by
human intervention in the loop. Then we examine closely the latest research and
findings of introducing humans into each step of the lifecycle of machine
learning. Finally, we analyze current research gaps and point out future
research directions.","Jiangtao Wang, Bin Guo, Liming Chen",2022,http://arxiv.org/abs/2202.10564v1
Can Machines Learn the True Probabilities?,"When there exists uncertainty, AI machines are designed to make decisions so
as to reach the best expected outcomes. Expectations are based on true facts
about the objective environment the machines interact with, and those facts can
be encoded into AI models in the form of true objective probability functions.
Accordingly, AI models involve probabilistic machine learning in which the
probabilities should be objectively interpreted. We prove under some basic
assumptions when machines can learn the true objective probabilities, if any,
and when machines cannot learn them.",Jinsook Kim,2024,http://arxiv.org/abs/2407.05526v1
Scientific Machine Learning Benchmarks,"The breakthrough in Deep Learning neural networks has transformed the use of
AI and machine learning technologies for the analysis of very large
experimental datasets. These datasets are typically generated by large-scale
experimental facilities at national laboratories. In the context of science,
scientific machine learning focuses on training machines to identify patterns,
trends, and anomalies to extract meaningful scientific insights from such
datasets. With a new generation of experimental facilities, the rate of data
generation and the scale of data volumes will increasingly require the use of
more automated data analysis. At present, identifying the most appropriate
machine learning algorithm for the analysis of any given scientific dataset is
still a challenge for scientists. This is due to many different machine
learning frameworks, computer architectures, and machine learning models.
Historically, for modelling and simulation on HPC systems such problems have
been addressed through benchmarking computer applications, algorithms, and
architectures. Extending such a benchmarking approach and identifying metrics
for the application of machine learning methods to scientific datasets is a new
challenge for both scientists and computer scientists. In this paper, we
describe our approach to the development of scientific machine learning
benchmarks and review other approaches to benchmarking scientific machine
learning.","Jeyan Thiyagalingam, Mallikarjun Shankar, Geoffrey Fox, Tony Hey",2021,http://arxiv.org/abs/2110.12773v1
Some Insights into Lifelong Reinforcement Learning Systems,"A lifelong reinforcement learning system is a learning system that has the
ability to learn through trail-and-error interaction with the environment over
its lifetime. In this paper, I give some arguments to show that the traditional
reinforcement learning paradigm fails to model this type of learning system.
Some insights into lifelong reinforcement learning are provided, along with a
simplistic prototype lifelong reinforcement learning system.",Changjian Li,2020,http://arxiv.org/abs/2001.09608v1
Bayesian Optimization for Machine Learning : A Practical Guidebook,"The engineering of machine learning systems is still a nascent field; relying
on a seemingly daunting collection of quickly evolving tools and best
practices. It is our hope that this guidebook will serve as a useful resource
for machine learning practitioners looking to take advantage of Bayesian
optimization techniques. We outline four example machine learning problems that
can be solved using open source machine learning libraries, and highlight the
benefits of using Bayesian optimization in the context of these common machine
learning applications.","Ian Dewancker, Michael McCourt, Scott Clark",2016,http://arxiv.org/abs/1612.04858v1
Towards A Rigorous Science of Interpretable Machine Learning,"As machine learning systems become ubiquitous, there has been a surge of
interest in interpretable machine learning: systems that provide explanation
for their outputs. These explanations are often used to qualitatively assess
other criteria such as safety or non-discrimination. However, despite the
interest in interpretability, there is very little consensus on what
interpretable machine learning is and how it should be measured. In this
position paper, we first define interpretability and describe when
interpretability is needed (and when it is not). Next, we suggest a taxonomy
for rigorous evaluation and expose open questions towards a more rigorous
science of interpretable machine learning.","Finale Doshi-Velez, Been Kim",2017,http://arxiv.org/abs/1702.08608v2
Infrastructure for Usable Machine Learning: The Stanford DAWN Project,"Despite incredible recent advances in machine learning, building machine
learning applications remains prohibitively time-consuming and expensive for
all but the best-trained, best-funded engineering organizations. This expense
comes not from a need for new and improved statistical models but instead from
a lack of systems and tools for supporting end-to-end machine learning
application development, from data preparation and labeling to
productionization and monitoring. In this document, we outline opportunities
for infrastructure supporting usable, end-to-end machine learning applications
in the context of the nascent DAWN (Data Analytics for What's Next) project at
Stanford.","Peter Bailis, Kunle Olukotun, Christopher Re, Matei Zaharia",2017,http://arxiv.org/abs/1705.07538v2
Techniques for Interpretable Machine Learning,"Interpretable machine learning tackles the important problem that humans
cannot understand the behaviors of complex machine learning models and how
these models arrive at a particular decision. Although many approaches have
been proposed, a comprehensive understanding of the achievements and challenges
is still lacking. We provide a survey covering existing techniques to increase
the interpretability of machine learning models. We also discuss crucial issues
that the community should consider in future work such as designing
user-friendly explanations and developing comprehensive evaluation metrics to
further push forward the area of interpretable machine learning.","Mengnan Du, Ninghao Liu, Xia Hu",2018,http://arxiv.org/abs/1808.00033v3
Solving machine learning optimization problems using quantum computers,"Classical optimization algorithms in machine learning often take a long time
to compute when applied to a multi-dimensional problem and require a huge
amount of CPU and GPU resource. Quantum parallelism has a potential to speed up
machine learning algorithms. We describe a generic mathematical model to
leverage quantum parallelism to speed-up machine learning algorithms. We also
apply quantum machine learning and quantum parallelism applied to a
$3$-dimensional image that vary with time.","Venkat R. Dasari, Mee Seong Im, Lubjana Beshaj",2019,http://arxiv.org/abs/1911.08587v1
Lale: Consistent Automated Machine Learning,"Automated machine learning makes it easier for data scientists to develop
pipelines by searching over possible choices for hyperparameters, algorithms,
and even pipeline topologies. Unfortunately, the syntax for automated machine
learning tools is inconsistent with manual machine learning, with each other,
and with error checks. Furthermore, few tools support advanced features such as
topology search or higher-order operators. This paper introduces Lale, a
library of high-level Python interfaces that simplifies and unifies automated
machine learning in a consistent way.","Guillaume Baudart, Martin Hirzel, Kiran Kate, Parikshit Ram, Avraham Shinnar",2020,http://arxiv.org/abs/2007.01977v1
Differential Replication in Machine Learning,"When deployed in the wild, machine learning models are usually confronted
with data and requirements that constantly vary, either because of changes in
the generating distribution or because external constraints change the
environment where the model operates. To survive in such an ecosystem, machine
learning models need to adapt to new conditions by evolving over time. The idea
of model adaptability has been studied from different perspectives. In this
paper, we propose a solution based on reusing the knowledge acquired by the
already deployed machine learning models and leveraging it to train future
generations. This is the idea behind differential replication of machine
learning models.","Irene Unceta, Jordi Nin, Oriol Pujol",2020,http://arxiv.org/abs/2007.07981v1
mlr3proba: An R Package for Machine Learning in Survival Analysis,"As machine learning has become increasingly popular over the last few
decades, so too has the number of machine learning interfaces for implementing
these models. Whilst many R libraries exist for machine learning, very few
offer extended support for survival analysis. This is problematic considering
its importance in fields like medicine, bioinformatics, economics, engineering,
and more. mlr3proba provides a comprehensive machine learning interface for
survival analysis and connects with mlr3's general model tuning and
benchmarking facilities to provide a systematic infrastructure for survival
modeling and evaluation.","Raphael Sonabend, Franz J. Király, Andreas Bender, Bernd Bischl, Michel Lang",2020,http://arxiv.org/abs/2008.08080v2
"Teaching Uncertainty Quantification in Machine Learning through Use
  Cases","Uncertainty in machine learning is not generally taught as general knowledge
in Machine Learning course curricula. In this paper we propose a short
curriculum for a course about uncertainty in machine learning, and complement
the course with a selection of use cases, aimed to trigger discussion and let
students play with the concepts of uncertainty in a programming setting. Our
use cases cover the concept of output uncertainty, Bayesian neural networks and
weight distributions, sources of uncertainty, and out of distribution
detection. We expect that this curriculum and set of use cases motivates the
community to adopt these important concepts into courses for safety in AI.",Matias Valdenegro-Toro,2021,http://arxiv.org/abs/2108.08712v1
"Introduction to Machine Learning for Physicians: A Survival Guide for
  Data Deluge","Many modern research fields increasingly rely on collecting and analysing
massive, often unstructured, and unwieldy datasets. Consequently, there is
growing interest in machine learning and artificial intelligence applications
that can harness this `data deluge'. This broad nontechnical overview provides
a gentle introduction to machine learning with a specific focus on medical and
biological applications. We explain the common types of machine learning
algorithms and typical tasks that can be solved, illustrating the basics with
concrete examples from healthcare. Lastly, we provide an outlook on open
challenges, limitations, and potential impacts of machine-learning-powered
medicine.","Ričards Marcinkevičs, Ece Ozkan, Julia E. Vogt",2022,http://arxiv.org/abs/2212.12303v1
"Machine learning-assisted close-set X-ray diffraction phase
  identification of transition metals","Machine learning has been applied to the problem of X-ray diffraction phase
prediction with promising results. In this paper, we describe a method for
using machine learning to predict crystal structure phases from X-ray
diffraction data of transition metals and their oxides. We evaluate the
performance of our method and compare the variety of its settings. Our results
demonstrate that the proposed machine learning framework achieves competitive
performance. This demonstrates the potential for machine learning to
significantly impact the field of X-ray diffraction and crystal structure
determination. Open-source implementation:
https://github.com/maxnygma/NeuralXRD.","Maksim Zhdanov, Andrey Zhdanov",2023,http://arxiv.org/abs/2305.15410v1
Insights From Insurance for Fair Machine Learning,"We argue that insurance can act as an analogon for the social situatedness of
machine learning systems, hence allowing machine learning scholars to take
insights from the rich and interdisciplinary insurance literature. Tracing the
interaction of uncertainty, fairness and responsibility in insurance provides a
fresh perspective on fairness in machine learning. We link insurance fairness
conceptions to their machine learning relatives, and use this bridge to
problematize fairness as calibration. In this process, we bring to the
forefront two themes that have been largely overlooked in the machine learning
literature: responsibility and aggregate-individual tensions.","Christian Fröhlich, Robert C. Williamson",2023,http://arxiv.org/abs/2306.14624v2
Quantum Dynamics of Machine Learning,"The quantum dynamic equation (QDE) of machine learning is obtained based on
Schr\""odinger equation and potential energy equivalence relationship. Through
Wick rotation, the relationship between quantum dynamics and thermodynamics is
also established in this paper. This equation reformulates the iterative
process of machine learning into a time-dependent partial differential equation
with a clear mathematical structure, offering a theoretical framework for
investigating machine learning iterations through quantum and mathematical
theories. Within this framework, the fundamental iterative process, the
diffusion model, and the Softmax and Sigmoid functions are examined, validating
the proposed quantum dynamics equations. This approach not only presents a
rigorous theoretical foundation for machine learning but also holds promise for
supporting the implementation of machine learning algorithms on quantum
computers.","Peng Wang, Maimaitiniyazi Maimaitiabudula",2024,http://arxiv.org/abs/2407.19890v1
"On the Conditions for Domain Stability for Machine Learning: a
  Mathematical Approach","This work proposes a mathematical approach that (re)defines a property of
Machine Learning models named stability and determines sufficient conditions to
validate it. Machine Learning models are represented as functions, and the
characteristics in scope depend upon the domain of the function, what allows us
to adopt topological and metric spaces theory as a basis. Finally, this work
provides some equivalences useful to prove and test stability in Machine
Learning models. The results suggest that whenever stability is aligned with
the notion of function smoothness, then the stability of Machine Learning
models primarily depends upon certain topological, measurable properties of the
classification sets within the ML model domain.",Gabriel Pedroza,2024,http://arxiv.org/abs/2412.00464v1
Distributed Multitask Learning,"We consider the problem of distributed multi-task learning, where each
machine learns a separate, but related, task. Specifically, each machine learns
a linear predictor in high-dimensional space,where all tasks share the same
small support. We present a communication-efficient estimator based on the
debiased lasso and show that it is comparable with the optimal centralized
method.","Jialei Wang, Mladen Kolar, Nathan Srebro",2015,http://arxiv.org/abs/1510.00633v1
Distributed Stochastic Multi-Task Learning with Graph Regularization,"We propose methods for distributed graph-based multi-task learning that are
based on weighted averaging of messages from other machines. Uniform averaging
or diminishing stepsize in these methods would yield consensus (single task)
learning. We show how simply skewing the averaging weights or controlling the
stepsize allows learning different, but related, tasks on the different
machines.","Weiran Wang, Jialei Wang, Mladen Kolar, Nathan Srebro",2018,http://arxiv.org/abs/1802.03830v1
Category Theory in Machine Learning,"Over the past two decades machine learning has permeated almost every realm
of technology. At the same time, many researchers have begun using category
theory as a unifying language, facilitating communication between different
scientific disciplines. It is therefore unsurprising that there is a burgeoning
interest in applying category theory to machine learning. We aim to document
the motivations, goals and common themes across these applications. We touch on
gradient-based learning, probability, and equivariant learning.","Dan Shiebler, Bruno Gavranović, Paul Wilson",2021,http://arxiv.org/abs/2106.07032v1
Energy-Harvesting Distributed Machine Learning,"This paper provides a first study of utilizing energy harvesting for
sustainable machine learning in distributed networks. We consider a distributed
learning setup in which a machine learning model is trained over a large number
of devices that can harvest energy from the ambient environment, and develop a
practical learning framework with theoretical convergence guarantees. We
demonstrate through numerical experiments that the proposed framework can
significantly outperform energy-agnostic benchmarks. Our framework is scalable,
requires only local estimation of the energy statistics, and can be applied to
a wide range of distributed training settings, including machine learning in
wireless networks, edge computing, and mobile internet of things.","Basak Guler, Aylin Yener",2021,http://arxiv.org/abs/2102.05639v1
Representation Learning for Electronic Health Records,"Information in electronic health records (EHR), such as clinical narratives,
examination reports, lab measurements, demographics, and other patient
encounter entries, can be transformed into appropriate data representations
that can be used for downstream clinical machine learning tasks using
representation learning. Learning better representations is critical to improve
the performance of downstream tasks. Due to the advances in machine learning,
we now can learn better and meaningful representations from EHR through
disentangling the underlying factors inside data and distilling large amounts
of information and knowledge from heterogeneous EHR sources. In this chapter,
we first introduce the background of learning representations and reasons why
we need good EHR representations in machine learning for medicine and
healthcare in Section 1. Next, we explain the commonly-used machine learning
and evaluation methods for representation learning using a deep learning
approach in Section 2. Following that, we review recent related studies of
learning patient state representation from EHR for clinical machine learning
tasks in Section 3. Finally, in Section 4 we discuss more techniques, studies,
and challenges for learning natural language representations when free texts,
such as clinical notes, examination reports, or biomedical literature are used.
We also discuss challenges and opportunities in these rapidly growing research
fields.","Wei-Hung Weng, Peter Szolovits",2019,http://arxiv.org/abs/1909.09248v1
Meta-Learning: A Survey,"Meta-learning, or learning to learn, is the science of systematically
observing how different machine learning approaches perform on a wide range of
learning tasks, and then learning from this experience, or meta-data, to learn
new tasks much faster than otherwise possible. Not only does this dramatically
speed up and improve the design of machine learning pipelines or neural
architectures, it also allows us to replace hand-engineered algorithms with
novel approaches learned in a data-driven way. In this chapter, we provide an
overview of the state of the art in this fascinating and continuously evolving
field.",Joaquin Vanschoren,2018,http://arxiv.org/abs/1810.03548v1
Introduction to intelligent computing unit 1,"This brief note highlights some basic concepts required toward understanding
the evolution of machine learning and deep learning models. The note starts
with an overview of artificial intelligence and its relationship to biological
neuron that ultimately led to the evolution of todays intelligent models.",Isa Inuwa-Dutse,2017,http://arxiv.org/abs/1711.06552v1
"In-Machine-Learning Database: Reimagining Deep Learning with Old-School
  SQL","In-database machine learning has been very popular, almost being a cliche.
However, can we do it the other way around? In this work, we say ""yes"" by
applying plain old SQL to deep learning, in a sense implementing deep learning
algorithms with SQL. Most deep learning frameworks, as well as generic machine
learning ones, share a de facto standard of multidimensional array operations,
underneath fancier infrastructure such as automatic differentiation. As SQL
tables can be regarded as generalisations of (multi-dimensional) arrays, we
have found a way to express common deep learning operations in SQL, encouraging
a different way of thinking and thus potentially novel models. In particular,
one of the latest trend in deep learning was the introduction of sparsity in
the name of graph convolutional networks, whereas we take sparsity almost for
granted in the database world. As both databases and machine learning involve
transformation of datasets, we hope this work can inspire further works
utilizing the large body of existing wisdom, algorithms and technologies in the
database field to advance the state of the art in machine learning, rather than
merely integerating machine learning into databases.",Len Du,2020,http://arxiv.org/abs/2004.05366v2
Machine Learning Interpretability: A Science rather than a tool,"The term ""interpretability"" is oftenly used by machine learning researchers
each with their own intuitive understanding of it. There is no universal well
agreed upon definition of interpretability in machine learning. As any type of
science discipline is mainly driven by the set of formulated questions rather
than by different tools in that discipline, e.g. astrophysics is the discipline
that learns the composition of stars, not as the discipline that use the
spectroscopes. Similarly, we propose that machine learning interpretability
should be a discipline that answers specific questions related to
interpretability. These questions can be of statistical, causal and
counterfactual nature. Therefore, there is a need to look into the
interpretability problem of machine learning in the context of questions that
need to be addressed rather than different tools. We discuss about a
hypothetical interpretability framework driven by a question based scientific
approach rather than some specific machine learning model. Using a question
based notion of interpretability, we can step towards understanding the science
of machine learning rather than its engineering. This notion will also help us
understanding any specific problem more in depth rather than relying solely on
machine learning methods.","Abdul Karim, Avinash Mishra, MA Hakim Newton, Abdul Sattar",2018,http://arxiv.org/abs/1807.06722v2
Automated Machine Learning on Graphs: A Survey,"Machine learning on graphs has been extensively studied in both academic and
industry. However, as the literature on graph learning booms with a vast number
of emerging methods and techniques, it becomes increasingly difficult to
manually design the optimal machine learning algorithm for different
graph-related tasks. To solve this critical challenge, automated machine
learning (AutoML) on graphs which combines the strength of graph machine
learning and AutoML together, is gaining attention from the research community.
Therefore, we comprehensively survey AutoML on graphs in this paper, primarily
focusing on hyper-parameter optimization (HPO) and neural architecture search
(NAS) for graph machine learning. We further overview libraries related to
automated graph machine learning and in-depth discuss AutoGL, the first
dedicated open-source library for AutoML on graphs. In the end, we share our
insights on future research directions for automated graph machine learning.
This paper is the first systematic and comprehensive review of automated
machine learning on graphs to the best of our knowledge.","Ziwei Zhang, Xin Wang, Wenwu Zhu",2021,http://arxiv.org/abs/2103.00742v4
Can Machine Learning be Moral?,"The ethics of Machine Learning has become an unavoidable topic in the AI
Community. The deployment of machine learning systems in multiple social
contexts has resulted in a closer ethical scrutiny of the design, development,
and application of these systems. The AI/ML community has come to terms with
the imperative to think about the ethical implications of machine learning, not
only as a product but also as a practice (Birhane, 2021; Shen et al. 2021). The
critical question that is troubling many debates is what can constitute an
ethically accountable machine learning system. In this paper we explore
possibilities for ethical evaluation of machine learning methodologies. We
scrutinize techniques, methods and technical practices in machine learning from
a relational ethics perspective, taking into consideration how machine learning
systems are part of the world and how they relate to different forms of agency.
Taking a page from Phil Agre (1997) we use the notion of a critical technical
practice as a means of analysis of machine learning approaches. Our radical
proposal is that supervised learning appears to be the only machine learning
method that is ethically defensible.","Miguel Sicart, Irina Shklovski, Mirabelle Jones",2021,http://arxiv.org/abs/2201.06921v1
Compressive Classification (Machine Learning without learning),"Compressive learning is a framework where (so far unsupervised) learning
tasks use not the entire dataset but a compressed summary (sketch) of it. We
propose a compressive learning classification method, and a novel sketch
function for images.","Vincent Schellekens, Laurent Jacques",2018,http://arxiv.org/abs/1812.01410v1
A Survey on Resilient Machine Learning,"Machine learning based system are increasingly being used for sensitive tasks
such as security surveillance, guiding autonomous vehicle, taking investment
decisions, detecting and blocking network intrusion and malware etc. However,
recent research has shown that machine learning models are venerable to attacks
by adversaries at all phases of machine learning (eg, training data collection,
training, operation). All model classes of machine learning systems can be
misled by providing carefully crafted inputs making them wrongly classify
inputs. Maliciously created input samples can affect the learning process of a
ML system by either slowing down the learning process, or affecting the
performance of the learned mode, or causing the system make error(s) only in
attacker's planned scenario. Because of these developments, understanding
security of machine learning algorithms and systems is emerging as an important
research area among computer security and machine learning researchers and
practitioners. We present a survey of this emerging area in machine learning.","Atul Kumar, Sameep Mehta",2017,http://arxiv.org/abs/1707.03184v1
An Introduction to MM Algorithms for Machine Learning and Statistical,"MM (majorization--minimization) algorithms are an increasingly popular tool
for solving optimization problems in machine learning and statistical
estimation. This article introduces the MM algorithm framework in general and
via three popular example applications: Gaussian mixture regressions,
multinomial logistic regressions, and support vector machines. Specific
algorithms for the three examples are derived and numerical demonstrations are
presented. Theoretical and practical aspects of MM algorithm design are
discussed.",Hien D. Nguyen,2016,http://arxiv.org/abs/1611.03969v1
"Some Requests for Machine Learning Research from the East African Tech
  Scene","Based on 46 in-depth interviews with scientists, engineers, and CEOs, this
document presents a list of concrete machine research problems, progress on
which would directly benefit tech ventures in East Africa.",Milan Cvitkovic,2018,http://arxiv.org/abs/1810.11383v2
Machine learning and deep learning,"Today, intelligent systems that offer artificial intelligence capabilities
often rely on machine learning. Machine learning describes the capacity of
systems to learn from problem-specific training data to automate the process of
analytical model building and solve associated tasks. Deep learning is a
machine learning concept based on artificial neural networks. For many
applications, deep learning models outperform shallow machine learning models
and traditional data analysis approaches. In this article, we summarize the
fundamentals of machine learning and deep learning to generate a broader
understanding of the methodical underpinning of current intelligent systems. In
particular, we provide a conceptual distinction between relevant terms and
concepts, explain the process of automated analytical model building through
machine learning and deep learning, and discuss the challenges that arise when
implementing such intelligent systems in the field of electronic markets and
networked business. These naturally go beyond technological aspects and
highlight issues in human-machine interaction and artificial intelligence
servitization.","Christian Janiesch, Patrick Zschech, Kai Heinrich",2021,http://arxiv.org/abs/2104.05314v2
"TF.Learn: TensorFlow's High-level Module for Distributed Machine
  Learning","TF.Learn is a high-level Python module for distributed machine learning
inside TensorFlow. It provides an easy-to-use Scikit-learn style interface to
simplify the process of creating, configuring, training, evaluating, and
experimenting a machine learning model. TF.Learn integrates a wide range of
state-of-art machine learning algorithms built on top of TensorFlow's low level
APIs for small to large-scale supervised and unsupervised problems. This module
focuses on bringing machine learning to non-specialists using a general-purpose
high-level language as well as researchers who want to implement, benchmark,
and compare their new methods in a structured environment. Emphasis is put on
ease of use, performance, documentation, and API consistency.",Yuan Tang,2016,http://arxiv.org/abs/1612.04251v1
"Using Deep Learning and Machine Learning to Detect Epileptic Seizure
  with Electroencephalography (EEG) Data","The prediction of epileptic seizure has always been extremely challenging in
medical domain. However, as the development of computer technology, the
application of machine learning introduced new ideas for seizure forecasting.
Applying machine learning model onto the predication of epileptic seizure could
help us obtain a better result and there have been plenty of scientists who
have been doing such works so that there are sufficient medical data provided
for researchers to do training of machine learning models.","Haotian Liu, Lin Xi, Ying Zhao, Zhixiang Li",2019,http://arxiv.org/abs/1910.02544v1
Machine Learning in Network Security Using KNIME Analytics,"Machine learning has more and more effect on our every day's life. This field
keeps growing and expanding into new areas. Machine learning is based on the
implementation of artificial intelligence that gives systems the capability to
automatically learn and enhance from experiments without being explicitly
programmed. Machine Learning algorithms apply mathematical equations to analyze
datasets and predict values based on the dataset. In the field of
cybersecurity, machine learning algorithms can be utilized to train and analyze
the Intrusion Detection Systems (IDSs) on security-related datasets. In this
paper, we tested different machine learning algorithms to analyze NSL-KDD
dataset using KNIME analytics.",Munther Abualkibash,2019,http://arxiv.org/abs/2001.11489v1
SELM: Software Engineering of Machine Learning Models,"One of the pillars of any machine learning model is its concepts. Using
software engineering, we can engineer these concepts and then develop and
expand them. In this article, we present a SELM framework for Software
Engineering of machine Learning Models. We then evaluate this framework through
a case study. Using the SELM framework, we can improve a machine learning
process efficiency and provide more accuracy in learning with less processing
hardware resources and a smaller training dataset. This issue highlights the
importance of an interdisciplinary approach to machine learning. Therefore, in
this article, we have provided interdisciplinary teams' proposals for machine
learning.","Nafiseh Jafari, Mohammad Reza Besharati, Mohammad Izadi, Maryam Hourali",2021,http://arxiv.org/abs/2103.11249v1
Machine Learning as Ecology,"Machine learning methods have had spectacular success on numerous problems.
Here we show that a prominent class of learning algorithms - including Support
Vector Machines (SVMs) -- have a natural interpretation in terms of ecological
dynamics. We use these ideas to design new online SVM algorithms that exploit
ecological invasions, and benchmark performance using the MNIST dataset. Our
work provides a new ecological lens through which we can view statistical
learning and opens the possibility of designing ecosystems for machine
learning.
  Supplemental code is found at https://github.com/owenhowell20/EcoSVM.","Owen Howell, Cui Wenping, Robert Marsland III, Pankaj Mehta",2019,http://arxiv.org/abs/1908.00868v2
Challenges and Opportunities in Quantum Machine Learning,"At the intersection of machine learning and quantum computing, Quantum
Machine Learning (QML) has the potential of accelerating data analysis,
especially for quantum data, with applications for quantum materials,
biochemistry, and high-energy physics. Nevertheless, challenges remain
regarding the trainability of QML models. Here we review current methods and
applications for QML. We highlight differences between quantum and classical
machine learning, with a focus on quantum neural networks and quantum deep
learning. Finally, we discuss opportunities for quantum advantage with QML.","M. Cerezo, Guillaume Verdon, Hsin-Yuan Huang, Lukasz Cincio, Patrick J. Coles",2023,http://arxiv.org/abs/2303.09491v1
A Theory of Machine Learning,"We critically review three major theories of machine learning and provide a
new theory according to which machines learn a function when the machines
successfully compute it. We show that this theory challenges common assumptions
in the statistical and the computational learning theories, for it implies that
learning true probabilities is equivalent neither to obtaining a correct
calculation of the true probabilities nor to obtaining an almost-sure
convergence to them. We also briefly discuss some case studies from natural
language processing and macroeconomics from the perspective of the new theory.","Jinsook Kim, Jinho Kang",2024,http://arxiv.org/abs/2407.05520v1
Learning Moore Machines from Input-Output Traces,"The problem of learning automata from example traces (but no equivalence or
membership queries) is fundamental in automata learning theory and practice. In
this paper we study this problem for finite state machines with inputs and
outputs, and in particular for Moore machines. We develop three algorithms for
solving this problem: (1) the PTAP algorithm, which transforms a set of
input-output traces into an incomplete Moore machine and then completes the
machine with self-loops; (2) the PRPNI algorithm, which uses the well-known
RPNI algorithm for automata learning to learn a product of automata encoding a
Moore machine; and (3) the MooreMI algorithm, which directly learns a Moore
machine using PTAP extended with state merging. We prove that MooreMI has the
fundamental identification in the limit property. We also compare the
algorithms experimentally in terms of the size of the learned machine and
several notions of accuracy, introduced in this paper. Finally, we compare with
OSTIA, an algorithm that learns a more general class of transducers, and find
that OSTIA generally does not learn a Moore machine, even when fed with a
characteristic sample.","Georgios Giantamidis, Stavros Tripakis",2016,http://arxiv.org/abs/1605.07805v2
"How Developers Iterate on Machine Learning Workflows -- A Survey of the
  Applied Machine Learning Literature","Machine learning workflow development is anecdotally regarded to be an
iterative process of trial-and-error with humans-in-the-loop. However, we are
not aware of quantitative evidence corroborating this popular belief. A
quantitative characterization of iteration can serve as a benchmark for machine
learning workflow development in practice, and can aid the development of
human-in-the-loop machine learning systems. To this end, we conduct a
small-scale survey of the applied machine learning literature from five
distinct application domains. We collect and distill statistics on the role of
iteration within machine learning workflow development, and report preliminary
trends and insights from our investigation, as a starting point towards this
benchmark. Based on our findings, we finally describe desiderata for effective
and versatile human-in-the-loop machine learning systems that can cater to
users in diverse domains.","Doris Xin, Litian Ma, Shuchen Song, Aditya Parameswaran",2018,http://arxiv.org/abs/1803.10311v2
"Julia Language in Machine Learning: Algorithms, Applications, and Open
  Issues","Machine learning is driving development across many fields in science and
engineering. A simple and efficient programming language could accelerate
applications of machine learning in various fields. Currently, the programming
languages most commonly used to develop machine learning algorithms include
Python, MATLAB, and C/C ++. However, none of these languages well balance both
efficiency and simplicity. The Julia language is a fast, easy-to-use, and
open-source programming language that was originally designed for
high-performance computing, which can well balance the efficiency and
simplicity. This paper summarizes the related research work and developments in
the application of the Julia language in machine learning. It first surveys the
popular machine learning algorithms that are developed in the Julia language.
Then, it investigates applications of the machine learning algorithms
implemented with the Julia language. Finally, it discusses the open issues and
the potential future directions that arise in the use of the Julia language in
machine learning.","Kaifeng Gao, Gang Mei, Francesco Piccialli, Salvatore Cuomo, Jingzhi Tu, Zenan Huo",2020,http://arxiv.org/abs/2003.10146v2
"Modeling Generalization in Machine Learning: A Methodological and
  Computational Study","As machine learning becomes more and more available to the general public,
theoretical questions are turning into pressing practical issues. Possibly, one
of the most relevant concerns is the assessment of our confidence in trusting
machine learning predictions. In many real-world cases, it is of utmost
importance to estimate the capabilities of a machine learning algorithm to
generalize, i.e., to provide accurate predictions on unseen data, depending on
the characteristics of the target problem. In this work, we perform a
meta-analysis of 109 publicly-available classification data sets, modeling
machine learning generalization as a function of a variety of data set
characteristics, ranging from number of samples to intrinsic dimensionality,
from class-wise feature skewness to $F1$ evaluated on test samples falling
outside the convex hull of the training set. Experimental results demonstrate
the relevance of using the concept of the convex hull of the training data in
assessing machine learning generalization, by emphasizing the difference
between interpolated and extrapolated predictions. Besides several predictable
correlations, we observe unexpectedly weak associations between the
generalization ability of machine learning models and all metrics related to
dimensionality, thus challenging the common assumption that the \textit{curse
of dimensionality} might impair generalization in machine learning.","Pietro Barbiero, Giovanni Squillero, Alberto Tonda",2020,http://arxiv.org/abs/2006.15680v1
Practical Solutions for Machine Learning Safety in Autonomous Vehicles,"Autonomous vehicles rely on machine learning to solve challenging tasks in
perception and motion planning. However, automotive software safety standards
have not fully evolved to address the challenges of machine learning safety
such as interpretability, verification, and performance limitations. In this
paper, we review and organize practical machine learning safety techniques that
can complement engineering safety for machine learning based software in
autonomous vehicles. Our organization maps safety strategies to
state-of-the-art machine learning techniques in order to enhance dependability
and safety of machine learning algorithms. We also discuss security limitations
and user experience aspects of machine learning components in autonomous
vehicles.","Sina Mohseni, Mandar Pitale, Vasu Singh, Zhangyang Wang",2019,http://arxiv.org/abs/1912.09630v1
Mental Models of Adversarial Machine Learning,"Although machine learning is widely used in practice, little is known about
practitioners' understanding of potential security challenges. In this work, we
close this substantial gap and contribute a qualitative study focusing on
developers' mental models of the machine learning pipeline and potentially
vulnerable components. Similar studies have helped in other security fields to
discover root causes or improve risk communication. Our study reveals two
\facets of practitioners' mental models of machine learning security. Firstly,
practitioners often confuse machine learning security with threats and defences
that are not directly related to machine learning. Secondly, in contrast to
most academic research, our participants perceive security of machine learning
as not solely related to individual models, but rather in the context of entire
workflows that consist of multiple components. Jointly with our additional
findings, these two facets provide a foundation to substantiate mental models
for machine learning security and have implications for the integration of
adversarial machine learning into corporate workflows, \new{decreasing
practitioners' reported uncertainty}, and appropriate regulatory frameworks for
machine learning security.","Lukas Bieringer, Kathrin Grosse, Michael Backes, Battista Biggio, Katharina Krombholz",2021,http://arxiv.org/abs/2105.03726v4
"Applying Machine Learning to Life Insurance: some knowledge sharing to
  master it","Machine Learning permeates many industries, which brings new source of
benefits for companies. However within the life insurance industry, Machine
Learning is not widely used in practice as over the past years statistical
models have shown their efficiency for risk assessment. Thus insurers may face
difficulties to assess the value of the artificial intelligence. Focusing on
the modification of the life insurance industry over time highlights the stake
of using Machine Learning for insurers and benefits that it can bring by
unleashing data value. This paper reviews traditional actuarial methodologies
for survival modeling and extends them with Machine Learning techniques. It
points out differences with regular machine learning models and emphasizes
importance of specific implementations to face censored data with machine
learning models family. In complement to this article, a Python library has
been developed. Different open-source Machine Learning algorithms have been
adjusted to adapt the specificities of life insurance data, namely censoring
and truncation. Such models can be easily applied from this SCOR library to
accurately model life insurance risks.","Antoine Chancel, Laura Bradier, Antoine Ly, Razvan Ionescu, Laurene Martin, Marguerite Sauce",2022,http://arxiv.org/abs/2209.02057v3
Machine learning and domain decomposition methods -- a survey,"Hybrid algorithms, which combine black-box machine learning methods with
experience from traditional numerical methods and domain expertise from diverse
application areas, are progressively gaining importance in scientific machine
learning and various industrial domains, especially in computational science
and engineering. In the present survey, several promising avenues of research
will be examined which focus on the combination of machine learning (ML) and
domain decomposition methods (DDMs). The aim of this survey is to provide an
overview of existing work within this field and to structure it into domain
decomposition for machine learning and machine learning-enhanced domain
decomposition, including: domain decomposition for classical machine learning,
domain decomposition to accelerate the training of physics-aware neural
networks, machine learning to enhance the convergence properties or
computational efficiency of DDMs, and machine learning as a discretization
method in a DDM for the solution of PDEs. In each of these fields, we summarize
existing work and key advances within a common framework and, finally, disuss
ongoing challenges and opportunities for future research.","Axel Klawonn, Martin Lanser, Janine Weber",2023,http://arxiv.org/abs/2312.14050v1
"Beyond Model Interpretability: Socio-Structural Explanations in Machine
  Learning","What is it to interpret the outputs of an opaque machine learning model. One
approach is to develop interpretable machine learning techniques. These
techniques aim to show how machine learning models function by providing either
model centric local or global explanations, which can be based on mechanistic
interpretations revealing the inner working mechanisms of models or
nonmechanistic approximations showing input feature output data relationships.
In this paper, we draw on social philosophy to argue that interpreting machine
learning outputs in certain normatively salient domains could require appealing
to a third type of explanation that we call sociostructural explanation. The
relevance of this explanation type is motivated by the fact that machine
learning models are not isolated entities but are embedded within and shaped by
social structures. Sociostructural explanations aim to illustrate how social
structures contribute to and partially explain the outputs of machine learning
models. We demonstrate the importance of sociostructural explanations by
examining a racially biased healthcare allocation algorithm. Our proposal
highlights the need for transparency beyond model interpretability,
understanding the outputs of machine learning systems could require a broader
analysis that extends beyond the understanding of the machine learning model
itself.","Andrew Smart, Atoosa Kasirzadeh",2024,http://arxiv.org/abs/2409.03632v1
"Aspects of Artificial Intelligence: Transforming Machine Learning
  Systems Naturally","In this paper, we study the machine learning elements which we are interested
in together as a machine learning system, consisting of a collection of machine
learning elements and a collection of relations between the elements. The
relations we concern are algebraic operations, binary relations, and binary
relations with composition that can be reasoned categorically. A machine
learning system transformation between two systems is a map between the
systems, which preserves the relations we concern. The system transformations
given by quotient or clustering, representable functor, and Yoneda embedding
are highlighted and discussed by machine learning examples. An adjunction
between machine learning systems, a special machine learning system
transformation loop, provides the optimal way of solving problems. Machine
learning system transformations are linked and compared by their maps at
2-cell, natural transformations. New insights and structures can be obtained
from universal properties and algebraic structures given by monads, which are
generated from adjunctions.",Xiuzhan Guo,2025,http://arxiv.org/abs/2502.01708v1
Learning Theory and Support Vector Machines - a primer,"The main goal of statistical learning theory is to provide a fundamental
framework for the problem of decision making and model construction based on
sets of data. Here, we present a brief introduction to the fundamentals of
statistical learning theory, in particular the difference between empirical and
structural risk minimization, including one of its most prominent
implementations, i.e. the Support Vector Machine.",Michael Banf,2019,http://arxiv.org/abs/1902.04622v1
Machine Learning using Stata/Python,"We present two related Stata modules, r_ml_stata and c_ml_stata, for fitting
popular Machine Learning (ML) methods both in regression and classification
settings. Using the recent Stata/Python integration platform (sfi) of Stata 16,
these commands provide hyper-parameters' optimal tuning via K-fold
cross-validation using greed search. More specifically, they make use of the
Python Scikit-learn API to carry out both cross-validation and outcome/label
prediction.",Giovanni Cerulli,2021,http://arxiv.org/abs/2103.03122v1
Financial Time Series Data Processing for Machine Learning,"This article studies the financial time series data processing for machine
learning. It introduces the most frequent scaling methods, then compares the
resulting stationarity and preservation of useful information for trend
forecasting. It proposes an empirical test based on the capability to learn
simple data relationship with simple models. It also speaks about the data
split method specific to time series, avoiding unwanted overfitting and
proposes various labelling for classification and regression.",Fabrice Daniel,2019,http://arxiv.org/abs/1907.03010v1
Pen and Paper Exercises in Machine Learning,"This is a collection of (mostly) pen-and-paper exercises in machine learning.
The exercises are on the following topics: linear algebra, optimisation,
directed graphical models, undirected graphical models, expressive power of
graphical models, factor graphs and message passing, inference for hidden
Markov models, model-based learning (including ICA and unnormalised models),
sampling and Monte-Carlo integration, and variational inference.",Michael U. Gutmann,2022,http://arxiv.org/abs/2206.13446v1
Classic machine learning methods,"In this chapter, we present the main classic machine learning methods. A
large part of the chapter is devoted to supervised learning techniques for
classification and regression, including nearest-neighbor methods, linear and
logistic regressions, support vector machines and tree-based algorithms. We
also describe the problem of overfitting as well as strategies to overcome it.
We finally provide a brief overview of unsupervised learning methods, namely
for clustering and dimensionality reduction.","Johann Faouzi, Olivier Colliot",2023,http://arxiv.org/abs/2310.11470v1
Information Theory and its Relation to Machine Learning,"In this position paper, I first describe a new perspective on machine
learning (ML) by four basic problems (or levels), namely, ""What to learn?"",
""How to learn?"", ""What to evaluate?"", and ""What to adjust?"". The paper stresses
more on the first level of ""What to learn?"", or ""Learning Target Selection"".
Towards this primary problem within the four levels, I briefly review the
existing studies about the connection between information theoretical learning
(ITL [1]) and machine learning. A theorem is given on the relation between the
empirically-defined similarity measure and information measures. Finally, a
conjecture is proposed for pursuing a unified mathematical interpretation to
learning target selection.",Bao-Gang Hu,2015,http://arxiv.org/abs/1501.04309v1
Discussion on Mechanical Learning and Learning Machine,"Mechanical learning is a computing system that is based on a set of simple
and fixed rules, and can learn from incoming data. A learning machine is a
system that realizes mechanical learning. Importantly, we emphasis that it is
based on a set of simple and fixed rules, contrasting to often called machine
learning that is sophisticated software based on very complicated mathematical
theory, and often needs human intervene for software fine tune and manual
adjustments. Here, we discuss some basic facts and principles of such system,
and try to lay down a framework for further study. We propose 2 directions to
approach mechanical learning, just like Church-Turing pair: one is trying to
realize a learning machine, another is trying to well describe the mechanical
learning.",Chuyu Xiong,2016,http://arxiv.org/abs/1602.00198v1
Opening the black box of deep learning,"The great success of deep learning shows that its technology contains
profound truth, and understanding its internal mechanism not only has important
implications for the development of its technology and effective application in
various fields, but also provides meaningful insights into the understanding of
human brain mechanism. At present, most of the theoretical research on deep
learning is based on mathematics. This dissertation proposes that the neural
network of deep learning is a physical system, examines deep learning from
three different perspectives: microscopic, macroscopic, and physical world
views, answers multiple theoretical puzzles in deep learning by using physics
principles. For example, from the perspective of quantum mechanics and
statistical physics, this dissertation presents the calculation methods for
convolution calculation, pooling, normalization, and Restricted Boltzmann
Machine, as well as the selection of cost functions, explains why deep learning
must be deep, what characteristics are learned in deep learning, why
Convolutional Neural Networks do not have to be trained layer by layer, and the
limitations of deep learning, etc., and proposes the theoretical direction and
basis for the further development of deep learning now and in the future. The
brilliance of physics flashes in deep learning, we try to establish the deep
learning technology based on the scientific theory of physics.","Dian Lei, Xiaoxiao Chen, Jianfei Zhao",2018,http://arxiv.org/abs/1805.08355v1
Concept-Oriented Deep Learning,"Concepts are the foundation of human deep learning, understanding, and
knowledge integration and transfer. We propose concept-oriented deep learning
(CODL) which extends (machine) deep learning with concept representations and
conceptual understanding capability. CODL addresses some of the major
limitations of deep learning: interpretability, transferability, contextual
adaptation, and requirement for lots of labeled training data. We discuss the
major aspects of CODL including concept graph, concept representations, concept
exemplars, and concept representation learning systems supporting incremental
and continual learning.",Daniel T Chang,2018,http://arxiv.org/abs/1806.01756v1
"Deep learning research landscape & roadmap in a nutshell: past, present
  and future -- Towards deep cortical learning","The past, present and future of deep learning is presented in this work.
Given this landscape & roadmap, we predict that deep cortical learning will be
the convergence of deep learning & cortical learning which builds an artificial
cortical column ultimately.",Aras R. Dargazany,2019,http://arxiv.org/abs/1908.02130v1
A First Look at Deep Learning Apps on Smartphones,"We are in the dawn of deep learning explosion for smartphones. To bridge the
gap between research and practice, we present the first empirical study on
16,500 the most popular Android apps, demystifying how smartphone apps exploit
deep learning in the wild. To this end, we build a new static tool that
dissects apps and analyzes their deep learning functions. Our study answers
threefold questions: what are the early adopter apps of deep learning, what do
they use deep learning for, and how do their deep learning models look like.
Our study has strong implications for app developers, smartphone vendors, and
deep learning R\&D. On one hand, our findings paint a promising picture of deep
learning for smartphones, showing the prosperity of mobile deep learning
frameworks as well as the prosperity of apps building their cores atop deep
learning. On the other hand, our findings urge optimizations on deep learning
models deployed on smartphones, the protection of these models, and validation
of research ideas on these models.","Mengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, Xuanzhe Liu",2018,http://arxiv.org/abs/1812.05448v4
"Geometrization of deep networks for the interpretability of deep
  learning systems","How to understand deep learning systems remains an open problem. In this
paper we propose that the answer may lie in the geometrization of deep
networks. Geometrization is a bridge to connect physics, geometry, deep network
and quantum computation and this may result in a new scheme to reveal the rule
of the physical world. By comparing the geometry of image matching and deep
networks, we show that geometrization of deep networks can be used to
understand existing deep learning systems and it may also help to solve the
interpretability problem of deep learning systems.","Xiao Dong, Ling Zhou",2019,http://arxiv.org/abs/1901.02354v2
Why & When Deep Learning Works: Looking Inside Deep Learnings,"The Intel Collaborative Research Institute for Computational Intelligence
(ICRI-CI) has been heavily supporting Machine Learning and Deep Learning
research from its foundation in 2012. We have asked six leading ICRI-CI Deep
Learning researchers to address the challenge of ""Why & When Deep Learning
works"", with the goal of looking inside Deep Learning, providing insights on
how deep networks function, and uncovering key observations on their
expressiveness, limitations, and potential. The output of this challenge
resulted in five papers that address different facets of deep learning. These
different facets include a high-level understating of why and when deep
networks work (and do not work), the impact of geometry on the expressiveness
of deep networks, and making deep networks interpretable.",Ronny Ronen,2017,http://arxiv.org/abs/1705.03921v1
Learning Task-aware Robust Deep Learning Systems,"Many works demonstrate that deep learning system is vulnerable to adversarial
attack. A deep learning system consists of two parts: the deep learning task
and the deep model. Nowadays, most existing works investigate the impact of the
deep model on robustness of deep learning systems, ignoring the impact of the
learning task. In this paper, we adopt the binary and interval label encoding
strategy to redefine the classification task and design corresponding loss to
improve robustness of the deep learning system. Our method can be viewed as
improving the robustness of deep learning systems from both the learning task
and deep model. Experimental results demonstrate that our learning task-aware
method is much more robust than traditional classification while retaining the
accuracy.","Keji Han, Yun Li, Xianzhong Long, Yao Ge",2020,http://arxiv.org/abs/2010.05125v2
Deep Learning in Software Engineering,"Recent years, deep learning is increasingly prevalent in the field of
Software Engineering (SE). However, many open issues still remain to be
investigated. How do researchers integrate deep learning into SE problems?
Which SE phases are facilitated by deep learning? Do practitioners benefit from
deep learning? The answers help practitioners and researchers develop practical
deep learning models for SE tasks. To answer these questions, we conduct a
bibliography analysis on 98 research papers in SE that use deep learning
techniques. We find that 41 SE tasks in all SE phases have been facilitated by
deep learning integrated solutions. In which, 84.7% papers only use standard
deep learning models and their variants to solve SE problems. The
practicability becomes a concern in utilizing deep learning techniques. How to
improve the effectiveness, efficiency, understandability, and testability of
deep learning based solutions may attract more SE researchers in the future.","Xiaochen Li, He Jiang, Zhilei Ren, Ge Li, Jingxuan Zhang",2018,http://arxiv.org/abs/1805.04825v1
Moving Deep Learning into Web Browser: How Far Can We Go?,"Recently, several JavaScript-based deep learning frameworks have emerged,
making it possible to perform deep learning tasks directly in browsers.
However, little is known on what and how well we can do with these frameworks
for deep learning in browsers. To bridge the knowledge gap, in this paper, we
conduct the first empirical study of deep learning in browsers. We survey 7
most popular JavaScript-based deep learning frameworks, investigating to what
extent deep learning tasks have been supported in browsers so far. Then we
measure the performance of different frameworks when running different deep
learning tasks. Finally, we dig out the performance gap between deep learning
in browsers and on native platforms by comparing the performance of
TensorFlow.js and TensorFlow in Python. Our findings could help application
developers, deep-learning framework vendors and browser vendors to improve the
efficiency of deep learning in browsers.","Yun Ma, Dongwei Xiang, Shuyu Zheng, Deyu Tian, Xuanzhe Liu",2019,http://arxiv.org/abs/1901.09388v2
Greedy Deep Dictionary Learning,"In this work we propose a new deep learning tool called deep dictionary
learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at
a time. This requires solving a simple (shallow) dictionary learning problem,
the solution to this is well known. We apply the proposed technique on some
benchmark deep learning datasets. We compare our results with other deep
learning tools like stacked autoencoder and deep belief network; and state of
the art supervised dictionary learning tools like discriminative KSVD and label
consistent KSVD. Our method yields better results than all.","Snigdha Tariyal, Angshul Majumdar, Richa Singh, Mayank Vatsa",2016,http://arxiv.org/abs/1602.00203v1
"Quantum Neural Networks: Concepts, Applications, and Challenges","Quantum deep learning is a research field for the use of quantum computing
techniques for training deep neural networks. The research topics and
directions of deep learning and quantum computing have been separated for long
time, however by discovering that quantum circuits can act like artificial
neural networks, quantum deep learning research is widely adopted. This paper
explains the backgrounds and basic principles of quantum deep learning and also
introduces major achievements. After that, this paper discusses the challenges
of quantum deep learning research in multiple perspectives. Lastly, this paper
presents various future research directions and application fields of quantum
deep learning.","Yunseok Kwak, Won Joon Yun, Soyi Jung, Joongheon Kim",2021,http://arxiv.org/abs/2108.01468v1
"NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders
  of Deep Giants","Tiny deep learning has attracted increasing attention driven by the
substantial demand for deploying deep learning on numerous intelligent
Internet-of-Things devices. However, it is still challenging to unleash tiny
deep learning's full potential on both large-scale datasets and downstream
tasks due to the under-fitting issues caused by the limited model capacity of
tiny neural networks (TNNs). To this end, we propose a framework called
NetBooster to empower tiny deep learning by augmenting the architectures of
TNNs via an expansion-then-contraction strategy. Extensive experiments show
that NetBooster consistently outperforms state-of-the-art tiny deep learning
solutions.","Zhongzhi Yu, Yonggan Fu, Jiayi Yuan, Haoran You, Yingyan Lin",2023,http://arxiv.org/abs/2306.13586v1
Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey,"Deep reinforcement learning augments the reinforcement learning framework and
utilizes the powerful representation of deep neural networks. Recent works have
demonstrated the remarkable successes of deep reinforcement learning in various
domains including finance, medicine, healthcare, video games, robotics, and
computer vision. In this work, we provide a detailed review of recent and
state-of-the-art research advances of deep reinforcement learning in computer
vision. We start with comprehending the theories of deep learning,
reinforcement learning, and deep reinforcement learning. We then propose a
categorization of deep reinforcement learning methodologies and discuss their
advantages and limitations. In particular, we divide deep reinforcement
learning into seven main categories according to their applications in computer
vision, i.e. (i)landmark localization (ii) object detection; (iii) object
tracking; (iv) registration on both 2D image and 3D image volumetric data (v)
image segmentation; (vi) videos analysis; and (vii) other applications. Each of
these categories is further analyzed with reinforcement learning techniques,
network design, and performance. Moreover, we provide a comprehensive analysis
of the existing publicly available datasets and examine source code
availability. Finally, we present some open issues and discuss future research
directions on deep reinforcement learning in computer vision","Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki, Khoa Luu, Marios Savvides",2021,http://arxiv.org/abs/2108.11510v1
"Probabilistic Deep Learning with Probabilistic Neural Networks and Deep
  Probabilistic Models","Probabilistic deep learning is deep learning that accounts for uncertainty,
both model uncertainty and data uncertainty. It is based on the use of
probabilistic models and deep neural networks. We distinguish two approaches to
probabilistic deep learning: probabilistic neural networks and deep
probabilistic models. The former employs deep neural networks that utilize
probabilistic layers which can represent and process uncertainty; the latter
uses probabilistic models that incorporate deep neural network components which
capture complex non-linear stochastic relationships between the random
variables. We discuss some major examples of each approach including Bayesian
neural networks and mixture density networks (for probabilistic neural
networks), and variational autoencoders, deep Gaussian processes and deep mixed
effects models (for deep probabilistic models). TensorFlow Probability is a
library for probabilistic modeling and inference which can be used for both
approaches of probabilistic deep learning. We include its code examples for
illustration.",Daniel T. Chang,2021,http://arxiv.org/abs/2106.00120v3
"Towards energy-efficient Deep Learning: An overview of energy-efficient
  approaches along the Deep Learning Lifecycle","Deep Learning has enabled many advances in machine learning applications in
the last few years. However, since current Deep Learning algorithms require
much energy for computations, there are growing concerns about the associated
environmental costs. Energy-efficient Deep Learning has received much attention
from researchers and has already made much progress in the last couple of
years. This paper aims to gather information about these advances from the
literature and show how and at which points along the lifecycle of Deep
Learning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation)
it is possible to reduce energy consumption.","Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon",2023,http://arxiv.org/abs/2303.01980v1
A Unified Framework of Deep Neural Networks by Capsules,"With the growth of deep learning, how to describe deep neural networks
unifiedly is becoming an important issue. We first formalize neural networks
mathematically with their directed graph representations, and prove a
generation theorem about the induced networks of connected directed acyclic
graphs. Then, we set up a unified framework for deep learning with capsule
networks. This capsule framework could simplify the description of existing
deep neural networks, and provide a theoretical basis of graphic designing and
programming techniques for deep learning models, thus would be of great
significance to the advancement of deep learning.","Yujian Li, Chuanhui Shan",2018,http://arxiv.org/abs/1805.03551v2
Integrating Learning and Reasoning with Deep Logic Models,"Deep learning is very effective at jointly learning feature representations
and classification models, especially when dealing with high dimensional input
patterns. Probabilistic logic reasoning, on the other hand, is capable to take
consistent and robust decisions in complex environments. The integration of
deep learning and logic reasoning is still an open-research problem and it is
considered to be the key for the development of real intelligent agents. This
paper presents Deep Logic Models, which are deep graphical models integrating
deep learning and logic reasoning both for learning and inference. Deep Logic
Models create an end-to-end differentiable architecture, where deep learners
are embedded into a network implementing a continuous relaxation of the logic
knowledge. The learning process allows to jointly learn the weights of the deep
learners and the meta-parameters controlling the high-level reasoning. The
experimental results show that the proposed methodology overtakes the
limitations of the other approaches that have been proposed to bridge deep
learning and reasoning.","Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Marco Gori",2019,http://arxiv.org/abs/1901.04195v1
Deep Learning in the Field of Biometric Template Protection: An Overview,"Today, deep learning represents the most popular and successful form of
machine learning. Deep learning has revolutionised the field of pattern
recognition, including biometric recognition. Biometric systems utilising deep
learning have been shown to achieve auspicious recognition accuracy, surpassing
human performance. Apart from said breakthrough advances in terms of biometric
performance, the use of deep learning was reported to impact different
covariates of biometrics such as algorithmic fairness, vulnerability to
attacks, or template protection. Technologies of biometric template protection
are designed to enable a secure and privacy-preserving deployment of
biometrics. In the recent past, deep learning techniques have been frequently
applied in biometric template protection systems for various purposes. This
work provides an overview of how advances in deep learning take influence on
the field of biometric template protection. The interrelation between improved
biometric performance rates and security in biometric template protection is
elaborated. Further, the use of deep learning for obtaining feature
representations that are suitable for biometric template protection is
discussed. Novel methods that apply deep learning to achieve various goals of
biometric template protection are surveyed along with deep learning-based
attacks.","Christian Rathgeb, Jascha Kolberg, Andreas Uhl, Christoph Busch",2023,http://arxiv.org/abs/2303.02715v1
A Survey Analyzing Generalization in Deep Reinforcement Learning,"Reinforcement learning research obtained significant success and attention
with the utilization of deep neural networks to solve problems in high
dimensional state or action spaces. While deep reinforcement learning policies
are currently being deployed in many different fields from medical applications
to large language models, there are still ongoing questions the field is trying
to answer on the generalization capabilities of deep reinforcement learning
policies. In this paper, we will formalize and analyze generalization in deep
reinforcement learning. We will explain the fundamental reasons why deep
reinforcement learning policies encounter overfitting problems that limit their
generalization capabilities. Furthermore, we will categorize and explain the
manifold solution approaches to increase generalization, and overcome
overfitting in deep reinforcement learning policies. From exploration to
adversarial analysis and from regularization to robustness our paper provides
an analysis on a wide range of subfields within deep reinforcement learning
with a broad scope and in-depth view. We believe our study can provide a
compact guideline for the current advancements in deep reinforcement learning,
and help to construct robust deep neural policies with higher generalization
skills.",Ezgi Korkmaz,2024,http://arxiv.org/abs/2401.02349v2
What Really is Deep Learning Doing?,"Deep learning has achieved a great success in many areas, from computer
vision to natural language processing, to game playing, and much more. Yet,
what deep learning is really doing is still an open question. There are a lot
of works in this direction. For example, [5] tried to explain deep learning by
group renormalization, and [6] tried to explain deep learning from the view of
functional approximation. In order to address this very crucial question, here
we see deep learning from perspective of mechanical learning and learning
machine (see [1], [2]). From this particular angle, we can see deep learning
much better and answer with confidence: What deep learning is really doing? why
it works well, how it works, and how much data is necessary for learning. We
also will discuss advantages and disadvantages of deep learning at the end of
this work.",Chuyu Xiong,2017,http://arxiv.org/abs/1711.03577v1
Transferability in Deep Learning: A Survey,"The success of deep learning algorithms generally depends on large-scale
data, while humans appear to have inherent ability of knowledge transfer, by
recognizing and applying relevant knowledge from previous learning experiences
when encountering and solving unseen tasks. Such an ability to acquire and
reuse knowledge is known as transferability in deep learning. It has formed the
long-term quest towards making deep learning as data-efficient as human
learning, and has been motivating fruitful design of more powerful deep
learning algorithms. We present this survey to connect different isolated areas
in deep learning with their relation to transferability, and to provide a
unified and complete view to investigating transferability through the whole
lifecycle of deep learning. The survey elaborates the fundamental goals and
challenges in parallel with the core principles and methods, covering recent
cornerstones in deep architectures, pre-training, task adaptation and domain
adaptation. This highlights unanswered questions on the appropriate objectives
for learning transferable knowledge and for adapting the knowledge to new tasks
and domains, avoiding catastrophic forgetting and negative transfer. Finally,
we implement a benchmark and an open-source library, enabling a fair evaluation
of deep learning methods in terms of transferability.","Junguang Jiang, Yang Shu, Jianmin Wang, Mingsheng Long",2022,http://arxiv.org/abs/2201.05867v1
"Feature versus Raw Sequence: Deep Learning Comparative Study on
  Predicting Pre-miRNA","Should we input known genome sequence features or input sequence itself in
deep learning framework? As deep learning more popular in various applications,
researchers often come to question whether to generate features or use raw
sequences for deep learning. To answer this question, we study the prediction
accuracy of precursor miRNA prediction of feature-based deep belief network and
sequence-based convolution neural network. Tested on a variant of six-layer
convolution neural net and three-layer deep belief network, we find the raw
sequence input based convolution neural network model performs similar or
slightly better than feature based deep belief networks with best accuracy
values of 0.995 and 0.990, respectively. Both the models outperform existing
benchmarks models. The results shows us that if provided large enough data,
well devised raw sequence based deep learning models can replace feature based
deep learning models. However, construction of well behaved deep learning model
can be very challenging. In cased features can be easily extracted,
feature-based deep learning models may be a better alternative.","Jaya Thomas, Sonia Thomas, Lee Sael",2017,http://arxiv.org/abs/1710.06798v1
"Distributed Deep Reinforcement Learning: A Survey and A Multi-Player
  Multi-Agent Learning Toolbox","With the breakthrough of AlphaGo, deep reinforcement learning becomes a
recognized technique for solving sequential decision-making problems. Despite
its reputation, data inefficiency caused by its trial and error learning
mechanism makes deep reinforcement learning hard to be practical in a wide
range of areas. Plenty of methods have been developed for sample efficient deep
reinforcement learning, such as environment modeling, experience transfer, and
distributed modifications, amongst which, distributed deep reinforcement
learning has shown its potential in various applications, such as
human-computer gaming, and intelligent transportation. In this paper, we
conclude the state of this exciting field, by comparing the classical
distributed deep reinforcement learning methods, and studying important
components to achieve efficient distributed learning, covering single player
single agent distributed deep reinforcement learning to the most complex
multiple players multiple agents distributed deep reinforcement learning.
Furthermore, we review recently released toolboxes that help to realize
distributed deep reinforcement learning without many modifications of their
non-distributed versions. By analyzing their strengths and weaknesses, a
multi-player multi-agent distributed deep reinforcement learning toolbox is
developed and released, which is further validated on Wargame, a complex
environment, showing usability of the proposed toolbox for multiple players and
multiple agents distributed deep reinforcement learning under complex games.
Finally, we try to point out challenges and future trends, hoping this brief
review can provide a guide or a spark for researchers who are interested in
distributed deep reinforcement learning.","Qiyue Yin, Tongtong Yu, Shengqi Shen, Jun Yang, Meijing Zhao, Kaiqi Huang, Bin Liang, Liang Wang",2022,http://arxiv.org/abs/2212.00253v1
Are Efficient Deep Representations Learnable?,"Many theories of deep learning have shown that a deep network can require
dramatically fewer resources to represent a given function compared to a
shallow network. But a question remains: can these efficient representations be
learned using current deep learning techniques? In this work, we test whether
standard deep learning methods can in fact find the efficient representations
posited by several theories of deep representation. Specifically, we train deep
neural networks to learn two simple functions with known efficient solutions:
the parity function and the fast Fourier transform. We find that using
gradient-based optimization, a deep network does not learn the parity function,
unless initialized very close to a hand-coded exact solution. We also find that
a deep linear neural network does not learn the fast Fourier transform, even in
the best-case scenario of infinite training data, unless the weights are
initialized very close to the exact hand-coded solution. Our results suggest
that not every element of the class of compositional functions can be learned
efficiently by a deep network, and further restrictions are necessary to
understand what functions are both efficiently representable and learnable.","Maxwell Nye, Andrew Saxe",2018,http://arxiv.org/abs/1807.06399v1
Deep Learning: A Critical Appraisal,"Although deep learning has historical roots going back decades, neither the
term ""deep learning"" nor the approach was popular just over five years ago,
when the field was reignited by papers such as Krizhevsky, Sutskever and
Hinton's now classic (2012) deep network model of Imagenet. What has the field
discovered in the five subsequent years? Against a background of considerable
progress in areas such as speech recognition, image recognition, and game
playing, and considerable enthusiasm in the popular press, I present ten
concerns for deep learning, and suggest that deep learning must be supplemented
by other techniques if we are to reach artificial general intelligence.",Gary Marcus,2018,http://arxiv.org/abs/1801.00631v1
Deep Learning for Sentiment Analysis : A Survey,"Deep learning has emerged as a powerful machine learning technique that
learns multiple layers of representations or features of the data and produces
state-of-the-art prediction results. Along with the success of deep learning in
many other application domains, deep learning is also popularly used in
sentiment analysis in recent years. This paper first gives an overview of deep
learning and then provides a comprehensive survey of its current applications
in sentiment analysis.","Lei Zhang, Shuai Wang, Bing Liu",2018,http://arxiv.org/abs/1801.07883v2
Deep Learning for Visual Navigation of Underwater Robots,"This paper aims to briefly survey deep learning methods for visual navigation
of underwater robotics. The scope of this paper includes the visual perception
of underwater robotics with deep learning methods, the available visual
underwater datasets, imitation learning, and reinforcement learning methods for
navigation. Additionally, relevant works will be categorized under the
imitation learning or deep learning paradigm for underwater robots for clarity
of the training methodologies in the current landscape. Literature that uses
deep learning algorithms to process non-visual data for underwater navigation
will not be considered, except as contrasting examples.",M. Sunbeam,2023,http://arxiv.org/abs/2310.19495v1
When deep learning meets security,"Deep learning is an emerging research field that has proven its effectiveness
towards deploying more efficient intelligent systems. Security, on the other
hand, is one of the most essential issues in modern communication systems.
Recently many papers have shown that using deep learning models can achieve
promising results when applied to the security domain. In this work, we provide
an overview for the recent studies that apply deep learning techniques to the
field of security.",Majd Latah,2018,http://arxiv.org/abs/1807.04739v1
Deep Causal Learning for Robotic Intelligence,"This invited review discusses causal learning in the context of robotic
intelligence. The paper introduced the psychological findings on causal
learning in human cognition, then it introduced the traditional statistical
solutions on causal discovery and causal inference. The paper reviewed recent
deep causal learning algorithms with a focus on their architectures and the
benefits of using deep nets and discussed the gap between deep causal learning
and the needs of robotic intelligence.",Yangming Li,2022,http://arxiv.org/abs/2212.12597v1
"Deep learning in radiology: an overview of the concepts and a survey of
  the state of the art","Deep learning is a branch of artificial intelligence where networks of simple
interconnected units are used to extract patterns from data in order to solve
complex problems. Deep learning algorithms have shown groundbreaking
performance in a variety of sophisticated tasks, especially those related to
images. They have often matched or exceeded human performance. Since the
medical field of radiology mostly relies on extracting useful information from
images, it is a very natural application area for deep learning, and research
in this area has rapidly grown in recent years. In this article, we review the
clinical reality of radiology and discuss the opportunities for application of
deep learning algorithms. We also introduce basic concepts of deep learning
including convolutional neural networks. Then, we present a survey of the
research in deep learning applied to radiology. We organize the studies by the
types of specific tasks that they attempt to solve and review the broad range
of utilized deep learning algorithms. Finally, we briefly discuss opportunities
and challenges for incorporating deep learning in the radiology practice of the
future.","Maciej A. Mazurowski, Mateusz Buda, Ashirbani Saha, Mustafa R. Bashir",2018,http://arxiv.org/abs/1802.08717v1
A Survey on Deep Learning Methods for Robot Vision,"Deep learning has allowed a paradigm shift in pattern recognition, from using
hand-crafted features together with statistical classifiers to using
general-purpose learning procedures for learning data-driven representations,
features, and classifiers together. The application of this new paradigm has
been particularly successful in computer vision, in which the development of
deep learning methods for vision applications has become a hot research topic.
Given that deep learning has already attracted the attention of the robot
vision community, the main purpose of this survey is to address the use of deep
learning in robot vision. To achieve this, a comprehensive overview of deep
learning and its usage in computer vision is given, that includes a description
of the most frequently used neural models and their main application areas.
Then, the standard methodology and tools used for designing deep-learning based
vision systems are presented. Afterwards, a review of the principal work using
deep learning in robot vision is presented, as well as current and future
trends related to the use of deep learning in robotics. This survey is intended
to be a guide for the developers of robot vision systems.","Javier Ruiz-del-Solar, Patricio Loncomilla, Naiomi Soto",2018,http://arxiv.org/abs/1803.10862v1
A Selective Overview of Deep Learning,"Deep learning has arguably achieved tremendous success in recent years. In
simple words, deep learning uses the composition of many nonlinear functions to
model the complex dependency between input features and labels. While neural
networks have a long history, recent advances have greatly improved their
performance in computer vision, natural language processing, etc. From the
statistical and scientific perspective, it is natural to ask: What is deep
learning? What are the new characteristics of deep learning, compared with
classical methods? What are the theoretical foundations of deep learning? To
answer these questions, we introduce common neural network models (e.g.,
convolutional neural nets, recurrent neural nets, generative adversarial nets)
and training techniques (e.g., stochastic gradient descent, dropout, batch
normalization) from a statistical point of view. Along the way, we highlight
new characteristics of deep learning (including depth and over-parametrization)
and explain their practical and theoretical benefits. We also sample recent
results on theories of deep learning, many of which are only suggestive. While
a complete understanding of deep learning remains elusive, we hope that our
perspectives and discussions serve as a stimulus for new statistical research.","Jianqing Fan, Cong Ma, Yiqiao Zhong",2019,http://arxiv.org/abs/1904.05526v2
Interpretations of Deep Learning by Forests and Haar Wavelets,"This paper presents a basic property of region dividing of ReLU (rectified
linear unit) deep learning when new layers are successively added, by which two
new perspectives of interpreting deep learning are given. The first is related
to decision trees and forests; we construct a deep learning structure
equivalent to a forest in classification abilities, which means that certain
kinds of ReLU deep learning can be considered as forests. The second
perspective is that Haar wavelet represented functions can be approximated by
ReLU deep learning with arbitrary precision; and then a general conclusion of
function approximation abilities of ReLU deep learning is given. Finally,
generalize some of the conclusions of ReLU deep learning to the case of
sigmoid-unit deep learning.",Changcun Huang,2019,http://arxiv.org/abs/1906.06706v7
A Brief Survey of Deep Reinforcement Learning,"Deep reinforcement learning is poised to revolutionise the field of AI and
represents a step towards building autonomous systems with a higher level
understanding of the visual world. Currently, deep learning is enabling
reinforcement learning to scale to problems that were previously intractable,
such as learning to play video games directly from pixels. Deep reinforcement
learning algorithms are also applied to robotics, allowing control policies for
robots to be learned directly from camera inputs in the real world. In this
survey, we begin with an introduction to the general field of reinforcement
learning, then progress to the main streams of value-based and policy-based
methods. Our survey will cover central algorithms in deep reinforcement
learning, including the deep $Q$-network, trust region policy optimisation, and
asynchronous advantage actor-critic. In parallel, we highlight the unique
advantages of deep neural networks, focusing on visual understanding via
reinforcement learning. To conclude, we describe several current areas of
research within the field.","Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath",2017,http://arxiv.org/abs/1708.05866v2
Topological Deep Learning: A Review of an Emerging Paradigm,"Topological data analysis (TDA) provides insight into data shape. The
summaries obtained by these methods are principled global descriptions of
multi-dimensional data whilst exhibiting stable properties such as robustness
to deformation and noise. Such properties are desirable in deep learning
pipelines but they are typically obtained using non-TDA strategies. This is
partly caused by the difficulty of combining TDA constructs (e.g. barcode and
persistence diagrams) with current deep learning algorithms. Fortunately, we
are now witnessing a growth of deep learning applications embracing
topologically-guided components. In this survey, we review the nascent field of
topological deep learning by first revisiting the core concepts of TDA. We then
explore how the use of TDA techniques has evolved over time to support deep
learning frameworks, and how they can be integrated into different aspects of
deep learning. Furthermore, we touch on TDA usage for analyzing existing deep
models; deep topological analytics. Finally, we discuss the challenges and
future prospects of topological deep learning.","Ali Zia, Abdelwahed Khamis, James Nichols, Zeeshan Hayder, Vivien Rolland, Lars Petersson",2023,http://arxiv.org/abs/2302.03836v1
Generalization and Expressivity for Deep Nets,"Along with the rapid development of deep learning in practice, the
theoretical explanations for its success become urgent. Generalization and
expressivity are two widely used measurements to quantify theoretical behaviors
of deep learning. The expressivity focuses on finding functions expressible by
deep nets but cannot be approximated by shallow nets with the similar number of
neurons. It usually implies the large capacity. The generalization aims at
deriving fast learning rate for deep nets. It usually requires small capacity
to reduce the variance. Different from previous studies on deep learning,
pursuing either expressivity or generalization, we take both factors into
account to explore the theoretical advantages of deep nets. For this purpose,
we construct a deep net with two hidden layers possessing excellent
expressivity in terms of localized and sparse approximation. Then, utilizing
the well known covering number to measure the capacity, we find that deep nets
possess excellent expressive power (measured by localized and sparse
approximation) without enlarging the capacity of shallow nets. As a
consequence, we derive near optimal learning rates for implementing empirical
risk minimization (ERM) on the constructed deep nets. These results
theoretically exhibit the advantage of deep nets from learning theory
viewpoints.",Shao-Bo Lin,2018,http://arxiv.org/abs/1803.03772v2
Deep Incremental Boosting,"This paper introduces Deep Incremental Boosting, a new technique derived from
AdaBoost, specifically adapted to work with Deep Learning methods, that reduces
the required training time and improves generalisation. We draw inspiration
from Transfer of Learning approaches to reduce the start-up time to training
each incremental Ensemble member. We show a set of experiments that outlines
some preliminary results on some common Deep Learning datasets and discuss the
potential improvements Deep Incremental Boosting brings to traditional Ensemble
methods in Deep Learning.","Alan Mosca, George D Magoulas",2017,http://arxiv.org/abs/1708.03704v1
Combining Deep Learning with Good Old-Fashioned Machine Learning,"We present a comprehensive, stacking-based framework for combining deep
learning with good old-fashioned machine learning, called Deep GOld. Our
framework involves ensemble selection from 51 retrained pretrained deep
networks as first-level models, and 10 machine-learning algorithms as
second-level models. Enabled by today's state-of-the-art software tools and
hardware platforms, Deep GOld delivers consistent improvement when tested on
four image-classification datasets: Fashion MNIST, CIFAR10, CIFAR100, and Tiny
ImageNet. Of 120 experiments, in all but 10 Deep GOld improved the original
networks' performance.",Moshe Sipper,2022,http://arxiv.org/abs/2207.03757v2
"Deep frequency principle towards understanding why deeper learning is
  faster","Understanding the effect of depth in deep learning is a critical problem. In
this work, we utilize the Fourier analysis to empirically provide a promising
mechanism to understand why feedforward deeper learning is faster. To this end,
we separate a deep neural network, trained by normal stochastic gradient
descent, into two parts during analysis, i.e., a pre-condition component and a
learning component, in which the output of the pre-condition one is the input
of the learning one. We use a filtering method to characterize the frequency
distribution of a high-dimensional function. Based on experiments of deep
networks and real dataset, we propose a deep frequency principle, that is, the
effective target function for a deeper hidden layer biases towards lower
frequency during the training. Therefore, the learning component effectively
learns a lower frequency function if the pre-condition component has more
layers. Due to the well-studied frequency principle, i.e., deep neural networks
learn lower frequency functions faster, the deep frequency principle provides a
reasonable explanation to why deeper learning is faster. We believe these
empirical studies would be valuable for future theoretical studies of the
effect of depth in deep learning.","Zhi-Qin John Xu, Hanxu Zhou",2020,http://arxiv.org/abs/2007.14313v2
Deep Bayesian Active Learning with Image Data,"Even though active learning forms an important pillar of machine learning,
deep learning tools are not prevalent within it. Deep learning poses several
difficulties when used in an active learning setting. First, active learning
(AL) methods generally rely on being able to learn and update models from small
amounts of data. Recent advances in deep learning, on the other hand, are
notorious for their dependence on large amounts of data. Second, many AL
acquisition functions rely on model uncertainty, yet deep learning methods
rarely represent such model uncertainty. In this paper we combine recent
advances in Bayesian deep learning into the active learning framework in a
practical way. We develop an active learning framework for high dimensional
data, a task which has been extremely challenging so far, with very sparse
existing literature. Taking advantage of specialised models such as Bayesian
convolutional neural networks, we demonstrate our active learning techniques
with image data, obtaining a significant improvement on existing active
learning approaches. We demonstrate this on both the MNIST dataset, as well as
for skin cancer diagnosis from lesion images (ISIC2016 task).","Yarin Gal, Riashat Islam, Zoubin Ghahramani",2017,http://arxiv.org/abs/1703.02910v1
"Deep reinforcement learning for optical systems: A case study of
  mode-locked lasers","We demonstrate that deep reinforcement learning (deep RL) provides a highly
effective strategy for the control and self-tuning of optical systems. Deep RL
integrates the two leading machine learning architectures of deep neural
networks and reinforcement learning to produce robust and stable learning for
control. Deep RL is ideally suited for optical systems as the tuning and
control relies on interactions with its environment with a goal-oriented
objective to achieve optimal immediate or delayed rewards. This allows the
optical system to recognize bi-stable structures and navigate, via trajectory
planning, to optimally performing solutions, the first such algorithm
demonstrated to do so in optical systems. We specifically demonstrate the deep
RL architecture on a mode-locked laser, where robust self-tuning and control
can be established through access of the deep RL agent to its waveplates and
polarizers. We further integrate transfer learning to help the deep RL agent
rapidly learn new parameter regimes and generalize its control authority.
Additionally, the deep RL learning can be easily integrated with other control
paradigms to provide a broad framework to control any optical system.","Chang Sun, Eurika Kaiser, Steven L. Brunton, J. Nathan Kutz",2020,http://arxiv.org/abs/2006.05579v1
"Error Bounds for a Matrix-Vector Product Approximation with Deep ReLU
  Neural Networks","Among the several paradigms of artificial intelligence (AI) or machine
learning (ML), a remarkably successful paradigm is deep learning. Deep
learning's phenomenal success has been hoped to be interpreted via fundamental
research on the theory of deep learning. Accordingly, applied research on deep
learning has spurred the theory of deep learning-oriented depth and breadth of
developments. Inspired by such developments, we pose these fundamental
questions: can we accurately approximate an arbitrary matrix-vector product
using deep rectified linear unit (ReLU) feedforward neural networks (FNNs)? If
so, can we bound the resulting approximation error? In light of these
questions, we derive error bounds in Lebesgue and Sobolev norms that comprise
our developed deep approximation theory. Guided by this theory, we have
successfully trained deep ReLU FNNs whose test results justify our developed
theory. The developed theory is also applicable for guiding and easing the
training of teacher deep ReLU FNNs in view of the emerging teacher-student AI
or ML paradigms that are essential for solving several AI or ML problems in
wireless communications and signal processing; network science and graph signal
processing; and network neuroscience and brain physics.",Tilahun M. Getu,2021,http://arxiv.org/abs/2111.12963v1
Introduction to deep learning,"Deep Learning (DL) has made a major impact on data science in the last
decade. This chapter introduces the basic concepts of this field. It includes
both the basic structures used to design deep neural networks and a brief
survey of some of its popular use cases.","Lihi Shiloh-Perl, Raja Giryes",2020,http://arxiv.org/abs/2003.03253v1
Deep Learning: From Basics to Building Deep Neural Networks with Python,"This book is intended for beginners who have no familiarity with deep
learning. Our only expectation from readers is that they already have the basic
programming skills in Python.",Milad Vazan,2022,http://arxiv.org/abs/2205.01069v1
"A Nesterov's Accelerated quasi-Newton method for Global Routing using
  Deep Reinforcement Learning","Deep Q-learning method is one of the most popularly used deep reinforcement
learning algorithms which uses deep neural networks to approximate the
estimation of the action-value function. Training of the deep Q-network (DQN)
is usually restricted to first order gradient based methods. This paper
attempts to accelerate the training of deep Q-networks by introducing a second
order Nesterov's accelerated quasi-Newton method. We evaluate the performance
of the proposed method on deep reinforcement learning using double DQNs for
global routing. The results show that the proposed method can obtain better
routing solutions compared to the DQNs trained with first order Adam and
RMSprop methods.","S. Indrapriyadarsini, Shahrzad Mahboubi, Hiroshi Ninomiya, Takeshi Kamio, Hideki Asai",2020,http://arxiv.org/abs/2010.09465v1
Augmented Q Imitation Learning (AQIL),"The study of unsupervised learning can be generally divided into two
categories: imitation learning and reinforcement learning. In imitation
learning the machine learns by mimicking the behavior of an expert system
whereas in reinforcement learning the machine learns via direct environment
feedback. Traditional deep reinforcement learning takes a significant time
before the machine starts to converge to an optimal policy. This paper proposes
Augmented Q-Imitation-Learning, a method by which deep reinforcement learning
convergence can be accelerated by applying Q-imitation-learning as the initial
training process in traditional Deep Q-learning.","Xiao Lei Zhang, Anish Agarwal",2020,http://arxiv.org/abs/2004.00993v2
Deep Reinforcement Learning for Conversational AI,"Deep reinforcement learning is revolutionizing the artificial intelligence
field. Currently, it serves as a good starting point for constructing
intelligent autonomous systems which offer a better knowledge of the visual
world. It is possible to scale deep reinforcement learning with the use of deep
learning and do amazing tasks such as use of pixels in playing video games. In
this paper, key concepts of deep reinforcement learning including reward
function, differences between reinforcement learning and supervised learning
and models for implementation of reinforcement are discussed. Key challenges
related to the implementation of reinforcement learning in conversational AI
domain are identified as well as discussed in detail. Various conversational
models which are based on deep reinforcement learning (as well as deep
learning) are also discussed. In summary, this paper discusses key aspects of
deep reinforcement learning which are crucial for designing an efficient
conversational AI.","Mahipal Jadeja, Neelanshi Varia, Agam Shah",2017,http://arxiv.org/abs/1709.05067v1
An Overview of Deep Semi-Supervised Learning,"Deep neural networks demonstrated their ability to provide remarkable
performances on a wide range of supervised learning tasks (e.g., image
classification) when trained on extensive collections of labeled data (e.g.,
ImageNet). However, creating such large datasets requires a considerable amount
of resources, time, and effort. Such resources may not be available in many
practical cases, limiting the adoption and the application of many deep
learning methods. In a search for more data-efficient deep learning methods to
overcome the need for large annotated datasets, there is a rising research
interest in semi-supervised learning and its applications to deep neural
networks to reduce the amount of labeled data required, by either developing
novel methods or adopting existing semi-supervised learning frameworks for a
deep learning setting. In this paper, we provide a comprehensive overview of
deep semi-supervised learning, starting with an introduction to the field,
followed by a summarization of the dominant semi-supervised approaches in deep
learning.","Yassine Ouali, Céline Hudelot, Myriam Tami",2020,http://arxiv.org/abs/2006.05278v2
Node-By-Node Greedy Deep Learning for Interpretable Features,"Multilayer networks have seen a resurgence under the umbrella of deep
learning. Current deep learning algorithms train the layers of the network
sequentially, improving algorithmic performance as well as providing some
regularization. We present a new training algorithm for deep networks which
trains \emph{each node in the network} sequentially. Our algorithm is orders of
magnitude faster, creates more interpretable internal representations at the
node level, while not sacrificing on the ultimate out-of-sample performance.","Ke Wu, Malik Magdon-Ismail",2016,http://arxiv.org/abs/1602.06183v1
Accelerating Deep Learning with Shrinkage and Recall,"Deep Learning is a very powerful machine learning model. Deep Learning trains
a large number of parameters for multiple layers and is very slow when data is
in large scale and the architecture size is large. Inspired from the shrinking
technique used in accelerating computation of Support Vector Machines (SVM)
algorithm and screening technique used in LASSO, we propose a shrinking Deep
Learning with recall (sDLr) approach to speed up deep learning computation. We
experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network
(DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data
sets. Results show that the speedup using shrinking Deep Learning with recall
(sDLr) can reach more than 2.0 while still giving competitive classification
performance.","Shuai Zheng, Abhinav Vishnu, Chris Ding",2016,http://arxiv.org/abs/1605.01369v2
Deep Learning for Genomics: A Concise Overview,"Advancements in genomic research such as high-throughput sequencing
techniques have driven modern genomic studies into ""big data"" disciplines. This
data explosion is constantly challenging conventional methods used in genomics.
In parallel with the urgent demand for robust algorithms, deep learning has
succeeded in a variety of fields such as vision, speech, and text processing.
Yet genomics entails unique challenges to deep learning since we are expecting
from deep learning a superhuman intelligence that explores beyond our knowledge
to interpret the genome. A powerful deep learning model should rely on
insightful utilization of task-specific knowledge. In this paper, we briefly
discuss the strengths of different deep learning models from a genomic
perspective so as to fit each particular task with a proper deep architecture,
and remark on practical considerations of developing modern deep learning
architectures for genomics. We also provide a concise review of deep learning
applications in various aspects of genomic research, as well as pointing out
potential opportunities and obstacles for future genomics applications.","Tianwei Yue, Yuanxin Wang, Longxiang Zhang, Chunming Gu, Haoru Xue, Wenping Wang, Qi Lyu, Yujie Dun",2018,http://arxiv.org/abs/1802.00810v4
"Adversarial Attack Based Countermeasures against Deep Learning
  Side-Channel Attacks","Numerous previous works have studied deep learning algorithms applied in the
context of side-channel attacks, which demonstrated the ability to perform
successful key recoveries. These studies show that modern cryptographic devices
are increasingly threatened by side-channel attacks with the help of deep
learning. However, the existing countermeasures are designed to resist
classical side-channel attacks, and cannot protect cryptographic devices from
deep learning based side-channel attacks. Thus, there arises a strong need for
countermeasures against deep learning based side-channel attacks. Although deep
learning has the high potential in solving complex problems, it is vulnerable
to adversarial attacks in the form of subtle perturbations to inputs that lead
a model to predict incorrectly.
  In this paper, we propose a kind of novel countermeasures based on
adversarial attacks that is specifically designed against deep learning based
side-channel attacks. We estimate several models commonly used in deep learning
based side-channel attacks to evaluate the proposed countermeasures. It shows
that our approach can effectively protect cryptographic devices from deep
learning based side-channel attacks in practice. In addition, our experiments
show that the new countermeasures can also resist classical side-channel
attacks.","Ruizhe Gu, Ping Wang, Mengce Zheng, Honggang Hu, Nenghai Yu",2020,http://arxiv.org/abs/2009.10568v1
"Adversarial Robustness of Deep Learning: Theory, Algorithms, and
  Applications","This tutorial aims to introduce the fundamentals of adversarial robustness of
deep learning, presenting a well-structured review of up-to-date techniques to
assess the vulnerability of various types of deep learning models to
adversarial examples. This tutorial will particularly highlight
state-of-the-art techniques in adversarial attacks and robustness verification
of deep neural networks (DNNs). We will also introduce some effective
countermeasures to improve the robustness of deep learning models, with a
particular focus on adversarial training. We aim to provide a comprehensive
overall picture about this emerging direction and enable the community to be
aware of the urgency and importance of designing robust deep learning models in
safety-critical data analytical applications, ultimately enabling the end-users
to trust deep learning classifiers. We will also summarize potential research
directions concerning the adversarial robustness of deep learning, and its
potential benefits to enable accountable and trustworthy deep learning-based
data analytical systems and applications.","Wenjie Ruan, Xinping Yi, Xiaowei Huang",2021,http://arxiv.org/abs/2108.10451v1
Efficient Deep Feature Learning and Extraction via StochasticNets,"Deep neural networks are a powerful tool for feature learning and extraction
given their ability to model high-level abstractions in highly complex data.
One area worth exploring in feature learning and extraction using deep neural
networks is efficient neural connectivity formation for faster feature learning
and extraction. Motivated by findings of stochastic synaptic connectivity
formation in the brain as well as the brain's uncanny ability to efficiently
represent information, we propose the efficient learning and extraction of
features via StochasticNets, where sparsely-connected deep neural networks can
be formed via stochastic connectivity between neurons. To evaluate the
feasibility of such a deep neural network architecture for feature learning and
extraction, we train deep convolutional StochasticNets to learn abstract
features using the CIFAR-10 dataset, and extract the learned features from
images to perform classification on the SVHN and STL-10 datasets. Experimental
results show that features learned using deep convolutional StochasticNets,
with fewer neural connections than conventional deep convolutional neural
networks, can allow for better or comparable classification accuracy than
conventional deep neural networks: relative test error decrease of ~4.5% for
classification on the STL-10 dataset and ~1% for classification on the SVHN
dataset. Furthermore, it was shown that the deep features extracted using deep
convolutional StochasticNets can provide comparable classification accuracy
even when only 10% of the training data is used for feature learning. Finally,
it was also shown that significant gains in feature extraction speed can be
achieved in embedded applications using StochasticNets. As such, StochasticNets
allow for faster feature learning and extraction performance while facilitate
for better or comparable accuracy performances.","Mohammad Javad Shafiee, Parthipan Siva, Paul Fieguth, Alexander Wong",2015,http://arxiv.org/abs/1512.03844v1
Modern Deep Reinforcement Learning Algorithms,"Recent advances in Reinforcement Learning, grounded on combining classical
theoretical results with Deep Learning paradigm, led to breakthroughs in many
artificial intelligence tasks and gave birth to Deep Reinforcement Learning
(DRL) as a field of research. In this work latest DRL algorithms are reviewed
with a focus on their theoretical justification, practical limitations and
observed empirical properties.","Sergey Ivanov, Alexander D'yakonov",2019,http://arxiv.org/abs/1906.10025v2
An Essay on Optimization Mystery of Deep Learning,"Despite the huge empirical success of deep learning, theoretical
understanding of neural networks learning process is still lacking. This is the
reason, why some of its features seem ""mysterious"". We emphasize two mysteries
of deep learning: generalization mystery, and optimization mystery. In this
essay we review and draw connections between several selected works concerning
the latter.",Eugene Golikov,2019,http://arxiv.org/abs/1905.07187v1
Deep Super Learner: A Deep Ensemble for Classification Problems,"Deep learning has become very popular for tasks such as predictive modeling
and pattern recognition in handling big data. Deep learning is a powerful
machine learning method that extracts lower level features and feeds them
forward for the next layer to identify higher level features that improve
performance. However, deep neural networks have drawbacks, which include many
hyper-parameters and infinite architectures, opaqueness into results, and
relatively slower convergence on smaller datasets. While traditional machine
learning algorithms can address these drawbacks, they are not typically capable
of the performance levels achieved by deep neural networks. To improve
performance, ensemble methods are used to combine multiple base learners. Super
learning is an ensemble that finds the optimal combination of diverse learning
algorithms. This paper proposes deep super learning as an approach which
achieves log loss and accuracy results competitive to deep neural networks
while employing traditional machine learning algorithms in a hierarchical
structure. The deep super learner is flexible, adaptable, and easy to train
with good performance across different tasks using identical hyper-parameter
values. Using traditional machine learning requires fewer hyper-parameters,
allows transparency into results, and has relatively fast convergence on
smaller datasets. Experimental results show that the deep super learner has
superior performance compared to the individual base learners, single-layer
ensembles, and in some cases deep neural networks. Performance of the deep
super learner may further be improved with task-specific tuning.","Steven Young, Tamer Abdou, Ayse Bener",2018,http://arxiv.org/abs/1803.02323v1
Deep Embedding Kernel,"In this paper, we propose a novel supervised learning method that is called
Deep Embedding Kernel (DEK). DEK combines the advantages of deep learning and
kernel methods in a unified framework. More specifically, DEK is a learnable
kernel represented by a newly designed deep architecture. Compared with
pre-defined kernels, this kernel can be explicitly trained to map data to an
optimized high-level feature space where data may have favorable features
toward the application. Compared with typical deep learning using SoftMax or
logistic regression as the top layer, DEK is expected to be more generalizable
to new data. Experimental results show that DEK has superior performance than
typical machine learning methods in identity detection, classification,
regression, dimension reduction, and transfer learning.","Linh Le, Ying Xie",2018,http://arxiv.org/abs/1804.05806v1
Priors in Bayesian Deep Learning: A Review,"While the choice of prior is one of the most critical parts of the Bayesian
inference workflow, recent Bayesian deep learning models have often fallen back
on vague priors, such as standard Gaussians. In this review, we highlight the
importance of prior choices for Bayesian deep learning and present an overview
of different priors that have been proposed for (deep) Gaussian processes,
variational autoencoders, and Bayesian neural networks. We also outline
different methods of learning priors for these models from data. We hope to
motivate practitioners in Bayesian deep learning to think more carefully about
the prior specification for their models and to provide them with some
inspiration in this regard.",Vincent Fortuin,2021,http://arxiv.org/abs/2105.06868v3
Deep Learning for Epidemiologists: An Introduction to Neural Networks,"Deep learning methods are increasingly being applied to problems in medicine
and healthcare. However, few epidemiologists have received formal training in
these methods. To bridge this gap, this article introduces to the fundamentals
of deep learning from an epidemiological perspective. Specifically, this
article reviews core concepts in machine learning (overfitting, regularization,
hyperparameters), explains several fundamental deep learning architectures
(convolutional neural networks, recurrent neural networks), and summarizes
training, evaluation, and deployment of models. We aim to enable the reader to
engage with and critically evaluate medical applications of deep learning,
facilitating a dialogue between computer scientists and epidemiologists that
will improve the safety and efficacy of applications of this technology.","Stylianos Serghiou, Kathryn Rough",2022,http://arxiv.org/abs/2202.01319v1
"Improving Cancer Imaging Diagnosis with Bayesian Networks and Deep
  Learning: A Bayesian Deep Learning Approach","With recent advancements in the development of artificial intelligence
applications using theories and algorithms in machine learning, many accurate
models can be created to train and predict on given datasets. With the
realization of the importance of imaging interpretation in cancer diagnosis,
this article aims to investigate the theory behind Deep Learning and Bayesian
Network prediction models. Based on the advantages and drawbacks of each model,
different approaches will be used to construct a Bayesian Deep Learning Model,
combining the strengths while minimizing the weaknesses. Finally, the
applications and accuracy of the resulting Bayesian Deep Learning approach in
the health industry in classifying images will be analyzed.","Pei Xi, Lin",2024,http://arxiv.org/abs/2403.19083v1
"Application of deep reinforcement learning for Indian stock trading
  automation","In stock trading, feature extraction and trading strategy design are the two
important tasks to achieve long-term benefits using machine learning
techniques. Several methods have been proposed to design trading strategy by
acquiring trading signals to maximize the rewards. In the present paper the
theory of deep reinforcement learning is applied for stock trading strategy and
investment decisions to Indian markets. The experiments are performed
systematically with three classical Deep Reinforcement Learning models Deep
Q-Network, Double Deep Q-Network and Dueling Double Deep Q-Network on ten
Indian stock datasets. The performance of the models are evaluated and
comparison is made.",Supriya Bajpai,2021,http://arxiv.org/abs/2106.16088v1
"The Deep Ritz method: A deep learning-based numerical algorithm for
  solving variational problems","We propose a deep learning based method, the Deep Ritz Method, for
numerically solving variational problems, particularly the ones that arise from
partial differential equations. The Deep Ritz method is naturally nonlinear,
naturally adaptive and has the potential to work in rather high dimensions. The
framework is quite simple and fits well with the stochastic gradient descent
method used in deep learning. We illustrate the method on several problems
including some eigenvalue problems.","Weinan E, Bing Yu",2017,http://arxiv.org/abs/1710.00211v1
Deep Divergence Learning,"Classical linear metric learning methods have recently been extended along
two distinct lines: deep metric learning methods for learning embeddings of the
data using neural networks, and Bregman divergence learning approaches for
extending learning Euclidean distances to more general divergence measures such
as divergences over distributions. In this paper, we introduce deep Bregman
divergences, which are based on learning and parameterizing functional Bregman
divergences using neural networks, and which unify and extend these existing
lines of work. We show in particular how deep metric learning formulations,
kernel metric learning, Mahalanobis metric learning, and moment-matching
functions for comparing distributions arise as special cases of these
divergences in the symmetric setting. We then describe a deep learning
framework for learning general functional Bregman divergences, and show in
experiments that this method yields superior performance on benchmark datasets
as compared to existing deep metric learning approaches. We also discuss novel
applications, including a semi-supervised distributional clustering problem,
and a new loss function for unsupervised data generation.","Kubra Cilingir, Rachel Manzelli, Brian Kulis",2020,http://arxiv.org/abs/2005.02612v1
"Split learning for health: Distributed deep learning without sharing raw
  patient data","Can health entities collaboratively train deep learning models without
sharing sensitive raw data? This paper proposes several configurations of a
distributed deep learning method called SplitNN to facilitate such
collaborations. SplitNN does not share raw data or model details with
collaborating institutions. The proposed configurations of splitNN cater to
practical settings of i) entities holding different modalities of patient data,
ii) centralized and local health entities collaborating on multiple tasks and
iii) learning without sharing labels. We compare performance and resource
efficiency trade-offs of splitNN and other distributed deep learning methods
like federated learning, large batch synchronous stochastic gradient descent
and show highly encouraging results for splitNN.","Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, Ramesh Raskar",2018,http://arxiv.org/abs/1812.00564v1
Asset Pricing and Deep Learning,"Traditional machine learning methods have been widely studied in financial
innovation. My study focuses on the application of deep learning methods on
asset pricing. I investigate various deep learning methods for asset pricing,
especially for risk premia measurement. All models take the same set of
predictive signals (firm characteristics, systematic risks and macroeconomics).
I demonstrate high performance of all kinds of state-of-the-art (SOTA) deep
learning methods, and figure out that RNNs with memory mechanism and attention
have the best performance in terms of predictivity. Furthermore, I demonstrate
large economic gains to investors using deep learning forecasts. The results of
my comparative experiments highlight the importance of domain knowledge and
financial theory when designing deep learning models. I also show return
prediction tasks bring new challenges to deep learning. The time varying
distribution causes distribution shift problem, which is essential for
financial time series prediction. I demonstrate that deep learning methods can
improve asset risk premium measurement. Due to the booming deep learning
studies, they can constantly promote the study of underlying financial
mechanisms behind asset pricing. I also propose a promising research method
that learning from data and figuring out the underlying economic mechanisms
through explainable artificial intelligence (AI) methods. My findings not only
justify the value of deep learning in blooming fintech development, but also
highlight their prospects and advantages over traditional machine learning
methods.",Chen Zhang,2022,http://arxiv.org/abs/2209.12014v1
"Adaptivity of deep ReLU network for learning in Besov and mixed smooth
  Besov spaces: optimal rate and curse of dimensionality","Deep learning has shown high performances in various types of tasks from
visual recognition to natural language processing, which indicates superior
flexibility and adaptivity of deep learning. To understand this phenomenon
theoretically, we develop a new approximation and estimation error analysis of
deep learning with the ReLU activation for functions in a Besov space and its
variant with mixed smoothness. The Besov space is a considerably general
function space including the Holder space and Sobolev space, and especially can
capture spatial inhomogeneity of smoothness. Through the analysis in the Besov
space, it is shown that deep learning can achieve the minimax optimal rate and
outperform any non-adaptive (linear) estimator such as kernel ridge regression,
which shows that deep learning has higher adaptivity to the spatial
inhomogeneity of the target function than other estimators such as linear ones.
In addition to this, it is shown that deep learning can avoid the curse of
dimensionality if the target function is in a mixed smooth Besov space. We also
show that the dependency of the convergence rate on the dimensionality is tight
due to its minimax optimality. These results support high adaptivity of deep
learning and its superior ability as a feature extractor.",Taiji Suzuki,2018,http://arxiv.org/abs/1810.08033v1
Fairness in Deep Learning: A Computational Perspective,"Deep learning is increasingly being used in high-stake decision making
applications that affect individual lives. However, deep learning models might
exhibit algorithmic discrimination behaviors with respect to protected groups,
potentially posing negative impacts on individuals and society. Therefore,
fairness in deep learning has attracted tremendous attention recently. We
provide a review covering recent progresses to tackle algorithmic fairness
problems of deep learning from the computational perspective. Specifically, we
show that interpretability can serve as a useful ingredient to diagnose the
reasons that lead to algorithmic discrimination. We also discuss fairness
mitigation approaches categorized according to three stages of deep learning
life-cycle, aiming to push forward the area of fairness in deep learning and
build genuinely fair and reliable deep learning systems.","Mengnan Du, Fan Yang, Na Zou, Xia Hu",2019,http://arxiv.org/abs/1908.08843v2
The many faces of deep learning,"Deep learning has sparked a network of mutual interactions between different
disciplines and AI. Naturally, each discipline focuses and interprets the
workings of deep learning in different ways. This diversity of perspectives on
deep learning, from neuroscience to statistical physics, is a rich source of
inspiration that fuels novel developments in the theory and applications of
machine learning. In this perspective, we collect and synthesize different
intuitions scattered across several communities as for how deep learning works.
In particular, we will briefly discuss the different perspectives that
disciplines across mathematics, physics, computation, and neuroscience take on
how deep learning does its tricks. Our discussion on each perspective is
necessarily shallow due to the multiple views that had to be covered. The
deepness in this case should come from putting all these faces of deep learning
together in the reader's mind, so that one can look at the same problem from
different angles.",Raul Vicente,2019,http://arxiv.org/abs/1908.10206v1
Deep Active Learning by Leveraging Training Dynamics,"Active learning theories and methods have been extensively studied in
classical statistical learning settings. However, deep active learning, i.e.,
active learning with deep learning models, is usually based on empirical
criteria without solid theoretical justification, thus suffering from heavy
doubts when some of those fail to provide benefits in real applications. In
this paper, by exploring the connection between the generalization performance
and the training dynamics, we propose a theory-driven deep active learning
method (dynamicAL) which selects samples to maximize training dynamics. In
particular, we prove that the convergence speed of training and the
generalization performance are positively correlated under the ultra-wide
condition and show that maximizing the training dynamics leads to better
generalization performance. Furthermore, to scale up to large deep neural
networks and data sets, we introduce two relaxations for the subset selection
problem and reduce the time complexity from polynomial to constant. Empirical
results show that dynamicAL not only outperforms the other baselines
consistently but also scales well on large deep learning models. We hope our
work would inspire more attempts on bridging the theoretical findings of deep
networks and practical impacts of deep active learning in real applications.","Haonan Wang, Wei Huang, Ziwei Wu, Andrew Margenot, Hanghang Tong, Jingrui He",2021,http://arxiv.org/abs/2110.08611v2
DeepSI: Interactive Deep Learning for Semantic Interaction,"In this paper, we design novel interactive deep learning methods to improve
semantic interactions in visual analytics applications. The ability of semantic
interaction to infer analysts' precise intents during sensemaking is dependent
on the quality of the underlying data representation. We propose the
$\text{DeepSI}_{\text{finetune}}$ framework that integrates deep learning into
the human-in-the-loop interactive sensemaking pipeline, with two important
properties. First, deep learning extracts meaningful representations from raw
data, which improves semantic interaction inference. Second, semantic
interactions are exploited to fine-tune the deep learning representations,
which then further improves semantic interaction inference. This feedback loop
between human interaction and deep learning enables efficient learning of user-
and task-specific representations. To evaluate the advantage of embedding the
deep learning within the semantic interaction loop, we compare
$\text{DeepSI}_{\text{finetune}}$ against a state-of-the-art but more basic use
of deep learning as only a feature extractor pre-processed outside of the
interactive loop. Results of two complementary studies, a human-centered
qualitative case study and an algorithm-centered simulation-based quantitative
experiment, show that $\text{DeepSI}_{\text{finetune}}$ more accurately
captures users' complex mental models with fewer interactions.","Yali Bian, Chris North",2023,http://arxiv.org/abs/2305.18357v1
"Emerging Threats in Deep Learning-Based Autonomous Driving: A
  Comprehensive Survey","Since the 2004 DARPA Grand Challenge, the autonomous driving technology has
witnessed nearly two decades of rapid development. Particularly, in recent
years, with the application of new sensors and deep learning technologies
extending to the autonomous field, the development of autonomous driving
technology has continued to make breakthroughs. Thus, many carmakers and
high-tech giants dedicated to research and system development of autonomous
driving. However, as the foundation of autonomous driving, the deep learning
technology faces many new security risks. The academic community has proposed
deep learning countermeasures against the adversarial examples and AI backdoor,
and has introduced them into the autonomous driving field for verification.
Deep learning security matters to autonomous driving system security, and then
matters to personal safety, which is an issue that deserves attention and
research.This paper provides an summary of the concepts, developments and
recent research in deep learning security technologies in autonomous driving.
Firstly, we briefly introduce the deep learning framework and pipeline in the
autonomous driving system, which mainly include the deep learning technologies
and algorithms commonly used in this field. Moreover, we focus on the potential
security threats of the deep learning based autonomous driving system in each
functional layer in turn. We reviews the development of deep learning attack
technologies to autonomous driving, investigates the State-of-the-Art
algorithms, and reveals the potential risks. At last, we provides an outlook on
deep learning security in the autonomous driving field and proposes
recommendations for building a safe and trustworthy autonomous driving system.","Hui Cao, Wenlong Zou, Yinkun Wang, Ting Song, Mengjun Liu",2022,http://arxiv.org/abs/2210.11237v1
"Deep Recurrent Convolutional Neural Network: Improving Performance For
  Speech Recognition","A deep learning approach has been widely applied in sequence modeling
problems. In terms of automatic speech recognition (ASR), its performance has
significantly been improved by increasing large speech corpus and deeper neural
network. Especially, recurrent neural network and deep convolutional neural
network have been applied in ASR successfully. Given the arising problem of
training speed, we build a novel deep recurrent convolutional network for
acoustic modeling and then apply deep residual learning to it. Our experiments
show that it has not only faster convergence speed but better recognition
accuracy over traditional deep convolutional recurrent network. In the
experiments, we compare the convergence speed of our novel deep recurrent
convolutional networks and traditional deep convolutional recurrent networks.
With faster convergence speed, our novel deep recurrent convolutional networks
can reach the comparable performance. We further show that applying deep
residual learning can boost the convergence speed of our novel deep recurret
convolutional networks. Finally, we evaluate all our experimental networks by
phoneme error rate (PER) with our proposed bidirectional statistical n-gram
language model. Our evaluation results show that our newly proposed deep
recurrent convolutional network applied with deep residual learning can reach
the best PER of 17.33\% with the fastest convergence speed on TIMIT database.
The outstanding performance of our novel deep recurrent convolutional neural
network with deep residual learning indicates that it can be potentially
adopted in other sequential problems.","Zewang Zhang, Zheng Sun, Jiaqi Liu, Jingwen Chen, Zhao Huo, Xiao Zhang",2016,http://arxiv.org/abs/1611.07174v2
Geometric Understanding of Deep Learning,"Deep learning is the mainstream technique for many machine learning tasks,
including image recognition, machine translation, speech recognition, and so
on. It has outperformed conventional methods in various fields and achieved
great successes. Unfortunately, the understanding on how it works remains
unclear. It has the central importance to lay down the theoretic foundation for
deep learning.
  In this work, we give a geometric view to understand deep learning: we show
that the fundamental principle attributing to the success is the manifold
structure in data, namely natural high dimensional data concentrates close to a
low-dimensional manifold, deep learning learns the manifold and the probability
distribution on it.
  We further introduce the concepts of rectified linear complexity for deep
neural network measuring its learning capability, rectified linear complexity
of an embedding manifold describing the difficulty to be learned. Then we show
for any deep neural network with fixed architecture, there exists a manifold
that cannot be learned by the network. Finally, we propose to apply optimal
mass transportation theory to control the probability distribution in the
latent space.","Na Lei, Zhongxuan Luo, Shing-Tung Yau, David Xianfeng Gu",2018,http://arxiv.org/abs/1805.10451v2
Recent Advances in Deep Learning: An Overview,"Deep Learning is one of the newest trends in Machine Learning and Artificial
Intelligence research. It is also one of the most popular scientific research
trends now-a-days. Deep learning methods have brought revolutionary advances in
computer vision and machine learning. Every now and then, new and new deep
learning techniques are being born, outperforming state-of-the-art machine
learning and even existing deep learning techniques. In recent years, the world
has seen many major breakthroughs in this field. Since deep learning is
evolving at a huge speed, its kind of hard to keep track of the regular
advances especially for new researchers. In this paper, we are going to briefly
discuss about recent advances in Deep Learning for past few years.","Matiur Rahman Minar, Jibon Naher",2018,http://arxiv.org/abs/1807.08169v1
A Survey on Deep Learning for Skeleton-Based Human Animation,"Human character animation is often critical in entertainment content
production, including video games, virtual reality or fiction films. To this
end, deep neural networks drive most recent advances through deep learning and
deep reinforcement learning. In this article, we propose a comprehensive survey
on the state-of-the-art approaches based on either deep learning or deep
reinforcement learning in skeleton-based human character animation. First, we
introduce motion data representations, most common human motion datasets and
how basic deep models can be enhanced to foster learning of spatial and
temporal patterns in motion data. Second, we cover state-of-the-art approaches
divided into three large families of applications in human animation pipelines:
motion synthesis, character control and motion editing. Finally, we discuss the
limitations of the current state-of-the-art methods based on deep learning
and/or deep reinforcement learning in skeletal human character animation and
possible directions of future research to alleviate current limitations and
meet animators' needs.","L. Mourot, L. Hoyet, F. Le Clerc, François Schnitzler, Pierre Hellier",2021,http://arxiv.org/abs/2110.06901v2
Deep Meta-Learning: Learning to Learn in the Concept Space,"Few-shot learning remains challenging for meta-learning that learns a
learning algorithm (meta-learner) from many related tasks. In this work, we
argue that this is due to the lack of a good representation for meta-learning,
and propose deep meta-learning to integrate the representation power of deep
learning into meta-learning. The framework is composed of three modules, a
concept generator, a meta-learner, and a concept discriminator, which are
learned jointly. The concept generator, e.g. a deep residual net, extracts a
representation for each instance that captures its high-level concept, on which
the meta-learner performs few-shot learning, and the concept discriminator
recognizes the concepts. By learning to learn in the concept space rather than
in the complicated instance space, deep meta-learning can substantially improve
vanilla meta-learning, which is demonstrated on various few-shot image
recognition problems. For example, on 5-way-1-shot image recognition on
CIFAR-100 and CUB-200, it improves Matching Nets from 50.53% and 56.53% to
58.18% and 63.47%, improves MAML from 49.28% and 50.45% to 56.65% and 64.63%,
and improves Meta-SGD from 53.83% and 53.34% to 61.62% and 66.95%,
respectively.","Fengwei Zhou, Bin Wu, Zhenguo Li",2018,http://arxiv.org/abs/1802.03596v1
"In-Machine-Learning Database: Reimagining Deep Learning with Old-School
  SQL","In-database machine learning has been very popular, almost being a cliche.
However, can we do it the other way around? In this work, we say ""yes"" by
applying plain old SQL to deep learning, in a sense implementing deep learning
algorithms with SQL. Most deep learning frameworks, as well as generic machine
learning ones, share a de facto standard of multidimensional array operations,
underneath fancier infrastructure such as automatic differentiation. As SQL
tables can be regarded as generalisations of (multi-dimensional) arrays, we
have found a way to express common deep learning operations in SQL, encouraging
a different way of thinking and thus potentially novel models. In particular,
one of the latest trend in deep learning was the introduction of sparsity in
the name of graph convolutional networks, whereas we take sparsity almost for
granted in the database world. As both databases and machine learning involve
transformation of datasets, we hope this work can inspire further works
utilizing the large body of existing wisdom, algorithms and technologies in the
database field to advance the state of the art in machine learning, rather than
merely integerating machine learning into databases.",Len Du,2020,http://arxiv.org/abs/2004.05366v2
A Survey on Ensemble Learning under the Era of Deep Learning,"Due to the dominant position of deep learning (mostly deep neural networks)
in various artificial intelligence applications, recently, ensemble learning
based on deep neural networks (ensemble deep learning) has shown significant
performances in improving the generalization of learning system. However, since
modern deep neural networks usually have millions to billions of parameters,
the time and space overheads for training multiple base deep learners and
testing with the ensemble deep learner are far greater than that of traditional
ensemble learning. Though several algorithms of fast ensemble deep learning
have been proposed to promote the deployment of ensemble deep learning in some
applications, further advances still need to be made for many applications in
specific fields, where the developing time and computing resources are usually
restricted or the data to be processed is of large dimensionality. An urgent
problem needs to be solved is how to take the significant advantages of
ensemble deep learning while reduce the required expenses so that many more
applications in specific fields can benefit from it. For the alleviation of
this problem, it is essential to know about how ensemble learning has developed
under the era of deep learning. Thus, in this article, we present fundamental
discussions focusing on data analyses of published works, methodologies, recent
advances and unattainability of traditional ensemble learning and ensemble deep
learning. We hope this article will be helpful to realize the intrinsic
problems and technical challenges faced by future developments of ensemble
learning under the era of deep learning.","Yongquan Yang, Haijun Lv, Ning Chen",2021,http://arxiv.org/abs/2101.08387v6
A Survey on State-of-the-art Deep Learning Applications and Challenges,"Deep learning, a branch of artificial intelligence, is a data-driven method
that uses multiple layers of interconnected units or neurons to learn intricate
patterns and representations directly from raw input data. Empowered by this
learning capability, it has become a powerful tool for solving complex problems
and is the core driver of many groundbreaking technologies and innovations.
Building a deep learning model is challenging due to the algorithm's complexity
and the dynamic nature of real-world problems. Several studies have reviewed
deep learning concepts and applications. However, the studies mostly focused on
the types of deep learning models and convolutional neural network
architectures, offering limited coverage of the state-of-the-art deep learning
models and their applications in solving complex problems across different
domains. Therefore, motivated by the limitations, this study aims to
comprehensively review the state-of-the-art deep learning models in computer
vision, natural language processing, time series analysis and pervasive
computing, and robotics. We highlight the key features of the models and their
effectiveness in solving the problems within each domain. Furthermore, this
study presents the fundamentals of deep learning, various deep learning model
types and prominent convolutional neural network architectures. Finally,
challenges and future directions in deep learning research are discussed to
offer a broader perspective for future researchers.","Mohd Halim Mohd Noor, Ayokunle Olalekan Ige",2024,http://arxiv.org/abs/2403.17561v6
A Survey of Deep Learning Based Software Refactoring,"Refactoring is one of the most important activities in software engineering
which is used to improve the quality of a software system. With the advancement
of deep learning techniques, researchers are attempting to apply deep learning
techniques to software refactoring. Consequently, dozens of deep learning-based
refactoring approaches have been proposed. However, there is a lack of
comprehensive reviews on such works as well as a taxonomy for deep
learning-based refactoring. To this end, in this paper, we present a survey on
deep learning-based software refactoring. We classify related works into five
categories according to the major tasks they cover. Among these categories, we
further present key aspects (i.e., code smell types, refactoring types,
training strategies, and evaluation) to give insight into the details of the
technologies that have supported refactoring through deep learning. The
classification indicates that there is an imbalance in the adoption of deep
learning techniques for the process of refactoring. Most of the deep learning
techniques have been used for the detection of code smells and the
recommendation of refactoring solutions as found in 56.25\% and 33.33\% of the
literature respectively. In contrast, only 6.25\% and 4.17\% were towards the
end-to-end code transformation as refactoring and the mining of refactorings,
respectively. Notably, we found no literature representation for the quality
assurance for refactoring. We also observe that most of the deep learning
techniques have been used to support refactoring processes occurring at the
method level whereas classes and variables attracted minimal attention.
Finally, we discuss the challenges and limitations associated with the
employment of deep learning-based refactorings and present some potential
research opportunities for future work.","Bridget Nyirongo, Yanjie Jiang, He Jiang, Hui Liu",2024,http://arxiv.org/abs/2404.19226v1
Probabilistic Generative Deep Learning for Molecular Design,"Probabilistic generative deep learning for molecular design involves the
discovery and design of new molecules and analysis of their structure,
properties and activities by probabilistic generative models using the deep
learning approach. It leverages the existing huge databases and publications of
experimental results, and quantum-mechanical calculations, to learn and explore
molecular structure, properties and activities. We discuss the major components
of probabilistic generative deep learning for molecular design, which include
molecular structure, molecular representations, deep generative models,
molecular latent representations and latent space, molecular structure-property
and structure-activity relationships, molecular similarity and molecular
design. We highlight significant recent work using or applicable to this new
approach.",Daniel T. Chang,2019,http://arxiv.org/abs/1902.05148v1
MinCall - MinION end2end convolutional deep learning basecaller,"The Oxford Nanopore Technologies's MinION is the first portable DNA
sequencing device. It is capable of producing long reads, over 100 kBp were
reported. However, it has significantly higher error rate than other methods.
In this study, we present MinCall, an end2end basecaller model for the MinION.
The model is based on deep learning and uses convolutional neural networks
(CNN) in its implementation. For extra performance, it uses cutting edge deep
learning techniques and architectures, batch normalization and Connectionist
Temporal Classification (CTC) loss. The best performing deep learning model
achieves 91.4% median match rate on E. Coli dataset using R9 pore chemistry and
1D reads.","Neven Miculinić, Marko Ratković, Mile Šikić",2019,http://arxiv.org/abs/1904.10337v1
Model Complexity of Deep Learning: A Survey,"Model complexity is a fundamental problem in deep learning. In this paper we
conduct a systematic overview of the latest studies on model complexity in deep
learning. Model complexity of deep learning can be categorized into expressive
capacity and effective model complexity. We review the existing studies on
those two categories along four important factors, including model framework,
model size, optimization process and data complexity. We also discuss the
applications of deep learning model complexity including understanding model
generalization, model optimization, and model selection and design. We conclude
by proposing several interesting future directions.","Xia Hu, Lingyang Chu, Jian Pei, Weiqing Liu, Jiang Bian",2021,http://arxiv.org/abs/2103.05127v2
"PGN: A perturbation generation network against deep reinforcement
  learning","Deep reinforcement learning has advanced greatly and applied in many areas.
In this paper, we explore the vulnerability of deep reinforcement learning by
proposing a novel generative model for creating effective adversarial examples
to attack the agent. Our proposed model can achieve both targeted attacks and
untargeted attacks. Considering the specificity of deep reinforcement learning,
we propose the action consistency ratio as a measure of stealthiness, and a new
measurement index of effectiveness and stealthiness. Experiment results show
that our method can ensure the effectiveness and stealthiness of attack
compared with other algorithms. Moreover, our methods are considerably faster
and thus can achieve rapid and efficient verification of the vulnerability of
deep reinforcement learning.","Xiangjuan Li, Feifan Li, Yang Li, Quan Pan",2023,http://arxiv.org/abs/2312.12904v1
Stochastic Variational Deep Kernel Learning,"Deep kernel learning combines the non-parametric flexibility of kernel
methods with the inductive biases of deep learning architectures. We propose a
novel deep kernel learning model and stochastic variational inference procedure
which generalizes deep kernel learning approaches to enable classification,
multi-task learning, additive covariance structures, and stochastic gradient
training. Specifically, we apply additive base kernels to subsets of output
features from deep neural architectures, and jointly learn the parameters of
the base kernels and deep network through a Gaussian process marginal
likelihood objective. Within this framework, we derive an efficient form of
stochastic variational inference which leverages local kernel interpolation,
inducing points, and structure exploiting algebra. We show improved performance
over stand alone deep networks, SVMs, and state of the art scalable Gaussian
processes on several classification benchmarks, including an airline delay
dataset containing 6 million training points, CIFAR, and ImageNet.","Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing",2016,http://arxiv.org/abs/1611.00336v2
"Deep Learning and Its Applications to Machine Health Monitoring: A
  Survey","Since 2006, deep learning (DL) has become a rapidly growing research
direction, redefining state-of-the-art performances in a wide range of areas
such as object recognition, image segmentation, speech recognition and machine
translation. In modern manufacturing systems, data-driven machine health
monitoring is gaining in popularity due to the widespread deployment of
low-cost sensors and their connection to the Internet. Meanwhile, deep learning
provides useful tools for processing and analyzing these big machinery data.
The main purpose of this paper is to review and summarize the emerging research
work of deep learning on machine health monitoring. After the brief
introduction of deep learning techniques, the applications of deep learning in
machine health monitoring systems are reviewed mainly from the following
aspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and
its variants including Deep Belief Network (DBN) and Deep Boltzmann Machines
(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).
Finally, some new trends of DL-based machine health monitoring methods are
discussed.","Rui Zhao, Ruqiang Yan, Zhenghua Chen, Kezhi Mao, Peng Wang, Robert X. Gao",2016,http://arxiv.org/abs/1612.07640v1
"The Foundations of Deep Learning with a Path Towards General
  Intelligence","Like any field of empirical science, AI may be approached axiomatically. We
formulate requirements for a general-purpose, human-level AI system in terms of
postulates. We review the methodology of deep learning, examining the explicit
and tacit assumptions in deep learning research. Deep Learning methodology
seeks to overcome limitations in traditional machine learning research as it
combines facets of model richness, generality, and practical applicability. The
methodology so far has produced outstanding results due to a productive synergy
of function approximation, under plausible assumptions of irreducibility and
the efficiency of back-propagation family of algorithms. We examine these
winning traits of deep learning, and also observe the various known failure
modes of deep learning. We conclude by giving recommendations on how to extend
deep learning methodology to cover the postulates of general-purpose AI
including modularity, and cognitive architecture. We also relate deep learning
to advances in theoretical neuroscience research.",Eray Özkural,2018,http://arxiv.org/abs/1806.08874v1
"Sequential Interpretability: Methods, Applications, and Future Direction
  for Understanding Deep Learning Models in the Context of Sequential Data","Deep learning continues to revolutionize an ever-growing number of critical
application areas including healthcare, transportation, finance, and basic
sciences. Despite their increased predictive power, model transparency and
human explainability remain a significant challenge due to the ""black box""
nature of modern deep learning models. In many cases the desired balance
between interpretability and performance is predominately task specific.
Human-centric domains such as healthcare necessitate a renewed focus on
understanding how and why these frameworks are arriving at critical and
potentially life-or-death decisions. Given the quantity of research and
empirical successes of deep learning for computer vision, most of the existing
interpretability research has focused on image processing techniques.
Comparatively, less attention has been paid to interpreting deep learning
frameworks using sequential data. Given recent deep learning advancements in
highly sequential domains such as natural language processing and physiological
signal processing, the need for deep sequential explanations is at an all-time
high. In this paper, we review current techniques for interpreting deep
learning techniques involving sequential data, identify similarities to
non-sequential methods, and discuss current limitations and future avenues of
sequential interpretability research.","Benjamin Shickel, Parisa Rashidi",2020,http://arxiv.org/abs/2004.12524v1
Adding Context to Source Code Representations for Deep Learning,"Deep learning models have been successfully applied to a variety of software
engineering tasks, such as code classification, summarisation, and bug and
vulnerability detection. In order to apply deep learning to these tasks, source
code needs to be represented in a format that is suitable for input into the
deep learning model. Most approaches to representing source code, such as
tokens, abstract syntax trees (ASTs), data flow graphs (DFGs), and control flow
graphs (CFGs) only focus on the code itself and do not take into account
additional context that could be useful for deep learning models. In this
paper, we argue that it is beneficial for deep learning models to have access
to additional contextual information about the code being analysed. We present
preliminary evidence that encoding context from the call hierarchy along with
information from the code itself can improve the performance of a
state-of-the-art deep learning model for two software engineering tasks. We
outline our research agenda for adding further contextual information to source
code representations for deep learning.","Fuwei Tian, Christoph Treude",2022,http://arxiv.org/abs/2208.00203v1
A Review of the Convergence of 5G/6G Architecture and Deep Learning,"The convergence of 5G architecture and deep learning has gained a lot of
research interests in both the fields of wireless communication and artificial
intelligence. This is because deep learning technologies have been identified
to be the potential driver of the 5G technologies, that make up the 5G
architecture. Hence, there have been extensive surveys on the convergence of 5G
architecture and deep learning. However, most of the existing survey papers
mainly focused on how deep learning can converge with a specific 5G technology,
thus, not covering the full spectrum of the 5G architecture. Although there is
a recent survey paper that appears to be robust, a review of that paper shows
that it is not well structured to specifically cover the convergence of deep
learning and the 5G technologies. Hence, this paper provides a robust overview
of the convergence of the key 5G technologies and deep learning. The challenges
faced by such convergence are discussed. In addition, a brief overview of the
future 6G architecture, and how it can converge with deep learning is also
discussed.","Olusola T. Odeyomi, Olubiyi O. Akintade, Temitayo O. Olowu, Gergely Zaruba",2022,http://arxiv.org/abs/2208.07643v1
Deep-learning-powered data analysis in plankton ecology,"The implementation of deep learning algorithms has brought new perspectives
to plankton ecology. Emerging as an alternative approach to established
methods, deep learning offers objective schemes to investigate plankton
organisms in diverse environments. We provide an overview of
deep-learning-based methods including detection and classification of phyto-
and zooplankton images, foraging and swimming behaviour analysis, and finally
ecological modelling. Deep learning has the potential to speed up the analysis
and reduce the human experimental bias, thus enabling data acquisition at
relevant temporal and spatial scales with improved reproducibility. We also
discuss shortcomings and show how deep learning architectures have evolved to
mitigate imprecise readouts. Finally, we suggest opportunities where deep
learning is particularly likely to catalyze plankton research. The examples are
accompanied by detailed tutorials and code samples that allow readers to apply
the methods described in this review to their own data.","Harshith Bachimanchi, Matthew I. M. Pinder, Chloé Robert, Pierre De Wit, Jonathan Havenhand, Alexandra Kinnby, Daniel Midtvedt, Erik Selander, Giovanni Volpe",2023,http://arxiv.org/abs/2309.08500v1
"Active Learning with Neural Networks: Insights from Nonparametric
  Statistics","Deep neural networks have great representation power, but typically require
large numbers of training examples. This motivates deep active learning methods
that can significantly reduce the amount of labeled training data. Empirical
successes of deep active learning have been recently reported in the
literature, however, rigorous label complexity guarantees of deep active
learning have remained elusive. This constitutes a significant gap between
theory and practice. This paper tackles this gap by providing the first
near-optimal label complexity guarantees for deep active learning. The key
insight is to study deep active learning from the nonparametric classification
perspective. Under standard low noise conditions, we show that active learning
with neural networks can provably achieve the minimax label complexity, up to
disagreement coefficient and other logarithmic terms. When equipped with an
abstention option, we further develop an efficient deep active learning
algorithm that achieves $\mathsf{polylog}(\frac{1}{\epsilon})$ label
complexity, without any low noise assumptions. We also provide extensions of
our results beyond the commonly studied Sobolev/H\""older spaces and develop
label complexity guarantees for learning in Radon $\mathsf{BV}^2$ spaces, which
have recently been proposed as natural function spaces associated with neural
networks.","Yinglun Zhu, Robert Nowak",2022,http://arxiv.org/abs/2210.08367v1
"Deep Causal Learning: Representation, Discovery and Inference","Causal learning has garnered significant attention in recent years because it
reveals the essential relationships that underpin phenomena and delineates the
mechanisms by which the world evolves. Nevertheless, traditional causal
learning methods face numerous challenges and limitations, including
high-dimensional, unstructured variables, combinatorial optimization problems,
unobserved confounders, selection biases, and estimation inaccuracies. Deep
causal learning, which leverages deep neural networks, offers innovative
insights and solutions for addressing these challenges. Although numerous deep
learning-based methods for causal discovery and inference have been proposed,
there remains a dearth of reviews examining the underlying mechanisms by which
deep learning can enhance causal learning. In this article, we comprehensively
review how deep learning can contribute to causal learning by tackling
traditional challenges across three key dimensions: representation, discovery,
and inference. We emphasize that deep causal learning is pivotal for advancing
the theoretical frontiers and broadening the practical applications of causal
science. We conclude by summarizing open issues and outlining potential
directions for future research.","Zizhen Deng, Xiaolong Zheng, Hu Tian, Daniel Dajun Zeng",2022,http://arxiv.org/abs/2211.03374v2
Boosting Deep Ensembles with Learning Rate Tuning,"The Learning Rate (LR) has a high impact on deep learning training
performance. A common practice is to train a Deep Neural Network (DNN) multiple
times with different LR policies to find the optimal LR policy, which has been
widely recognized as a daunting and costly task. Moreover, multiple times of
DNN training has not been effectively utilized. In practice, often only the
optimal LR is adopted, which misses the opportunities to further enhance the
overall accuracy of the deep learning system and results in a huge waste of
both computing resources and training time. This paper presents a novel
framework, LREnsemble, to effectively leverage effective learning rate tuning
to boost deep ensemble performance. We make three original contributions.
First, we show that the LR tuning with different LR policies can produce highly
diverse DNNs, which can be supplied as base models for deep ensembles. Second,
we leverage different ensemble selection algorithms to identify high-quality
deep ensembles from the large pool of base models with significant accuracy
improvements over the best single base model. Third, we propose LREnsemble, a
framework that utilizes the synergy of LR tuning and deep ensemble techniques
to enhance deep learning performance. The experiments on multiple benchmark
datasets have demonstrated the effectiveness of LREnsemble, generating up to
2.34% accuracy improvements over well-optimized baselines.","Hongpeng Jin, Yanzhao Wu",2024,http://arxiv.org/abs/2410.07564v1
Faster Deep Q-learning using Neural Episodic Control,"The research on deep reinforcement learning which estimates Q-value by deep
learning has been attracted the interest of researchers recently. In deep
reinforcement learning, it is important to efficiently learn the experiences
that an agent has collected by exploring environment. We propose NEC2DQN that
improves learning speed of a poor sample efficiency algorithm such as DQN by
using good one such as NEC at the beginning of learning. We show it is able to
learn faster than Double DQN or N-step DQN in the experiments of Pong.","Daichi Nishio, Satoshi Yamane",2018,http://arxiv.org/abs/1801.01968v4
Knowledge-augmented Column Networks: Guiding Deep Learning with Advice,"Recently, deep models have had considerable success in several tasks,
especially with low-level representations. However, effective learning from
sparse noisy samples is a major challenge in most deep models, especially in
domains with structured representations. Inspired by the proven success of
human guided machine learning, we propose Knowledge-augmented Column Networks,
a relational deep learning framework that leverages human advice/knowledge to
learn better models in presence of sparsity and systematic noise.","Mayukh Das, Devendra Singh Dhami, Yang Yu, Gautam Kunapuli, Sriraam Natarajan",2019,http://arxiv.org/abs/1906.01432v1
Reinforcement Learning and Video Games,"Reinforcement learning has exceeded human-level performance in game playing
AI with deep learning methods according to the experiments from DeepMind on Go
and Atari games. Deep learning solves high dimension input problems which stop
the development of reinforcement for many years. This study uses both two
techniques to create several agents with different algorithms that successfully
learn to play T-rex Runner. Deep Q network algorithm and three types of
improvements are implemented to train the agent. The results from some of them
are far from satisfactory but others are better than human experts. Batch
normalization is a method to solve internal covariate shift problems in deep
neural network. The positive influence of this on reinforcement learning has
also been proved in this study.",Yue Zheng,2019,http://arxiv.org/abs/1909.04751v1
"Poisoning Deep Reinforcement Learning Agents with In-Distribution
  Triggers","In this paper, we propose a new data poisoning attack and apply it to deep
reinforcement learning agents. Our attack centers on what we call
in-distribution triggers, which are triggers native to the data distributions
the model will be trained on and deployed in. We outline a simple procedure for
embedding these, and other, triggers in deep reinforcement learning agents
following a multi-task learning paradigm, and demonstrate in three common
reinforcement learning environments. We believe that this work has important
implications for the security of deep learning models.","Chace Ashcraft, Kiran Karra",2021,http://arxiv.org/abs/2106.07798v1
Deep Online Learning with Stochastic Constraints,"Deep learning models are considered to be state-of-the-art in many offline
machine learning tasks. However, many of the techniques developed are not
suitable for online learning tasks. The problem of using deep learning models
with sequential data becomes even harder when several loss functions need to be
considered simultaneously, as in many real-world applications. In this paper,
we, therefore, propose a novel online deep learning training procedure which
can be used regardless of the neural network's architecture, aiming to deal
with the multiple objectives case. We demonstrate and show the effectiveness of
our algorithm on the Neyman-Pearson classification problem on several benchmark
datasets.",Guy Uziel,2019,http://arxiv.org/abs/1905.10817v1
DOC3-Deep One Class Classification using Contradictions,"This paper introduces the notion of learning from contradictions (a.k.a
Universum learning) for deep one class classification problems. We formalize
this notion for the widely adopted one class large-margin loss, and propose the
Deep One Class Classification using Contradictions (DOC3) algorithm. We show
that learning from contradictions incurs lower generalization error by
comparing the Empirical Rademacher Complexity (ERC) of DOC3 against its
traditional inductive learning counterpart. Our empirical results demonstrate
the efficacy of DOC3 compared to popular baseline algorithms on several
real-life data sets.","Sauptik Dhar, Bernardo Gonzalez Torres",2021,http://arxiv.org/abs/2105.07636v2
Security of Deep Learning Methodologies: Challenges and Opportunities,"Despite the plethora of studies about security vulnerabilities and defenses
of deep learning models, security aspects of deep learning methodologies, such
as transfer learning, have been rarely studied. In this article, we highlight
the security challenges and research opportunities of these methodologies,
focusing on vulnerabilities and attacks unique to them.","Shahbaz Rezaei, Xin Liu",2019,http://arxiv.org/abs/1912.03735v1
"Pre-training with Non-expert Human Demonstration for Deep Reinforcement
  Learning","Deep reinforcement learning (deep RL) has achieved superior performance in
complex sequential tasks by using deep neural networks as function
approximators to learn directly from raw input images. However, learning
directly from raw images is data inefficient. The agent must learn feature
representation of complex states in addition to learning a policy. As a result,
deep RL typically suffers from slow learning speeds and often requires a
prohibitively large amount of training time and data to reach reasonable
performance, making it inapplicable to real-world settings where data is
expensive. In this work, we improve data efficiency in deep RL by addressing
one of the two learning goals, feature learning. We leverage supervised
learning to pre-train on a small set of non-expert human demonstrations and
empirically evaluate our approach using the asynchronous advantage actor-critic
algorithms (A3C) in the Atari domain. Our results show significant improvements
in learning speed, even when the provided demonstration is noisy and of low
quality.","Gabriel V. de la Cruz, Yunshu Du, Matthew E. Taylor",2018,http://arxiv.org/abs/1812.08904v1
"Low-Shot Classification: A Comparison of Classical and Deep Transfer
  Machine Learning Approaches","Despite the recent success of deep transfer learning approaches in NLP, there
is a lack of quantitative studies demonstrating the gains these models offer in
low-shot text classification tasks over existing paradigms. Deep transfer
learning approaches such as BERT and ULMFiT demonstrate that they can beat
state-of-the-art results on larger datasets, however when one has only 100-1000
labelled examples per class, the choice of approach is less clear, with
classical machine learning and deep transfer learning representing valid
options. This paper compares the current best transfer learning approach with
top classical machine learning approaches on a trinary sentiment classification
task to assess the best paradigm. We find that BERT, representing the best of
deep transfer learning, is the best performing approach, outperforming top
classical machine learning algorithms by 9.7% on average when trained with 100
examples per class, narrowing to 1.8% at 1000 labels per class. We also show
the robustness of deep transfer learning in moving across domains, where the
maximum loss in accuracy is only 0.7% in similar domain tasks and 3.2% cross
domain, compared to classical machine learning which loses up to 20.6%.","Peter Usherwood, Steven Smit",2019,http://arxiv.org/abs/1907.07543v1
Deep Gaussian Mixture Models,"Deep learning is a hierarchical inference method formed by subsequent
multiple layers of learning able to more efficiently describe complex
relationships. In this work, Deep Gaussian Mixture Models are introduced and
discussed. A Deep Gaussian Mixture model (DGMM) is a network of multiple layers
of latent variables, where, at each layer, the variables follow a mixture of
Gaussian distributions. Thus, the deep mixture model consists of a set of
nested mixtures of linear models, which globally provide a nonlinear model able
to describe the data in a very flexible way. In order to avoid
overparameterized solutions, dimension reduction by factor models can be
applied at each layer of the architecture thus resulting in deep mixtures of
factor analysers.","Cinzia Viroli, Geoffrey J. McLachlan",2017,http://arxiv.org/abs/1711.06929v1
A Survey of Deep Learning Techniques for Mobile Robot Applications,"Advancements in deep learning over the years have attracted research into how
deep artificial neural networks can be used in robotic systems. This research
survey will present a summarization of the current research with a specific
focus on the gains and obstacles for deep learning to be applied to mobile
robotics.","Jahanzaib Shabbir, Tarique Anwer",2018,http://arxiv.org/abs/1803.07608v1
"Pre-training Neural Networks with Human Demonstrations for Deep
  Reinforcement Learning","Deep reinforcement learning (deep RL) has achieved superior performance in
complex sequential tasks by using a deep neural network as its function
approximator and by learning directly from raw images. A drawback of using raw
images is that deep RL must learn the state feature representation from the raw
images in addition to learning a policy. As a result, deep RL can require a
prohibitively large amount of training time and data to reach reasonable
performance, making it difficult to use deep RL in real-world applications,
especially when data is expensive. In this work, we speed up training by
addressing half of what deep RL is trying to solve --- learning features. Our
approach is to learn some of the important features by pre-training deep RL
network's hidden layers via supervised learning using a small set of human
demonstrations. We empirically evaluate our approach using deep Q-network (DQN)
and asynchronous advantage actor-critic (A3C) algorithms on the Atari 2600
games of Pong, Freeway, and Beamrider. Our results show that: 1) pre-training
with human demonstrations in a supervised learning manner is better at
discovering features relative to pre-training naively in DQN, and 2)
initializing a deep RL network with a pre-trained model provides a significant
improvement in training time even when pre-training from a small number of
human demonstrations.","Gabriel V. de la Cruz Jr, Yunshu Du, Matthew E. Taylor",2017,http://arxiv.org/abs/1709.04083v2
Deep Clustering: A Comprehensive Survey,"Cluster analysis plays an indispensable role in machine learning and data
mining. Learning a good data representation is crucial for clustering
algorithms. Recently, deep clustering, which can learn clustering-friendly
representations using deep neural networks, has been broadly applied in a wide
range of clustering tasks. Existing surveys for deep clustering mainly focus on
the single-view fields and the network architectures, ignoring the complex
application scenarios of clustering. To address this issue, in this paper we
provide a comprehensive survey for deep clustering in views of data sources.
With different data sources and initial conditions, we systematically
distinguish the clustering methods in terms of methodology, prior knowledge,
and architecture. Concretely, deep clustering methods are introduced according
to four categories, i.e., traditional single-view deep clustering,
semi-supervised deep clustering, deep multi-view clustering, and deep transfer
clustering. Finally, we discuss the open challenges and potential future
opportunities in different fields of deep clustering.","Yazhou Ren, Jingyu Pu, Zhimeng Yang, Jie Xu, Guofeng Li, Xiaorong Pu, Philip S. Yu, Lifang He",2022,http://arxiv.org/abs/2210.04142v1
Deep Multi-task Representation Learning: A Tensor Factorisation Approach,"Most contemporary multi-task learning methods assume linear models. This
setting is considered shallow in the era of deep learning. In this paper, we
present a new deep multi-task representation learning framework that learns
cross-task sharing structure at every layer in a deep network. Our approach is
based on generalising the matrix factorisation techniques explicitly or
implicitly used by many conventional MTL algorithms to tensor factorisation, to
realise automatic learning of end-to-end knowledge sharing in deep networks.
This is in contrast to existing deep learning approaches that need a
user-defined multi-task sharing strategy. Our approach applies to both
homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our
deep multi-task representation learning in terms of both higher accuracy and
fewer design choices.","Yongxin Yang, Timothy Hospedales",2016,http://arxiv.org/abs/1605.06391v2
ScreenerNet: Learning Self-Paced Curriculum for Deep Neural Networks,"We propose to learn a curriculum or a syllabus for supervised learning and
deep reinforcement learning with deep neural networks by an attachable deep
neural network, called ScreenerNet. Specifically, we learn a weight for each
sample by jointly training the ScreenerNet and the main network in an
end-to-end self-paced fashion. The ScreenerNet neither has sampling bias nor
requires to remember the past learning history. We show the networks augmented
with the ScreenerNet achieve early convergence with better accuracy than the
state-of-the-art curricular learning methods in extensive experiments using
three popular vision datasets such as MNIST, CIFAR10 and Pascal VOC2012, and a
Cart-pole task using Deep Q-learning. Moreover, the ScreenerNet can extend
other curriculum learning methods such as Prioritized Experience Replay (PER)
for further accuracy improvement.","Tae-Hoon Kim, Jonghyun Choi",2018,http://arxiv.org/abs/1801.00904v4
"Deep Loopy Neural Network Model for Graph Structured Data Representation
  Learning","Existing deep learning models may encounter great challenges in handling
graph structured data. In this paper, we introduce a new deep learning model
for graph data specifically, namely the deep loopy neural network.
Significantly different from the previous deep models, inside the deep loopy
neural network, there exist a large number of loops created by the extensive
connections among nodes in the input graph data, which makes model learning an
infeasible task. To resolve such a problem, in this paper, we will introduce a
new learning algorithm for the deep loopy neural network specifically. Instead
of learning the model variables based on the original model, in the proposed
learning algorithm, errors will be back-propagated through the edges in a group
of extracted spanning trees. Extensive numerical experiments have been done on
several real-world graph datasets, and the experimental results demonstrate the
effectiveness of both the proposed model and the learning algorithm in handling
graph data.",Jiawei Zhang,2018,http://arxiv.org/abs/1805.07504v2
Iterative Deep Learning Based Unbiased Stereology With Human-in-the-Loop,"Lack of enough labeled data is a major problem in building machine learning
based models when the manual annotation (labeling) is error-prone, expensive,
tedious, and time-consuming. In this paper, we introduce an iterative deep
learning based method to improve segmentation and counting of cells based on
unbiased stereology applied to regions of interest of extended depth of field
(EDF) images. This method uses an existing machine learning algorithm called
the adaptive segmentation algorithm (ASA) to generate masks (verified by a
user) for EDF images to train deep learning models. Then an iterative deep
learning approach is used to feed newly predicted and accepted deep learning
masks/images (verified by a user) to the training set of the deep learning
model. The error rate in unbiased stereology count of cells on an unseen test
set reduced from about 3 % to less than 1 % after 5 iterations of the iterative
deep learning based unbiased stereology process.","Saeed S. Alahmari, Dmitry Goldgof, Lawrence O. Hall, Palak Dave, Hady Ahmady Phoulady, Peter R. Mouton",2019,http://arxiv.org/abs/1901.04355v1
"Deep Learning for Change Detection in Remote Sensing Images:
  Comprehensive Review and Meta-Analysis","Deep learning (DL) algorithms are considered as a methodology of choice for
remote-sensing image analysis over the past few years. Due to its effective
applications, deep learning has also been introduced for automatic change
detection and achieved great success. The present study attempts to provide a
comprehensive review and a meta-analysis of the recent progress in this
subfield. Specifically, we first introduce the fundamentals of deep learning
methods which arefrequently adopted for change detection. Secondly, we present
the details of the meta-analysis conducted to examine the status of change
detection DL studies. Then, we focus on deep learning-based change detection
methodologies for remote sensing images by giving a general overview of the
existing methods. Specifically, these deep learning-based methods were
classified into three groups; fully supervised learning-based methods, fully
unsupervised learning-based methods and transfer learning-based techniques. As
a result of these investigations, promising new directions were identified for
future research. This study will contribute in several ways to our
understanding of deep learning for change detection and will provide a basis
for further research.","Lazhar Khelifi, Max Mignotte",2020,http://arxiv.org/abs/2006.05612v1
Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning,"Deep learning has achieved astonishing results on many tasks with large
amounts of data and generalization within the proximity of training data. For
many important real-world applications, these requirements are unfeasible and
additional prior knowledge on the task domain is required to overcome the
resulting problems. In particular, learning physics models for model-based
control requires robust extrapolation from fewer samples - often collected
online in real-time - and model errors may lead to drastic damages of the
system. Directly incorporating physical insight has enabled us to obtain a
novel deep model learning approach that extrapolates well while requiring fewer
samples. As a first example, we propose Deep Lagrangian Networks (DeLaN) as a
deep network structure upon which Lagrangian Mechanics have been imposed. DeLaN
can learn the equations of motion of a mechanical system (i.e., system
dynamics) with a deep network efficiently while ensuring physical plausibility.
The resulting DeLaN network performs very well at robot tracking control. The
proposed method did not only outperform previous model learning approaches at
learning speed but exhibits substantially improved and more robust
extrapolation to novel trajectories and learns online in real-time","Michael Lutter, Christian Ritter, Jan Peters",2019,http://arxiv.org/abs/1907.04490v1
Realization of spatial sparseness by deep ReLU nets with massive data,"The great success of deep learning poses urgent challenges for understanding
its working mechanism and rationality. The depth, structure, and massive size
of the data are recognized to be three key ingredients for deep learning. Most
of the recent theoretical studies for deep learning focus on the necessity and
advantages of depth and structures of neural networks. In this paper, we aim at
rigorous verification of the importance of massive data in embodying the
out-performance of deep learning. To approximate and learn spatially sparse and
smooth functions, we establish a novel sampling theorem in learning theory to
show the necessity of massive data. We then prove that implementing the
classical empirical risk minimization on some deep nets facilitates in
realization of the optimal learning rates derived in the sampling theorem. This
perhaps explains why deep learning performs so well in the era of big data.","Charles K. Chui, Shao-Bo Lin, Bo Zhang, Ding-Xuan Zhou",2019,http://arxiv.org/abs/1912.07464v1
"Deep Clustering of Tabular Data by Weighted Gaussian Distribution
  Learning","Deep learning methods are primarily proposed for supervised learning of
images or text with limited applications to clustering problems. In contrast,
tabular data with heterogeneous features pose unique challenges in
representation learning, where deep learning has yet to replace traditional
machine learning. This paper addresses these challenges in developing one of
the first deep clustering methods for tabular data: Gaussian Cluster Embedding
in Autoencoder Latent Space (G-CEALS). G-CEALS is an unsupervised deep
clustering framework for learning the parameters of multivariate Gaussian
cluster distributions by iteratively updating individual cluster weights. The
G-CEALS method presents average rank orderings of 2.9(1.7) and 2.8(1.7) based
on clustering accuracy and adjusted Rand index (ARI) scores on sixteen tabular
data sets, respectively, and outperforms nine state-of-the-art clustering
methods. G-CEALS substantially improves clustering performance compared to
traditional K-means and GMM, which are still de facto methods for clustering
tabular data. Similar computationally efficient and high-performing deep
clustering frameworks are imperative to reap the myriad benefits of deep
learning on tabular data over traditional machine learning.","Shourav B. Rabbani, Ivan V. Medri, Manar D. Samad",2023,http://arxiv.org/abs/2301.00802v3
Deep Learning on Image Denoising: An overview,"Deep learning techniques have received much attention in the area of image
denoising. However, there are substantial differences in the various types of
deep learning methods dealing with image denoising. Specifically,
discriminative learning based on deep learning can ably address the issue of
Gaussian noise. Optimization models based on deep learning are effective in
estimating the real noise. However, there has thus far been little related
research to summarize the different deep learning techniques for image
denoising. In this paper, we offer a comparative study of deep techniques in
image denoising. We first classify the deep convolutional neural networks
(CNNs) for additive white noisy images; the deep CNNs for real noisy images;
the deep CNNs for blind denoising and the deep CNNs for hybrid noisy images,
which represents the combination of noisy, blurred and low-resolution images.
Then, we analyze the motivations and principles of the different types of deep
learning methods. Next, we compare the state-of-the-art methods on public
denoising datasets in terms of quantitative and qualitative analysis. Finally,
we point out some potential challenges and directions of future research.","Chunwei Tian, Lunke Fei, Wenxian Zheng, Yong Xu, Wangmeng Zuo, Chia-Wen Lin",2019,http://arxiv.org/abs/1912.13171v4
Asymmetric Deep Supervised Hashing,"Hashing has been widely used for large-scale approximate nearest neighbor
search because of its storage and search efficiency. Recent work has found that
deep supervised hashing can significantly outperform non-deep supervised
hashing in many applications. However, most existing deep supervised hashing
methods adopt a symmetric strategy to learn one deep hash function for both
query points and database (retrieval) points. The training of these symmetric
deep supervised hashing methods is typically time-consuming, which makes them
hard to effectively utilize the supervised information for cases with
large-scale database. In this paper, we propose a novel deep supervised hashing
method, called asymmetric deep supervised hashing (ADSH), for large-scale
nearest neighbor search. ADSH treats the query points and database points in an
asymmetric way. More specifically, ADSH learns a deep hash function only for
query points, while the hash codes for database points are directly learned.
The training of ADSH is much more efficient than that of traditional symmetric
deep supervised hashing methods. Experiments show that ADSH can achieve
state-of-the-art performance in real applications.","Qing-Yuan Jiang, Wu-Jun Li",2017,http://arxiv.org/abs/1707.08325v1
Convergence Analysis of Deep Residual Networks,"Various powerful deep neural network architectures have made great
contribution to the exciting successes of deep learning in the past two
decades. Among them, deep Residual Networks (ResNets) are of particular
importance because they demonstrated great usefulness in computer vision by
winning the first place in many deep learning competitions. Also, ResNets were
the first class of neural networks in the development history of deep learning
that are really deep. It is of mathematical interest and practical meaning to
understand the convergence of deep ResNets. We aim at characterizing the
convergence of deep ResNets as the depth tends to infinity in terms of the
parameters of the networks. Toward this purpose, we first give a matrix-vector
description of general deep neural networks with shortcut connections and
formulate an explicit expression for the networks by using the notions of
activation domains and activation matrices. The convergence is then reduced to
the convergence of two series involving infinite products of non-square
matrices. By studying the two series, we establish a sufficient condition for
pointwise convergence of ResNets. Our result is able to give justification for
the design of ResNets. We also conduct experiments on benchmark machine
learning data to verify our results.","Wentao Huang, Haizhang Zhang",2022,http://arxiv.org/abs/2205.06571v1
Deep Learning in Bioinformatics,"In the era of big data, transformation of biomedical big data into valuable
knowledge has been one of the most important challenges in bioinformatics. Deep
learning has advanced rapidly since the early 2000s and now demonstrates
state-of-the-art performance in various fields. Accordingly, application of
deep learning in bioinformatics to gain insight from data has been emphasized
in both academia and industry. Here, we review deep learning in bioinformatics,
presenting examples of current research. To provide a useful and comprehensive
perspective, we categorize research both by the bioinformatics domain (i.e.,
omics, biomedical imaging, biomedical signal processing) and deep learning
architecture (i.e., deep neural networks, convolutional neural networks,
recurrent neural networks, emergent architectures) and present brief
descriptions of each study. Additionally, we discuss theoretical and practical
issues of deep learning in bioinformatics and suggest future research
directions. We believe that this review will provide valuable insights and
serve as a starting point for researchers to apply deep learning approaches in
their bioinformatics studies.","Seonwoo Min, Byunghan Lee, Sungroh Yoon",2016,http://arxiv.org/abs/1603.06430v5
Security Risks in Deep Learning Implementations,"Advance in deep learning algorithms overshadows their security risk in
software implementations. This paper discloses a set of vulnerabilities in
popular deep learning frameworks including Caffe, TensorFlow, and Torch.
Contrast to the small code size of deep learning models, these deep learning
frameworks are complex and contain heavy dependencies on numerous open source
packages. This paper considers the risks caused by these vulnerabilities by
studying their impact on common deep learning applications such as voice
recognition and image classifications. By exploiting these framework
implementations, attackers can launch denial-of-service attacks that crash or
hang a deep learning application, or control-flow hijacking attacks that cause
either system compromise or recognition evasions. The goal of this paper is to
draw attention on the software implementations and call for the community
effort to improve the security of deep learning frameworks.","Qixue Xiao, Kang Li, Deyue Zhang, Weilin Xu",2017,http://arxiv.org/abs/1711.11008v1
"Exponential Convergence of the Deep Neural Network Approximation for
  Analytic Functions","We prove that for analytic functions in low dimension, the convergence rate
of the deep neural network approximation is exponential.","Weinan E, Qingcan Wang",2018,http://arxiv.org/abs/1807.00297v1
Exploiting Deep Learning for Persian Sentiment Analysis,"The rise of social media is enabling people to freely express their opinions
about products and services. The aim of sentiment analysis is to automatically
determine subject's sentiment (e.g., positive, negative, or neutral) towards a
particular aspect such as topic, product, movie, news etc. Deep learning has
recently emerged as a powerful machine learning technique to tackle a growing
demand of accurate sentiment analysis. However, limited work has been conducted
to apply deep learning algorithms to languages other than English, such as
Persian. In this work, two deep learning models (deep autoencoders and deep
convolutional neural networks (CNNs)) are developed and applied to a novel
Persian movie reviews dataset. The proposed deep learning models are analyzed
and compared with the state-of-the-art shallow multilayer perceptron (MLP)
based machine learning model. Simulation results demonstrate the enhanced
performance of deep learning over state-of-the-art MLP.","Kia Dashtipour, Mandar Gogate, Ahsan Adeel, Cosimo Ieracitano, Hadi Larijani, Amir Hussain",2018,http://arxiv.org/abs/1808.05077v1
Notes on Deep Learning for NLP,My notes on Deep Learning for NLP.,Antoine J. -P. Tixier,2018,http://arxiv.org/abs/1808.09772v2
Optimization Methods in Deep Learning: A Comprehensive Overview,"In recent years, deep learning has achieved remarkable success in various
fields such as image recognition, natural language processing, and speech
recognition. The effectiveness of deep learning largely depends on the
optimization methods used to train deep neural networks. In this paper, we
provide an overview of first-order optimization methods such as Stochastic
Gradient Descent, Adagrad, Adadelta, and RMSprop, as well as recent
momentum-based and adaptive gradient methods such as Nesterov accelerated
gradient, Adam, Nadam, AdaMax, and AMSGrad. We also discuss the challenges
associated with optimization in deep learning and explore techniques for
addressing these challenges, including weight initialization, batch
normalization, and layer normalization. Finally, we provide recommendations for
selecting optimization methods for different deep learning tasks and datasets.
This paper serves as a comprehensive guide to optimization methods in deep
learning and can be used as a reference for researchers and practitioners in
the field.",David Shulman,2023,http://arxiv.org/abs/2302.09566v2
Survey of Code Search Based on Deep Learning,"Code writing is repetitive and predictable, inspiring us to develop various
code intelligence techniques. This survey focuses on code search, that is, to
retrieve code that matches a given query by effectively capturing the semantic
similarity between the query and code. Deep learning, being able to extract
complex semantics information, has achieved great success in this field.
Recently, various deep learning methods, such as graph neural networks and
pretraining models, have been applied to code search with significant progress.
Deep learning is now the leading paradigm for code search. In this survey, we
provide a comprehensive overview of deep learning-based code search. We review
the existing deep learning-based code search framework which maps query/code to
vectors and measures their similarity. Furthermore, we propose a new taxonomy
to illustrate the state-of-the-art deep learning-based code search in a
three-steps process: query semantics modeling, code semantics modeling, and
matching modeling which involves the deep learning model training. Finally, we
suggest potential avenues for future research in this promising field.","Yutao Xie, Jiayi Lin, Hande Dong, Lei Zhang, Zhonghai Wu",2023,http://arxiv.org/abs/2305.05959v2
Performance Evaluation of Deep Learning Tools in Docker Containers,"With the success of deep learning techniques in a broad range of application
domains, many deep learning software frameworks have been developed and are
being updated frequently to adapt to new hardware features and software
libraries, which bring a big challenge for end users and system administrators.
To address this problem, container techniques are widely used to simplify the
deployment and management of deep learning software. However, it remains
unknown whether container techniques bring any performance penalty to deep
learning applications. The purpose of this work is to systematically evaluate
the impact of docker container on the performance of deep learning
applications. We first benchmark the performance of system components (IO, CPU
and GPU) in a docker container and the host system and compare the results to
see if there's any difference. According to our results, we find that
computational intensive jobs, either running on CPU or GPU, have small overhead
indicating docker containers can be applied to deep learning programs. Then we
evaluate the performance of some popular deep learning tools deployed in a
docker container and the host system. It turns out that the docker container
will not cause noticeable drawbacks while running those deep learning tools. So
encapsulating deep learning tool in a container is a feasible solution.","Pengfei Xu, Shaohuai Shi, Xiaowen Chu",2017,http://arxiv.org/abs/1711.03386v1
"Wolf in Sheep's Clothing - The Downscaling Attack Against Deep Learning
  Applications","This paper considers security risks buried in the data processing pipeline in
common deep learning applications. Deep learning models usually assume a fixed
scale for their training and input data. To allow deep learning applications to
handle a wide range of input data, popular frameworks, such as Caffe,
TensorFlow, and Torch, all provide data scaling functions to resize input to
the dimensions used by deep learning models. Image scaling algorithms are
intended to preserve the visual features of an image after scaling. However,
common image scaling algorithms are not designed to handle human crafted
images. Attackers can make the scaling outputs look dramatically different from
the corresponding input images.
  This paper presents a downscaling attack that targets the data scaling
process in deep learning applications. By carefully crafting input data that
mismatches with the dimension used by deep learning models, attackers can
create deceiving effects. A deep learning application effectively consumes data
that are not the same as those presented to users. The visual inconsistency
enables practical evasion and data poisoning attacks to deep learning
applications. This paper presents proof-of-concept attack samples to popular
deep-learning-based image classification applications. To address the
downscaling attacks, the paper also suggests multiple potential mitigation
strategies.","Qixue Xiao, Kang Li, Deyue Zhang, Yier Jin",2017,http://arxiv.org/abs/1712.07805v1
Deep learning for image segmentation: veritable or overhyped?,"Deep learning has achieved great success as a powerful classification tool
and also made great progress in sematic segmentation. As a result, many
researchers also believe that deep learning is the most powerful tool for pixel
level image segmentation. Could deep learning achieve the same pixel level
accuracy as traditional image segmentation techniques by mapping the features
of the object into a non-linear function? This paper gives a short survey of
the accuracies achieved by deep learning so far in image classification and
image segmentation. Compared to the high accuracies achieved by deep learning
in classifying limited categories in international vision challenges, the image
segmentation accuracies achieved by deep learning in the same challenges are
only about eighty percent. On the contrary, the image segmentation accuracies
achieved in international biomedical challenges are close to ninty five
percent. Why the difference is so big? Since the accuracies of the competitors
methods are only evaluated based on their submitted results instead of
reproducing the results by submitting the source codes or the software, are the
achieved accuracies verifiable or overhyped? We are going to find it out by
analyzing the working principle of deep learning. Finally, we compared the
accuracies of state of the art deep learning methods with a threshold selection
method quantitatively. Experimental results showed that the threshold selection
method could achieve significantly higher accuracy than deep learning methods
in image segmentation.",Zhenzhou Wang,2019,http://arxiv.org/abs/1904.08483v3
A Comprehensive Study on Deep Learning Bug Characteristics,"Deep learning has gained substantial popularity in recent years. Developers
mainly rely on libraries and tools to add deep learning capabilities to their
software. What kinds of bugs are frequently found in such software? What are
the root causes of such bugs? What impacts do such bugs have? Which stages of
deep learning pipeline are more bug prone? Are there any antipatterns?
Understanding such characteristics of bugs in deep learning software has the
potential to foster the development of better deep learning platforms,
debugging mechanisms, development practices, and encourage the development of
analysis and verification frameworks. Therefore, we study 2716 high-quality
posts from Stack Overflow and 500 bug fix commits from Github about five
popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to
understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone
stage of deep learning pipeline as well as whether there are some common
antipatterns found in this buggy software. The key findings of our study
include: data bug and logic bug are the most severe bug types in deep learning
software appearing more than 48% of the times, major root causes of these bugs
are Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up
more than 43% of the times. We have also found that the bugs in the usage of
deep learning libraries have some common antipatterns that lead to a strong
correlation of bug types among the libraries.","Md Johirul Islam, Giang Nguyen, Rangeet Pan, Hridesh Rajan",2019,http://arxiv.org/abs/1906.01388v1
Structure preserving deep learning,"Over the past few years, deep learning has risen to the foreground as a topic
of massive interest, mainly as a result of successes obtained in solving
large-scale image processing tasks. There are multiple challenging mathematical
problems involved in applying deep learning: most deep learning methods require
the solution of hard optimisation problems, and a good understanding of the
tradeoff between computational effort, amount of data and model complexity is
required to successfully design a deep learning approach for a given problem. A
large amount of progress made in deep learning has been based on heuristic
explorations, but there is a growing effort to mathematically understand the
structure in existing deep learning methods and to systematically design new
deep learning methods to preserve certain types of structure in deep learning.
In this article, we review a number of these directions: some deep neural
networks can be understood as discretisations of dynamical systems, neural
networks can be designed to have desirable properties such as invertibility or
group equivariance, and new algorithmic frameworks based on conformal
Hamiltonian systems and Riemannian manifolds to solve the optimisation problems
have been proposed. We conclude our review of each of these topics by
discussing some open problems that we consider to be interesting directions for
future research.","Elena Celledoni, Matthias J. Ehrhardt, Christian Etmann, Robert I McLachlan, Brynjulf Owren, Carola-Bibiane Schönlieb, Ferdia Sherry",2020,http://arxiv.org/abs/2006.03364v1
Skin disease diagnosis with deep learning: a review,"Skin cancer is one of the most threatening diseases worldwide. However,
diagnosing skin cancer correctly is challenging. Recently, deep learning
algorithms have emerged to achieve excellent performance on various tasks.
Particularly, they have been applied to the skin disease diagnosis tasks. In
this paper, we present a review on deep learning methods and their applications
in skin disease diagnosis. We first present a brief introduction to skin
diseases and image acquisition methods in dermatology, and list several
publicly available skin datasets for training and testing algorithms. Then, we
introduce the conception of deep learning and review popular deep learning
architectures. Thereafter, popular deep learning frameworks facilitating the
implementation of deep learning algorithms and performance evaluation metrics
are presented. As an important part of this article, we then review the
literature involving deep learning methods for skin disease diagnosis from
several aspects according to the specific tasks. Additionally, we discuss the
challenges faced in the area and suggest possible future research directions.
The major purpose of this article is to provide a conceptual and systematically
review of the recent works on skin disease diagnosis with deep learning. Given
the popularity of deep learning, there remains great challenges in the area, as
well as opportunities that we can explore in the future.","Hongfeng Li, Yini Pan, Jie Zhao, Li Zhang",2020,http://arxiv.org/abs/2011.05627v2
"Combining Physics and Deep Learning to learn Continuous-Time Dynamics
  Models","Deep learning has been widely used within learning algorithms for robotics.
One disadvantage of deep networks is that these networks are black-box
representations. Therefore, the learned approximations ignore the existing
knowledge of physics or robotics. Especially for learning dynamics models,
these black-box models are not desirable as the underlying principles are well
understood and the standard deep networks can learn dynamics that violate these
principles. To learn dynamics models with deep networks that guarantee
physically plausible dynamics, we introduce physics-inspired deep networks that
combine first principles from physics with deep learning. We incorporate
Lagrangian mechanics within the model learning such that all approximated
models adhere to the laws of physics and conserve energy. Deep Lagrangian
Networks (DeLaN) parametrize the system energy using two networks. The
parameters are obtained by minimizing the squared residual of the
Euler-Lagrange differential equation. Therefore, the resulting model does not
require specific knowledge of the individual system, is interpretable, and can
be used as a forward, inverse, and energy model. Previously these properties
were only obtained when using system identification techniques that require
knowledge of the kinematic structure. We apply DeLaN to learning dynamics
models and apply these models to control simulated and physical rigid body
systems. The results show that the proposed approach obtains dynamics models
that can be applied to physical systems for real-time control. Compared to
standard deep networks, the physics-inspired models learn better models and
capture the underlying structure of the dynamics.","Michael Lutter, Jan Peters",2021,http://arxiv.org/abs/2110.01894v2
"Statistical-mechanical analysis of pre-training and fine tuning in deep
  learning","In this paper, we present a statistical-mechanical analysis of deep learning.
We elucidate some of the essential components of deep learning---pre-training
by unsupervised learning and fine tuning by supervised learning. We formulate
the extraction of features from the training data as a margin criterion in a
high-dimensional feature-vector space. The self-organized classifier is then
supplied with small amounts of labelled data, as in deep learning. Although we
employ a simple single-layer perceptron model, rather than directly analyzing a
multi-layer neural network, we find a nontrivial phase transition that is
dependent on the number of unlabelled data in the generalization error of the
resultant classifier. In this sense, we evaluate the efficacy of the
unsupervised learning component of deep learning. The analysis is performed by
the replica method, which is a sophisticated tool in statistical mechanics. We
validate our result in the manner of deep learning, using a simple iterative
algorithm to learn the weight vector on the basis of belief propagation.",Masayuki Ohzeki,2015,http://arxiv.org/abs/1501.04413v1
Deep Reinforcement Learning: An Overview,"We give an overview of recent exciting achievements of deep reinforcement
learning (RL). We discuss six core elements, six important mechanisms, and
twelve applications. We start with background of machine learning, deep
learning and reinforcement learning. Next we discuss core RL elements,
including value function, in particular, Deep Q-Network (DQN), policy, reward,
model, planning, and exploration. After that, we discuss important mechanisms
for RL, including attention and memory, unsupervised learning, transfer
learning, multi-agent RL, hierarchical RL, and learning to learn. Then we
discuss various applications of RL, including games, in particular, AlphaGo,
robotics, natural language processing, including dialogue systems, machine
translation, and text generation, computer vision, neural architecture design,
business management, finance, healthcare, Industry 4.0, smart grid, intelligent
transportation systems, and computer systems. We mention topics not reviewed
yet, and list a collection of RL resources. After presenting a brief summary,
we close with discussions.
  Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant
update.",Yuxi Li,2017,http://arxiv.org/abs/1701.07274v6
Data-driven geophysics: from dictionary learning to deep learning,"Understanding the principles of geophysical phenomena is an essential and
challenging task. ""Model-driven"" approaches have supported the development of
geophysics for a long time; however, such methods suffer from the curse of
dimensionality and may inaccurately model the subsurface. ""Data-driven""
techniques may overcome these issues with increasingly available geophysical
data. In this article, we review the basic concepts of and recent advances in
data-driven approaches from dictionary learning to deep learning in a variety
of geophysical scenarios. Explorational geophysics including data processing,
inversion and interpretation will be mainly focused. Artificial intelligence
applications on geoscience involving deep Earth, earthquake, water resource,
atmospheric science, satellite remoe sensing and space sciences are also
reviewed. We present a coding tutorial and a summary of tips for beginners and
interested geophysical readers to rapidly explore deep learning. Some promising
directions are provided for future research involving deep learning in
geophysics, such as unsupervised learning, transfer learning, multimodal deep
learning, federated learning, uncertainty estimation, and activate learning.","Siwei Yu, Jianwei Ma",2020,http://arxiv.org/abs/2007.06183v2
Data efficient deep learning for medical image analysis: A survey,"The rapid evolution of deep learning has significantly advanced the field of
medical image analysis. However, despite these achievements, the further
enhancement of deep learning models for medical image analysis faces a
significant challenge due to the scarcity of large, well-annotated datasets. To
address this issue, recent years have witnessed a growing emphasis on the
development of data-efficient deep learning methods. This paper conducts a
thorough review of data-efficient deep learning methods for medical image
analysis. To this end, we categorize these methods based on the level of
supervision they rely on, encompassing categories such as no supervision,
inexact supervision, incomplete supervision, inaccurate supervision, and only
limited supervision. We further divide these categories into finer
subcategories. For example, we categorize inexact supervision into multiple
instance learning and learning with weak annotations. Similarly, we categorize
incomplete supervision into semi-supervised learning, active learning, and
domain-adaptive learning and so on. Furthermore, we systematically summarize
commonly used datasets for data efficient deep learning in medical image
analysis and investigate future research directions to conclude this survey.","Suruchi Kumari, Pravendra Singh",2023,http://arxiv.org/abs/2310.06557v1
MODL: Multilearner Online Deep Learning,"Online deep learning tackles the challenge of learning from data streams by
balancing two competing goals: fast learning and deep learning. However,
existing research primarily emphasizes deep learning solutions, which are more
adept at handling the ``deep'' aspect than the ``fast'' aspect of online
learning. In this work, we introduce an alternative paradigm through a hybrid
multilearner approach. We begin by developing a fast online logistic regression
learner, which operates without relying on backpropagation. It leverages
closed-form recursive updates of model parameters, efficiently addressing the
fast learning component of the online learning challenge. This approach is
further integrated with a cascaded multilearner design, where shallow and deep
learners are co-trained in a cooperative, synergistic manner to solve the
online learning problem. We demonstrate that this approach achieves
state-of-the-art performance on standard online learning datasets. We make our
code available: https://github.com/AntonValk/MODL","Antonios Valkanas, Boris N. Oreshkin, Mark Coates",2024,http://arxiv.org/abs/2405.18281v2
"Identifying Semantically Duplicate Questions Using Data Science
  Approach: A Quora Case Study","Identifying semantically identical questions on, Question and Answering
social media platforms like Quora is exceptionally significant to ensure that
the quality and the quantity of content are presented to users, based on the
intent of the question and thus enriching overall user experience. Detecting
duplicate questions is a challenging problem because natural language is very
expressive, and a unique intent can be conveyed using different words, phrases,
and sentence structuring. Machine learning and deep learning methods are known
to have accomplished superior results over traditional natural language
processing techniques in identifying similar texts. In this paper, taking Quora
for our case study, we explored and applied different machine learning and deep
learning techniques on the task of identifying duplicate questions on Quora's
dataset. By using feature engineering, feature importance techniques, and
experimenting with seven selected machine learning classifiers, we demonstrated
that our models outperformed previous studies on this task. Xgboost model with
character level term frequency and inverse term frequency is our best machine
learning model that has also outperformed a few of the Deep learning baseline
models. We applied deep learning techniques to model four different deep neural
networks of multiple layers consisting of Glove embeddings, Long Short Term
Memory, Convolution, Max pooling, Dense, Batch Normalization, Activation
functions, and model merge. Our deep learning models achieved better accuracy
than machine learning models. Three out of four proposed architectures
outperformed the accuracy from previous machine learning and deep learning
research work, two out of four models outperformed accuracy from previous deep
learning study on Quora's question pair dataset, and our best model achieved
accuracy of 85.82% which is close to Quora state of the art accuracy.","Navedanjum Ansari, Rajesh Sharma",2020,http://arxiv.org/abs/2004.11694v1
Streaming Active Deep Forest for Evolving Data Stream Classification,"In recent years, Deep Neural Networks (DNNs) have gained progressive momentum
in many areas of machine learning. The layer-by-layer process of DNNs has
inspired the development of many deep models, including deep ensembles. The
most notable deep ensemble-based model is Deep Forest, which can achieve highly
competitive performance while having much fewer hyper-parameters comparing to
DNNs. In spite of its huge success in the batch learning setting, no effort has
been made to adapt Deep Forest to the context of evolving data streams. In this
work, we introduce the Streaming Deep Forest (SDF) algorithm, a
high-performance deep ensemble method specially adapted to stream
classification. We also present the Augmented Variable Uncertainty (AVU) active
learning strategy to reduce the labeling cost in the streaming context. We
compare the proposed methods to state-of-the-art streaming algorithms in a wide
range of datasets. The results show that by following the AVU active learning
strategy, SDF with only 70\% of labeling budget significantly outperforms other
methods trained with all instances.","Anh Vu Luong, Tien Thanh Nguyen, Alan Wee-Chung Liew",2020,http://arxiv.org/abs/2002.11816v1
"Optimizing Wireless Networks with Deep Unfolding: Comparative Study on
  Two Deep Unfolding Mechanisms","In this work, we conduct a comparative study on two deep unfolding mechanisms
to efficiently perform power control in the next generation wireless networks.
The power control problem is formulated as energy efficiency over multiple
interference links. The problem is nonconvex. We employ fractional programming
transformation to design two solutions for the problem. The first solution is a
numerical solution while the second solution is a closed-form solution. Based
on the first solution, we design a semi-unfolding deep learning model where we
combine the domain knowledge of the wireless communications and the recent
advances in the data-driven deep learning. Moreover, on the highlights of the
closed-form solution, fully deep unfolded deep learning model is designed in
which we fully leveraged the expressive closed-form power control solution and
deep learning advances. In the simulation results, we compare the performance
of the proposed deep learning models and the iterative solutions in terms of
accuracy and inference speed to show their suitability for the real-time
application in next generation networks.","Abuzar B. M. Adam, Mohammed A. M. Elhassan, Elhadj Moustapha Diallo",2024,http://arxiv.org/abs/2403.18930v1
Genetic Algorithms for Evolving Deep Neural Networks,"In recent years, deep learning methods applying unsupervised learning to
train deep layers of neural networks have achieved remarkable results in
numerous fields. In the past, many genetic algorithms based methods have been
successfully applied to training neural networks. In this paper, we extend
previous work and propose a GA-assisted method for deep learning. Our
experimental results indicate that this GA-assisted approach improves the
performance of a deep autoencoder, producing a sparser neural network.","Eli David, Iddo Greental",2017,http://arxiv.org/abs/1711.07655v1
Symmetry Detection of Occluded Point Cloud Using Deep Learning,"Symmetry detection has been a classical problem in computer graphics, many of
which using traditional geometric methods. In recent years, however, we have
witnessed the arising deep learning changed the landscape of computer graphics.
In this paper, we aim to solve the symmetry detection of the occluded point
cloud in a deep-learning fashion. To the best of our knowledge, we are the
first to utilize deep learning to tackle such a problem. In such a deep
learning framework, double supervisions: points on the symmetry plane and
normal vectors are employed to help us pinpoint the symmetry plane. We
conducted experiments on the YCB- video dataset and demonstrate the efficacy of
our method.","Zhelun Wu, Hongyan Jiang, Siyun He",2020,http://arxiv.org/abs/2003.06520v1
Survey on Deep Learning-based Kuzushiji Recognition,"Owing to the overwhelming accuracy of the deep learning method demonstrated
at the 2012 image classification competition, deep learning has been
successfully applied to a variety of other tasks. The high-precision detection
and recognition of Kuzushiji, a Japanese cursive script used for transcribing
historical documents, has been made possible through the use of deep learning.
In recent years, competitions on Kuzushiji recognition have been held, and many
researchers have proposed various recognition methods. This study examines
recent research trends, current problems, and future prospects in Kuzushiji
recognition using deep learning.","Kazuya Ueki, Tomoka Kojima",2020,http://arxiv.org/abs/2007.09637v1
Deep Learning via Dynamical Systems: An Approximation Perspective,"We build on the dynamical systems approach to deep learning, where deep
residual networks are idealized as continuous-time dynamical systems, from the
approximation perspective. In particular, we establish general sufficient
conditions for universal approximation using continuous-time deep residual
networks, which can also be understood as approximation theories in $L^p$ using
flow maps of dynamical systems. In specific cases, rates of approximation in
terms of the time horizon are also established. Overall, these results reveal
that composition function approximation through flow maps present a new
paradigm in approximation theory and contributes to building a useful
mathematical framework to investigate deep learning.","Qianxiao Li, Ting Lin, Zuowei Shen",2019,http://arxiv.org/abs/1912.10382v2
Excess risk bound for deep learning under weak dependence,"This paper considers deep neural networks for learning weakly dependent
processes in a general framework that includes, for instance, regression
estimation, time series prediction, time series classification. The $\psi$-weak
dependence structure considered is quite large and covers other conditions such
as mixing, association,$\ldots$ Firstly, the approximation of smooth functions
by deep neural networks with a broad class of activation functions is
considered. We derive the required depth, width and sparsity of a deep neural
network to approximate any H\""{o}lder smooth function, defined on any compact
set $\mx$. Secondly, we establish a bound of the excess risk for the learning
of weakly dependent observations by deep neural networks. When the target
function is sufficiently smooth, this bound is close to the usual
$\mathcal{O}(n^{-1/2})$.",William Kengne,2023,http://arxiv.org/abs/2302.07503v1
"Demystifying Deep Learning: A Geometric Approach to Iterative
  Projections","Parametric approaches to Learning, such as deep learning (DL), are highly
popular in nonlinear regression, in spite of their extremely difficult training
with their increasing complexity (e.g. number of layers in DL). In this paper,
we present an alternative semi-parametric framework which foregoes the
ordinarily required feedback, by introducing the novel idea of geometric
regularization. We show that certain deep learning techniques such as residual
network (ResNet) architecture are closely related to our approach. Hence, our
technique can be used to analyze these types of deep learning. Moreover, we
present preliminary results which confirm that our approach can be easily
trained to obtain complex structures.","Ashkan Panahi, Hamid Krim, Liyi Dai",2018,http://arxiv.org/abs/1803.08416v1
Visual Analytics for Explainable Deep Learning,"Recently, deep learning has been advancing the state of the art in artificial
intelligence to a new level, and humans rely on artificial intelligence
techniques more than ever. However, even with such unprecedented advancements,
the lack of explanation regarding the decisions made by deep learning models
and absence of control over their internal processes act as major drawbacks in
critical decision-making processes, such as precision medicine and law
enforcement. In response, efforts are being made to make deep learning
interpretable and controllable by humans. In this paper, we review visual
analytics, information visualization, and machine learning perspectives
relevant to this aim, and discuss potential challenges and future research
directions.","Jaegul Choo, Shixia Liu",2018,http://arxiv.org/abs/1804.02527v1
Deep Learning,"Deep learning (DL) is a high dimensional data reduction technique for
constructing high-dimensional predictors in input-output models. DL is a form
of machine learning that uses hierarchical layers of latent features. In this
article, we review the state-of-the-art of deep learning from a modeling and
algorithmic perspective. We provide a list of successful areas of applications
in Artificial Intelligence (AI), Image Processing, Robotics and Automation.
Deep learning is predictive in its nature rather then inferential and can be
viewed as a black-box methodology for high-dimensional function estimation.","Nicholas G. Polson, Vadim O. Sokolov",2018,http://arxiv.org/abs/1807.07987v2
Deep Learning: Computational Aspects,"In this article we review computational aspects of Deep Learning (DL). Deep
learning uses network architectures consisting of hierarchical layers of latent
variables to construct predictors for high-dimensional input-output models.
Training a deep learning architecture is computationally intensive, and
efficient linear algebra libraries is the key for training and inference.
Stochastic gradient descent (SGD) optimization and batch sampling are used to
learn from massive data sets.","Nicholas Polson, Vadim Sokolov",2018,http://arxiv.org/abs/1808.08618v2
Off-the-grid model based deep learning (O-MODL),"We introduce a model based off-the-grid image reconstruction algorithm using
deep learned priors. The main difference of the proposed scheme with current
deep learning strategies is the learning of non-linear annihilation relations
in Fourier space. We rely on a model based framework, which allows us to use a
significantly smaller deep network, compared to direct approaches that also
learn how to invert the forward model. Preliminary comparisons against image
domain MoDL approach demonstrates the potential of the off-the-grid
formulation. The main benefit of the proposed scheme compared to structured
low-rank methods is the quite significant reduction in computational
complexity.","Aniket Pramanik, Hemant Kumar Aggarwal, Mathews Jacob",2018,http://arxiv.org/abs/1812.10747v1
"Gradient Descent based Optimization Algorithms for Deep Learning Models
  Training","In this paper, we aim at providing an introduction to the gradient descent
based optimization algorithms for learning deep neural network models. Deep
learning models involving multiple nonlinear projection layers are very
challenging to train. Nowadays, most of the deep learning model training still
relies on the back propagation algorithm actually. In back propagation, the
model variables will be updated iteratively until convergence with gradient
descent based optimization algorithms. Besides the conventional vanilla
gradient descent algorithm, many gradient descent variants have also been
proposed in recent years to improve the learning performance, including
Momentum, Adagrad, Adam, Gadam, etc., which will all be introduced in this
paper respectively.",Jiawei Zhang,2019,http://arxiv.org/abs/1903.03614v1
Two-Stage Learning for Uplink Channel Estimation in One-Bit Massive MIMO,"We develop a two-stage deep learning pipeline architecture to estimate the
uplink massive MIMO channel with one-bit ADCs. This deep learning pipeline is
composed of two separate generative deep learning models. The first one is a
supervised learning model and designed to compensate for the quantization loss.
The second one is an unsupervised learning model and optimized for denoising.
Our results show that the proposed deep learning-based channel estimator can
significantly outperform other state-of-the-art channel estimators for one-bit
quantized massive MIMO systems. In particular, our design provides 5-10 dB gain
in channel estimation error. Furthermore, it requires a reasonable amount of
pilots, on the order of 20 per coherence time interval.","Eren Balevi, Jeffrey G. Andrews",2019,http://arxiv.org/abs/1911.12461v1
Deep Learning for IoT,"Deep learning and other machine learning approaches are deployed to many
systems related to Internet of Things or IoT. However, it faces challenges that
adversaries can take loopholes to hack these systems through tampering history
data. This paper first presents overall points of adversarial machine learning.
Then, we illustrate traditional methods, such as Petri Net cannot solve this
new question efficiently. To help IoT data analysis more efficient, we propose
a retrieval method based on deep learning (recurrent neural network). Besides,
this paper presents a research on data retrieval solution to avoid hacking by
adversaries in the fields of adversary machine leaning. It further directs the
new approaches in terms of how to implementing this framework in IoT settings
based on adversarial deep learning.",Tao Lin,2021,http://arxiv.org/abs/2104.05569v1
Concept-Oriented Deep Learning: Generative Concept Representations,"Generative concept representations have three major advantages over
discriminative ones: they can represent uncertainty, they support integration
of learning and reasoning, and they are good for unsupervised and
semi-supervised learning. We discuss probabilistic and generative deep
learning, which generative concept representations are based on, and the use of
variational autoencoders and generative adversarial networks for learning
generative concept representations, particularly for concepts whose data are
sequences, structured data or graphs.",Daniel T. Chang,2018,http://arxiv.org/abs/1811.06622v1
Collaborative Deep Reinforcement Learning,"Besides independent learning, human learning process is highly improved by
summarizing what has been learned, communicating it with peers, and
subsequently fusing knowledge from different sources to assist the current
learning goal. This collaborative learning procedure ensures that the knowledge
is shared, continuously refined, and concluded from different perspectives to
construct a more profound understanding. The idea of knowledge transfer has led
to many advances in machine learning and data mining, but significant
challenges remain, especially when it comes to reinforcement learning,
heterogeneous model structures, and different learning tasks. Motivated by
human collaborative learning, in this paper we propose a collaborative deep
reinforcement learning (CDRL) framework that performs adaptive knowledge
transfer among heterogeneous learning agents. Specifically, the proposed CDRL
conducts a novel deep knowledge distillation method to address the
heterogeneity among different learning tasks with a deep alignment network.
Furthermore, we present an efficient collaborative Asynchronous Advantage
Actor-Critic (cA3C) algorithm to incorporate deep knowledge distillation into
the online training of agents, and demonstrate the effectiveness of the CDRL
framework using extensive empirical evaluation on OpenAI gym.","Kaixiang Lin, Shu Wang, Jiayu Zhou",2017,http://arxiv.org/abs/1702.05796v1
"Tracking the Race Between Deep Reinforcement Learning and Imitation
  Learning -- Extended Version","Learning-based approaches for solving large sequential decision making
problems have become popular in recent years. The resulting agents perform
differently and their characteristics depend on those of the underlying
learning approach. Here, we consider a benchmark planning problem from the
reinforcement learning domain, the Racetrack, to investigate the properties of
agents derived from different deep (reinforcement) learning approaches. We
compare the performance of deep supervised learning, in particular imitation
learning, to reinforcement learning for the Racetrack model. We find that
imitation learning yields agents that follow more risky paths. In contrast, the
decisions of deep reinforcement learning are more foresighted, i.e., avoid
states in which fatal decisions are more likely. Our evaluations show that for
this sequential decision making problem, deep reinforcement learning performs
best in many aspects even though for imitation learning optimal decisions are
considered.","Timo P. Gros, Daniel Höller, Jörg Hoffmann, Verena Wolf",2020,http://arxiv.org/abs/2008.00766v1
Deep Discrete Supervised Hashing,"Hashing has been widely used for large-scale search due to its low storage
cost and fast query speed. By using supervised information, supervised hashing
can significantly outperform unsupervised hashing. Recently, discrete
supervised hashing and deep hashing are two representative progresses in
supervised hashing. On one hand, hashing is essentially a discrete optimization
problem. Hence, utilizing supervised information to directly guide discrete
(binary) coding procedure can avoid sub-optimal solution and improve the
accuracy. On the other hand, deep hashing, which integrates deep feature
learning and hash-code learning into an end-to-end architecture, can enhance
the feedback between feature learning and hash-code learning. The key in
discrete supervised hashing is to adopt supervised information to directly
guide the discrete coding procedure in hashing. The key in deep hashing is to
adopt the supervised information to directly guide the deep feature learning
procedure. However, there have not existed works which can use the supervised
information to directly guide both discrete coding procedure and deep feature
learning procedure in the same framework. In this paper, we propose a novel
deep hashing method, called deep discrete supervised hashing (DDSH), to address
this problem. DDSH is the first deep hashing method which can utilize
supervised information to directly guide both discrete coding procedure and
deep feature learning procedure, and thus enhance the feedback between these
two important procedures. Experiments on three real datasets show that DDSH can
outperform other state-of-the-art baselines, including both discrete hashing
and deep hashing baselines, for image retrieval.","Qing-Yuan Jiang, Xue Cui, Wu-Jun Li",2017,http://arxiv.org/abs/1707.09905v1
"Dual Memory Architectures for Fast Deep Learning of Stream Data via an
  Online-Incremental-Transfer Strategy","The online learning of deep neural networks is an interesting problem of
machine learning because, for example, major IT companies want to manage the
information of the massive data uploaded on the web daily, and this technology
can contribute to the next generation of lifelong learning. We aim to train
deep models from new data that consists of new classes, distributions, and
tasks at minimal computational cost, which we call online deep learning.
Unfortunately, deep neural network learning through classical online and
incremental methods does not work well in both theory and practice. In this
paper, we introduce dual memory architectures for online incremental deep
learning. The proposed architecture consists of deep representation learners
and fast learnable shallow kernel networks, both of which synergize to track
the information of new data. During the training phase, we use various online,
incremental ensemble, and transfer learning techniques in order to achieve
lower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image
recognition tasks, the proposed dual memory architectures performs much better
than the classical online and incremental ensemble algorithm, and their
accuracies are similar to that of the batch learner.","Sang-Woo Lee, Min-Oh Heo, Jiwon Kim, Jeonghee Kim, Byoung-Tak Zhang",2015,http://arxiv.org/abs/1506.04477v1
"Examining Deep Learning Architectures for Crime Classification and
  Prediction","In this paper, a detailed study on crime classification and prediction using
deep learning architectures is presented. We examine the effectiveness of deep
learning algorithms on this domain and provide recommendations for designing
and training deep learning systems for predicting crime areas, using open data
from police reports. Having as training data time-series of crime types per
location, a comparative study of 10 state-of-the-art methods against 3
different deep learning configurations is conducted. In our experiments with
five publicly available datasets, we demonstrate that the deep learning-based
methods consistently outperform the existing best-performing methods. Moreover,
we evaluate the effectiveness of different parameters in the deep learning
architectures and give insights for configuring them in order to achieve
improved performance in crime classification and finally crime prediction.","Panagiotis Stalidis, Theodoros Semertzidis, Petros Daras",2018,http://arxiv.org/abs/1812.00602v1
"Guidelines and Benchmarks for Deployment of Deep Learning Models on
  Smartphones as Real-Time Apps","Deep learning solutions are being increasingly used in mobile applications.
Although there are many open-source software tools for the development of deep
learning solutions, there are no guidelines in one place in a unified manner
for using these tools towards real-time deployment of these solutions on
smartphones. From the variety of available deep learning tools, the most suited
ones are used in this paper to enable real-time deployment of deep learning
inference networks on smartphones. A uniform flow of implementation is devised
for both Android and iOS smartphones. The advantage of using multi-threading to
achieve or improve real-time throughputs is also showcased. A benchmarking
framework consisting of accuracy, CPU/GPU consumption and real-time throughput
is considered for validation purposes. The developed deployment approach allows
deep learning models to be turned into real-time smartphone apps with ease
based on publicly available deep learning and smartphone software tools. This
approach is applied to six popular or representative convolutional neural
network models and the validation results based on the benchmarking metrics are
reported.","Abhishek Sehgal, Nasser Kehtarnavaz",2019,http://arxiv.org/abs/1901.02144v1
"Safety Concerns and Mitigation Approaches Regarding the Use of Deep
  Learning in Safety-Critical Perception Tasks","Deep learning methods are widely regarded as indispensable when it comes to
designing perception pipelines for autonomous agents such as robots, drones or
automated vehicles. The main reasons, however, for deep learning not being used
for autonomous agents at large scale already are safety concerns. Deep learning
approaches typically exhibit a black-box behavior which makes it hard for them
to be evaluated with respect to safety-critical aspects. While there have been
some work on safety in deep learning, most papers typically focus on high-level
safety concerns. In this work, we seek to dive into the safety concerns of deep
learning methods and present a concise enumeration on a deeply technical level.
Additionally, we present extensive discussions on possible mitigation methods
and give an outlook regarding what mitigation methods are still missing in
order to facilitate an argumentation for the safety of a deep learning method.","Oliver Willers, Sebastian Sudholt, Shervin Raafatnia, Stephanie Abrecht",2020,http://arxiv.org/abs/2001.08001v1
Deep Learning for Wireless Communications,"Existing communication systems exhibit inherent limitations in translating
theory to practice when handling the complexity of optimization for emerging
wireless applications with high degrees of freedom. Deep learning has a strong
potential to overcome this challenge via data-driven solutions and improve the
performance of wireless systems in utilizing limited spectrum resources. In
this chapter, we first describe how deep learning is used to design an
end-to-end communication system using autoencoders. This flexible design
effectively captures channel impairments and optimizes transmitter and receiver
operations jointly in single-antenna, multiple-antenna, and multiuser
communications. Next, we present the benefits of deep learning in spectrum
situation awareness ranging from channel modeling and estimation to signal
detection and classification tasks. Deep learning improves the performance when
the model-based methods fail. Finally, we discuss how deep learning applies to
wireless communication security. In this context, adversarial machine learning
provides novel means to launch and defend against wireless attacks. These
applications demonstrate the power of deep learning in providing novel means to
design, optimize, adapt, and secure wireless communications.","Tugba Erpek, Timothy J. O'Shea, Yalin E. Sagduyu, Yi Shi, T. Charles Clancy",2020,http://arxiv.org/abs/2005.06068v1
"Deep Learning for Radio-based Human Sensing: Recent Advances and Future
  Directions","While decade-long research has clearly demonstrated the vast potential of
radio frequency (RF) for many human sensing tasks, scaling this technology to
large scenarios remained problematic with conventional approaches. Recently,
researchers have successfully applied deep learning to take radio-based sensing
to a new level. Many different types of deep learning models have been proposed
to achieve high sensing accuracy over a large population and activity set, as
well as in unseen environments. Deep learning has also enabled detection of
novel human sensing phenomena that were previously not possible. In this
survey, we provide a comprehensive review and taxonomy of recent research
efforts on deep learning based RF sensing. We also identify and compare several
publicly released labeled RF sensing datasets that can facilitate such deep
learning research. Finally, we summarize the lessons learned and discuss the
current limitations and future directions of deep learning based RF sensing.","Isura Nirmal, Abdelwahed Khamis, Mahbub Hassan, Wen Hu, Xiaoqing Zhu",2020,http://arxiv.org/abs/2010.12717v2
Recent Advances in Deep Learning for Object Detection,"Object detection is a fundamental visual recognition problem in computer
vision and has been widely studied in the past decades. Visual object detection
aims to find objects of certain target classes with precise localization in a
given image and assign each object instance a corresponding class label. Due to
the tremendous successes of deep learning based image classification, object
detection techniques using deep learning have been actively studied in recent
years. In this paper, we give a comprehensive survey of recent advances in
visual object detection with deep learning. By reviewing a large body of recent
related work in literature, we systematically analyze the existing object
detection frameworks and organize the survey into three major parts: (i)
detection components, (ii) learning strategies, and (iii) applications &
benchmarks. In the survey, we cover a variety of factors affecting the
detection performance in detail, such as detector architectures, feature
learning, proposal generation, sampling strategies, etc. Finally, we discuss
several future directions to facilitate and spur future research for visual
object detection with deep learning. Keywords: Object Detection, Deep Learning,
Deep Convolutional Neural Networks","Xiongwei Wu, Doyen Sahoo, Steven C. H. Hoi",2019,http://arxiv.org/abs/1908.03673v1
"Row-Sparse Discriminative Deep Dictionary Learning for Hyperspectral
  Image Classification","In recent studies in hyperspectral imaging, biometrics and energy analytics,
the framework of deep dictionary learning has shown promise. Deep dictionary
learning outperforms other traditional deep learning tools when training data
is limited; therefore hyperspectral imaging is one such example that benefits
from this framework. Most of the prior studies were based on the unsupervised
formulation; and in all cases, the training algorithm was greedy and hence
sub-optimal. This is the first work that shows how to learn the deep dictionary
learning problem in a joint fashion. Moreover, we propose a new discriminative
penalty to the said framework. The third contribution of this work is showing
how to incorporate stochastic regularization techniques into the deep
dictionary learning framework. Experimental results on hyperspectral image
classification shows that the proposed technique excels over all
state-of-the-art deep and shallow (traditional) learning based methods
published in recent times.","Vanika Singhal, Angshul Majumdar",2019,http://arxiv.org/abs/1912.10804v1
On tuning deep learning models: a data mining perspective,"Deep learning algorithms vary depending on the underlying connection
mechanism of nodes of them. They have various hyperparameters that are either
set via specific algorithms or randomly chosen. Meanwhile, hyperparameters of
deep learning algorithms have the potential to help enhance the performance of
the machine learning tasks. In this paper, a tuning guideline is provided for
researchers who cope with issues originated from hyperparameters of deep
learning models. To that end, four types of deep learning algorithms are
investigated in terms of tuning and data mining perspective. Further, common
search methods of hyperparameters are evaluated on four deep learning
algorithms. Normalization helps increase the performance of classification,
according to the results of this study. The number of features has not
contributed to the decline in the accuracy of deep learning algorithms. Even
though high sparsity results in low accuracy, a uniform distribution is much
more crucial to reach reliable results in terms of data mining.",M. M. Ozturk,2020,http://arxiv.org/abs/2011.09857v1
"Physics-Informed Deep Learning: A Promising Technique for System
  Reliability Assessment","Considerable research has been devoted to deep learning-based predictive
models for system prognostics and health management in the reliability and
safety community. However, there is limited study on the utilization of deep
learning for system reliability assessment. This paper aims to bridge this gap
and explore this new interface between deep learning and system reliability
assessment by exploiting the recent advances of physics-informed deep learning.
Particularly, we present an approach to frame system reliability assessment in
the context of physics-informed deep learning and discuss the potential value
of physics-informed generative adversarial networks for the uncertainty
quantification and measurement data incorporation in system reliability
assessment. The proposed approach is demonstrated by three numerical examples
involving a dual-processor computing system. The results indicate the potential
value of physics-informed deep learning to alleviate computational challenges
and combine measurement data and mathematical models for system reliability
assessment.","Taotao Zhou, Enrique Lopez Droguett, Ali Mosleh",2021,http://arxiv.org/abs/2108.10828v2
Text Classification: A Perspective of Deep Learning Methods,"In recent years, with the rapid development of information on the Internet,
the number of complex texts and documents has increased exponentially, which
requires a deeper understanding of deep learning methods in order to accurately
classify texts using deep learning techniques, and thus deep learning methods
have become increasingly important in text classification. Text classification
is a class of tasks that automatically classifies a set of documents into
multiple predefined categories based on their content and subject matter. Thus,
the main goal of text classification is to enable users to extract information
from textual resources and process processes such as retrieval, classification,
and machine learning techniques together in order to classify different
categories. Many new techniques of deep learning have already achieved
excellent results in natural language processing. The success of these learning
algorithms relies on their ability to understand complex models and non-linear
relationships in data. However, finding the right structure, architecture, and
techniques for text classification is a challenge for researchers. This paper
introduces deep learning-based text classification algorithms, including
important steps required for text classification tasks such as feature
extraction, feature reduction, and evaluation strategies and methods. At the
end of the article, different deep learning text classification methods are
compared and summarized.",Zhongwei Wan,2023,http://arxiv.org/abs/2309.13761v1
Plastic Learning with Deep Fourier Features,"Deep neural networks can struggle to learn continually in the face of
non-stationarity. This phenomenon is known as loss of plasticity. In this
paper, we identify underlying principles that lead to plastic algorithms. In
particular, we provide theoretical results showing that linear function
approximation, as well as a special case of deep linear networks, do not suffer
from loss of plasticity. We then propose deep Fourier features, which are the
concatenation of a sine and cosine in every layer, and we show that this
combination provides a dynamic balance between the trainability obtained
through linearity and the effectiveness obtained through the nonlinearity of
neural networks. Deep networks composed entirely of deep Fourier features are
highly trainable and sustain their trainability over the course of learning.
Our empirical results show that continual learning performance can be
drastically improved by replacing ReLU activations with deep Fourier features.
These results hold for different continual learning scenarios (e.g., label
noise, class incremental learning, pixel permutations) on all major supervised
learning datasets used for continual learning research, such as CIFAR10,
CIFAR100, and tiny-ImageNet.","Alex Lewandowski, Dale Schuurmans, Marlos C. Machado",2024,http://arxiv.org/abs/2410.20634v1
Characterizing the Weight Space for Different Learning Models,"Deep Learning has become one of the primary research areas in developing
intelligent machines. Most of the well-known applications (such as Speech
Recognition, Image Processing and NLP) of AI are driven by Deep Learning. Deep
Learning algorithms mimic human brain using artificial neural networks and
progressively learn to accurately solve a given problem. But there are
significant challenges in Deep Learning systems. There have been many attempts
to make deep learning models imitate the biological neural network. However,
many deep learning models have performed poorly in the presence of adversarial
examples. Poor performance in adversarial examples leads to adversarial attacks
and in turn leads to safety and security in most of the applications. In this
paper we make an attempt to characterize the solution space of a deep neural
network in terms of three different subsets viz. weights belonging to exact
trained patterns, weights belonging to generalized pattern set and weights
belonging to adversarial pattern sets. We attempt to characterize the solution
space with two seemingly different learning paradigms viz. the Deep Neural
Networks and the Dense Associative Memory Model, which try to achieve learning
via quite different mechanisms. We also show that adversarial attacks are
generally less successful against Associative Memory Models than Deep Neural
Networks.","Saurav Musunuru, Jay N. Paranjape, Rahul Kumar Dubey, Vijendran G. Venkoparao",2020,http://arxiv.org/abs/2006.02724v1
"Advancing Deep Active Learning & Data Subset Selection: Unifying
  Principles with Information-Theory Intuitions","At its core, this thesis aims to enhance the practicality of deep learning by
improving the label and training efficiency of deep learning models. To this
end, we investigate data subset selection techniques, specifically active
learning and active sampling, grounded in information-theoretic principles.
Active learning improves label efficiency, while active sampling enhances
training efficiency. Supervised deep learning models often require extensive
training with labeled data. Label acquisition can be expensive and
time-consuming, and training large models is resource-intensive, hindering the
adoption outside academic research and ""big tech."" Existing methods for data
subset selection in deep learning often rely on heuristics or lack a principled
information-theoretic foundation. In contrast, this thesis examines several
objectives for data subset selection and their applications within deep
learning, striving for a more principled approach inspired by information
theory. We begin by disentangling epistemic and aleatoric uncertainty in single
forward-pass deep neural networks, which provides helpful intuitions and
insights into different forms of uncertainty and their relevance for data
subset selection. We then propose and investigate various approaches for active
learning and data subset selection in (Bayesian) deep learning. Finally, we
relate various existing and proposed approaches to approximations of
information quantities in weight or prediction space. Underpinning this work is
a principled and practical notation for information-theoretic quantities that
includes both random variables and observed outcomes. This thesis demonstrates
the benefits of working from a unified perspective and highlights the potential
impact of our contributions to the practical application of deep learning.",Andreas Kirsch,2024,http://arxiv.org/abs/2401.04305v3
"A Comprehensive Survey on Deep Clustering: Taxonomy, Challenges, and
  Future Directions","Clustering is a fundamental machine learning task which has been widely
studied in the literature. Classic clustering methods follow the assumption
that data are represented as features in a vectorized form through various
representation learning techniques. As the data become increasingly complicated
and complex, the shallow (traditional) clustering methods can no longer handle
the high-dimensional data type. With the huge success of deep learning,
especially the deep unsupervised learning, many representation learning
techniques with deep architectures have been proposed in the past decade.
Recently, the concept of Deep Clustering, i.e., jointly optimizing the
representation learning and clustering, has been proposed and hence attracted
growing attention in the community. Motivated by the tremendous success of deep
learning in clustering, one of the most fundamental machine learning tasks, and
the large number of recent advances in this direction, in this paper we conduct
a comprehensive survey on deep clustering by proposing a new taxonomy of
different state-of-the-art approaches. We summarize the essential components of
deep clustering and categorize existing methods by the ways they design
interactions between deep representation learning and clustering. Moreover,
this survey also provides the popular benchmark datasets, evaluation metrics
and open-source implementations to clearly illustrate various experimental
settings. Last but not least, we discuss the practical applications of deep
clustering and suggest challenging topics deserving further investigations as
future directions.","Sheng Zhou, Hongjia Xu, Zhuonan Zheng, Jiawei Chen, Zhao li, Jiajun Bu, Jia Wu, Xin Wang, Wenwu Zhu, Martin Ester",2022,http://arxiv.org/abs/2206.07579v1
Data Optimization in Deep Learning: A Survey,"Large-scale, high-quality data are considered an essential factor for the
successful application of many deep learning techniques. Meanwhile, numerous
real-world deep learning tasks still have to contend with the lack of
sufficient amounts of high-quality data. Additionally, issues such as model
robustness, fairness, and trustworthiness are also closely related to training
data. Consequently, a huge number of studies in the existing literature have
focused on the data aspect in deep learning tasks. Some typical data
optimization techniques include data augmentation, logit perturbation, sample
weighting, and data condensation. These techniques usually come from different
deep learning divisions and their theoretical inspirations or heuristic
motivations may seem unrelated to each other. This study aims to organize a
wide range of existing data optimization methodologies for deep learning from
the previous literature, and makes the effort to construct a comprehensive
taxonomy for them. The constructed taxonomy considers the diversity of split
dimensions, and deep sub-taxonomies are constructed for each dimension. On the
basis of the taxonomy, connections among the extensive data optimization
methods for deep learning are built in terms of four aspects. We probe into
rendering several promising and interesting future directions. The constructed
taxonomy and the revealed connections will enlighten the better understanding
of existing methods and the design of novel data optimization techniques.
Furthermore, our aspiration for this survey is to promote data optimization as
an independent subdivision of deep learning. A curated, up-to-date list of
resources related to data optimization in deep learning is available at
\url{https://github.com/YaoRujing/Data-Optimization}.","Ou Wu, Rujing Yao",2023,http://arxiv.org/abs/2310.16499v1
Spectral Clustering via Ensemble Deep Autoencoder Learning (SC-EDAE),"Recently, a number of works have studied clustering strategies that combine
classical clustering algorithms and deep learning methods. These approaches
follow either a sequential way, where a deep representation is learned using a
deep autoencoder before obtaining clusters with k-means, or a simultaneous way,
where deep representation and clusters are learned jointly by optimizing a
single objective function. Both strategies improve clustering performance,
however the robustness of these approaches is impeded by several deep
autoencoder setting issues, among which the weights initialization, the width
and number of layers or the number of epochs. To alleviate the impact of such
hyperparameters setting on the clustering performance, we propose a new model
which combines the spectral clustering and deep autoencoder strengths in an
ensemble learning framework. Extensive experiments on various benchmark
datasets demonstrate the potential and robustness of our approach compared to
state-of-the-art deep clustering methods.","Severine Affeldt, Lazhar Labiod, Mohamed Nadif",2019,http://arxiv.org/abs/1901.02291v2
"Deep Reservoir Networks with Learned Hidden Reservoir Weights using
  Direct Feedback Alignment","Deep Reservoir Computing has emerged as a new paradigm for deep learning,
which is based around the reservoir computing principle of maintaining random
pools of neurons combined with hierarchical deep learning. The reservoir
paradigm reflects and respects the high degree of recurrence in biological
brains, and the role that neuronal dynamics play in learning. However, one
issue hampering deep reservoir network development is that one cannot
backpropagate through the reservoir layers. Recent deep reservoir architectures
do not learn hidden or hierarchical representations in the same manner as deep
artificial neural networks, but rather concatenate all hidden reservoirs
together to perform traditional regression. Here we present a novel Deep
Reservoir Network for time series prediction and classification that learns
through the non-differentiable hidden reservoir layers using a
biologically-inspired backpropagation alternative called Direct Feedback
Alignment, which resembles global dopamine signal broadcasting in the brain. We
demonstrate its efficacy on two real world multidimensional time series
datasets.","Matthew Evanusa, Cornelia Fermüller, Yiannis Aloimonos",2020,http://arxiv.org/abs/2010.06209v3
How to fine-tune deep neural networks in few-shot learning?,"Deep learning has been widely used in data-intensive applications. However,
training a deep neural network often requires a large data set. When there is
not enough data available for training, the performance of deep learning models
is even worse than that of shallow networks. It has been proved that few-shot
learning can generalize to new tasks with few training samples. Fine-tuning of
a deep model is simple and effective few-shot learning method. However, how to
fine-tune deep learning models (fine-tune convolution layer or BN layer?) still
lack deep investigation. Hence, we study how to fine-tune deep models through
experimental comparison in this paper. Furthermore, the weight of the models is
analyzed to verify the feasibility of the fine-tuning method.","Peng Peng, Jiugen Wang",2020,http://arxiv.org/abs/2012.00204v1
Ensemble deep learning: A review,"Ensemble learning combines several individual models to obtain better
generalization performance. Currently, deep learning architectures are showing
better performance compared to the shallow or traditional models. Deep ensemble
learning models combine the advantages of both the deep learning models as well
as the ensemble learning such that the final model has better generalization
performance. This paper reviews the state-of-art deep ensemble models and hence
serves as an extensive summary for the researchers. The ensemble models are
broadly categorised into bagging, boosting, stacking, negative correlation
based deep ensemble models, explicit/implicit ensembles,
homogeneous/heterogeneous ensemble, decision fusion strategies based deep
ensemble models. Applications of deep ensemble models in different domains are
also briefly discussed. Finally, we conclude this paper with some potential
future research directions.","M. A. Ganaie, Minghui Hu, A. K. Malik, M. Tanveer, P. N. Suganthan",2021,http://arxiv.org/abs/2104.02395v3
Deep Learning for Iris Recognition: A Survey,"In this survey, we provide a comprehensive review of more than 200 papers,
technical reports, and GitHub repositories published over the last 10 years on
the recent developments of deep learning techniques for iris recognition,
covering broad topics on algorithm designs, open-source tools, open challenges,
and emerging research. First, we conduct a comprehensive analysis of deep
learning techniques developed for two main sub-tasks in iris biometrics:
segmentation and recognition. Second, we focus on deep learning techniques for
the robustness of iris recognition systems against presentation attacks and via
human-machine pairing. Third, we delve deep into deep learning techniques for
forensic application, especially in post-mortem iris recognition. Fourth, we
review open-source resources and tools in deep learning techniques for iris
recognition. Finally, we highlight the technical challenges, emerging research
trends, and outlook for the future of deep learning in iris recognition.","Kien Nguyen, Hugo Proença, Fernando Alonso-Fernandez",2022,http://arxiv.org/abs/2210.05866v1
Deep Learning and Computational Physics (Lecture Notes),"These notes were compiled as lecture notes for a course developed and taught
at the University of the Southern California. They should be accessible to a
typical engineering graduate student with a strong background in Applied
Mathematics.
  The main objective of these notes is to introduce a student who is familiar
with concepts in linear algebra and partial differential equations to select
topics in deep learning. These lecture notes exploit the strong connections
between deep learning algorithms and the more conventional techniques of
computational physics to achieve two goals. First, they use concepts from
computational physics to develop an understanding of deep learning algorithms.
Not surprisingly, many concepts in deep learning can be connected to similar
concepts in computational physics, and one can utilize this connection to
better understand these algorithms. Second, several novel deep learning
algorithms can be used to solve challenging problems in computational physics.
Thus, they offer someone who is interested in modeling a physical phenomena
with a complementary set of tools.","Deep Ray, Orazio Pinti, Assad A. Oberai",2023,http://arxiv.org/abs/2301.00942v1
"A Study on the Impact of Face Image Quality on Face Recognition in the
  Wild","Deep learning has received increasing interests in face recognition recently.
Large quantities of deep learning methods have been proposed to handle various
problems appeared in face recognition. Quite a lot deep methods claimed that
they have gained or even surpassed human-level face verification performance in
certain databases. As we know, face image quality poses a great challenge to
traditional face recognition methods, e.g. model-driven methods with
hand-crafted features. However, a little research focus on the impact of face
image quality on deep learning methods, and even human performance. Therefore,
we raise a question: Is face image quality still one of the challenges for deep
learning based face recognition, especially in unconstrained condition. Based
on this, we further investigate this problem on human level. In this paper, we
partition face images into three different quality sets to evaluate the
performance of deep learning methods on cross-quality face images in the wild,
and then design a human face verification experiment on these cross-quality
data. The result indicates that quality issue still needs to be studied
thoroughly in deep learning, human own better capability in building the
relations between different face images with large quality gaps, and saying
deep learning method surpasses human-level is too optimistic.",Na Zhang,2023,http://arxiv.org/abs/2307.02679v1
"Deep Fast Vision: A Python Library for Accelerated Deep Transfer
  Learning Vision Prototyping","Deep learning-based vision is characterized by intricate frameworks that
often necessitate a profound understanding, presenting a barrier to newcomers
and limiting broad adoption. With many researchers grappling with the
constraints of smaller datasets, there's a pronounced reliance on pre-trained
neural networks, especially for tasks such as image classification. This
reliance is further intensified in niche imaging areas where obtaining vast
datasets is challenging. Despite the widespread use of transfer learning as a
remedy to the small dataset dilemma, a conspicuous absence of tailored auto-ML
solutions persists. Addressing these challenges is ""Deep Fast Vision"", a python
library that streamlines the deep learning process. This tool offers a
user-friendly experience, enabling results through a simple nested dictionary
definition, helping to democratize deep learning for non-experts. Designed for
simplicity and scalability, Deep Fast Vision appears as a bridge, connecting
the complexities of existing deep learning frameworks with the needs of a
diverse user base.",Fabi Prezja,2023,http://arxiv.org/abs/2311.06169v1
"Computability of Classification and Deep Learning: From Theoretical
  Limits to Practical Feasibility through Quantization","The unwavering success of deep learning in the past decade led to the
increasing prevalence of deep learning methods in various application fields.
However, the downsides of deep learning, most prominently its lack of
trustworthiness, may not be compatible with safety-critical or
high-responsibility applications requiring stricter performance guarantees.
Recently, several instances of deep learning applications have been shown to be
subject to theoretical limitations of computability, undermining the
feasibility of performance guarantees when employed on real-world computers. We
extend the findings by studying computability in the deep learning framework
from two perspectives: From an application viewpoint in the context of
classification problems and a general limitation viewpoint in the context of
training neural networks. In particular, we show restrictions on the
algorithmic solvability of classification problems that also render the
algorithmic detection of failure in computations in a general setting
infeasible. Subsequently, we prove algorithmic limitations in training deep
neural networks even in cases where the underlying problem is well-behaved.
Finally, we end with a positive observation, showing that in quantized versions
of classification and deep network training, computability restrictions do not
arise or can be overcome to a certain degree.","Holger Boche, Vit Fojtik, Adalbert Fono, Gitta Kutyniok",2024,http://arxiv.org/abs/2408.06212v1
Deep Transductive Semi-supervised Maximum Margin Clustering,"Semi-supervised clustering is an very important topic in machine learning and
computer vision. The key challenge of this problem is how to learn a metric,
such that the instances sharing the same label are more likely close to each
other on the embedded space. However, little attention has been paid to learn
better representations when the data lie on non-linear manifold. Fortunately,
deep learning has led to great success on feature learning recently. Inspired
by the advances of deep learning, we propose a deep transductive
semi-supervised maximum margin clustering approach. More specifically, given
pairwise constraints, we exploit both labeled and unlabeled data to learn a
non-linear mapping under maximum margin framework for clustering analysis.
Thus, our model unifies transductive learning, feature learning and maximum
margin techniques in the semi-supervised clustering framework. We pretrain the
deep network structure with restricted Boltzmann machines (RBMs) layer by layer
greedily, and optimize our objective function with gradient descent. By
checking the most violated constraints, our approach updates the model
parameters through error backpropagation, in which deep features are learned
automatically. The experimental results shows that our model is significantly
better than the state of the art on semi-supervised clustering.",Gang Chen,2015,http://arxiv.org/abs/1501.06237v1
"Deep Learning Approach for Enhanced Cyber Threat Indicators in Twitter
  Stream","In recent days, the amount of Cyber Security text data shared via social
media resources mainly Twitter has increased. An accurate analysis of this data
can help to develop cyber threat situational awareness framework for a cyber
threat. This work proposes a deep learning based approach for tweet data
analysis. To convert the tweets into numerical representations, various text
representations are employed. These features are feed into deep learning
architecture for optimal feature extraction as well as classification. Various
hyperparameter tuning approaches are used for identifying optimal text
representation method as well as optimal network parameters and network
structures for deep learning models. For comparative analysis, the classical
text representation method with classical machine learning algorithm is
employed. From the detailed analysis of experiments, we found that the deep
learning architecture with advanced text representation methods performed
better than the classical text representation and classical machine learning
algorithms. The primary reason for this is that the advanced text
representation methods have the capability to learn sequential properties which
exist among the textual data and deep learning architectures learns the optimal
features along with decreasing the feature size.","Simran K, Prathiksha Balakrishna, Vinayakumar R, Soman KP",2020,http://arxiv.org/abs/2004.00503v1
"Deep Learning is Singular, and That's Good","In singular models, the optimal set of parameters forms an analytic set with
singularities and classical statistical inference cannot be applied to such
models. This is significant for deep learning as neural networks are singular
and thus ""dividing"" by the determinant of the Hessian or employing the Laplace
approximation are not appropriate. Despite its potential for addressing
fundamental issues in deep learning, singular learning theory appears to have
made little inroads into the developing canon of deep learning theory. Via a
mix of theory and experiment, we present an invitation to singular learning
theory as a vehicle for understanding deep learning and suggest important
future work to make singular learning theory directly applicable to how deep
learning is performed in practice.","Daniel Murfet, Susan Wei, Mingming Gong, Hui Li, Jesse Gell-Redman, Thomas Quella",2020,http://arxiv.org/abs/2010.11560v1
Automated Architecture Design for Deep Neural Networks,"Machine learning has made tremendous progress in recent years and received
large amounts of public attention. Though we are still far from designing a
full artificially intelligent agent, machine learning has brought us many
applications in which computers solve human learning tasks remarkably well.
Much of this progress comes from a recent trend within machine learning, called
deep learning. Deep learning models are responsible for many state-of-the-art
applications of machine learning. Despite their success, deep learning models
are hard to train, very difficult to understand, and often times so complex
that training is only possible on very large GPU clusters. Lots of work has
been done on enabling neural networks to learn efficiently. However, the design
and architecture of such neural networks is often done manually through trial
and error and expert knowledge. This thesis inspects different approaches,
existing and novel, to automate the design of deep feedforward neural networks
in an attempt to create less complex models with good performance that take
away the burden of deciding on an architecture and make it more efficient to
design and train such deep networks.",Steven Abreu,2019,http://arxiv.org/abs/1908.10714v1
"Learning Empirical Bregman Divergence for Uncertain Distance
  Representation","Deep metric learning techniques have been used for visual representation in
various supervised and unsupervised learning tasks through learning embeddings
of samples with deep networks. However, classic approaches, which employ a
fixed distance metric as a similarity function between two embeddings, may lead
to suboptimal performance for capturing the complex data distribution. The
Bregman divergence generalizes measures of various distance metrics and arises
throughout many fields of deep metric learning. In this paper, we first show
how deep metric learning loss can arise from the Bregman divergence. We then
introduce a novel method for learning empirical Bregman divergence directly
from data based on parameterizing the convex function underlying the Bregman
divergence with a deep learning setting. We further experimentally show that
our approach performs effectively on five popular public datasets compared to
other SOTA deep metric learning methods, particularly for pattern recognition
problems.","Zhiyuan Li, Ziru Liu, Anna Zou, Anca L. Ralescu",2023,http://arxiv.org/abs/2304.07689v3
Cross multiscale vision transformer for deep fake detection,"The proliferation of deep fake technology poses significant challenges to
digital media authenticity, necessitating robust detection mechanisms. This
project evaluates deep fake detection using the SP Cup's 2025 deep fake
detection challenge dataset. We focused on exploring various deep learning
models for detecting deep fake content, utilizing traditional deep learning
techniques alongside newer architectures. Our approach involved training a
series of models and rigorously assessing their performance using metrics such
as accuracy.","Akhshan P, Taneti Sanjay, Chandrakala S",2025,http://arxiv.org/abs/2502.00833v1
"Integrating Deep Reinforcement Learning Networks with Health System
  Simulations","Background and motivation: Combining Deep Reinforcement Learning (Deep RL)
and Health Systems Simulations has significant potential, for both research
into improving Deep RL performance and safety, and in operational practice.
While individual toolkits exist for Deep RL and Health Systems Simulations, no
framework to integrate the two has been established.
  Aim: Provide a framework for integrating Deep RL Networks with Health System
Simulations, and to ensure this framework is compatible with Deep RL agents
that have been developed and tested using OpenAI Gym.
  Methods: We developed our framework based on the OpenAI Gym framework, and
demonstrate its use on a simple hospital bed capacity model. We built the Deep
RL agents using PyTorch, and the Hospital Simulatation using SimPy.
  Results: We demonstrate example models using a Double Deep Q Network or a
Duelling Double Deep Q Network as the Deep RL agent.
  Conclusion: SimPy may be used to create Health System Simulations that are
compatible with agents developed and tested on OpenAI Gym environments.
  GitHub repository of code:
https://github.com/MichaelAllen1966/learninghospital","Michael Allen, Thomas Monks",2020,http://arxiv.org/abs/2008.07434v1
"Deep network as memory space: complexity, generalization, disentangled
  representation and interpretability","By bridging deep networks and physics, the programme of geometrization of
deep networks was proposed as a framework for the interpretability of deep
learning systems. Following this programme we can apply two key ideas of
physics, the geometrization of physics and the least action principle, on deep
networks and deliver a new picture of deep networks: deep networks as memory
space of information, where the capacity, robustness and efficiency of the
memory are closely related with the complexity, generalization and
disentanglement of deep networks. The key components of this understanding
include:(1) a Fisher metric based formulation of the network complexity; (2)the
least action (complexity=action) principle on deep networks and (3)the geometry
built on deep network configurations. We will show how this picture will bring
us a new understanding of the interpretability of deep learning systems.","X. Dong, L. Zhou",2019,http://arxiv.org/abs/1907.06572v1
"Why Do Deep Residual Networks Generalize Better than Deep Feedforward
  Networks? -- A Neural Tangent Kernel Perspective","Deep residual networks (ResNets) have demonstrated better generalization
performance than deep feedforward networks (FFNets). However, the theory behind
such a phenomenon is still largely unknown. This paper studies this fundamental
problem in deep learning from a so-called ""neural tangent kernel"" perspective.
Specifically, we first show that under proper conditions, as the width goes to
infinity, training deep ResNets can be viewed as learning reproducing kernel
functions with some kernel function. We then compare the kernel of deep ResNets
with that of deep FFNets and discover that the class of functions induced by
the kernel of FFNets is asymptotically not learnable, as the depth goes to
infinity. In contrast, the class of functions induced by the kernel of ResNets
does not exhibit such degeneracy. Our discovery partially justifies the
advantages of deep ResNets over deep FFNets in generalization abilities.
Numerical results are provided to support our claim.","Kaixuan Huang, Yuqing Wang, Molei Tao, Tuo Zhao",2020,http://arxiv.org/abs/2002.06262v2
A C++ library for Multimodal Deep Learning,"MDL, Multimodal Deep Learning Library, is a deep learning framework that
supports multiple models, and this document explains its philosophy and
functionality. MDL runs on Linux, Mac, and Unix platforms. It depends on
OpenCV.",Jian Jin,2015,http://arxiv.org/abs/1512.06927v4
"Proceedings of the First International Workshop on Deep Learning and
  Music","Proceedings of the First International Workshop on Deep Learning and Music,
joint with IJCNN, Anchorage, US, May 17-18, 2017","Dorien Herremans, Ching-Hua Chuan",2017,http://arxiv.org/abs/1706.08675v1
Machine listening intelligence,"This manifesto paper will introduce machine listening intelligence, an
integrated research framework for acoustic and musical signals modelling, based
on signal processing, deep learning and computational musicology.",C. E. Cella,2017,http://arxiv.org/abs/1706.09557v1
Mathematics of Deep Learning,"Recently there has been a dramatic increase in the performance of recognition
systems due to the introduction of deep architectures for representation
learning and classification. However, the mathematical reasons for this success
remain elusive. This tutorial will review recent work that aims to provide a
mathematical justification for several properties of deep networks, such as
global optimality, geometric stability, and invariance of the learned
representations.","Rene Vidal, Joan Bruna, Raja Giryes, Stefano Soatto",2017,http://arxiv.org/abs/1712.04741v1
NECA: Network-Embedded Deep Representation Learning for Categorical Data,"We propose NECA, a deep representation learning method for categorical data.
Built upon the foundations of network embedding and deep unsupervised
representation learning, NECA deeply embeds the intrinsic relationship among
attribute values and explicitly expresses data objects with numeric vector
representations. Designed specifically for categorical data, NECA can support
important downstream data mining tasks, such as clustering. Extensive
experimental analysis demonstrated the effectiveness of NECA.","Xiaonan Gao, Sen Wu, Wenjun Zhou",2022,http://arxiv.org/abs/2205.12752v1
"Leveraging Topological Maps in Deep Reinforcement Learning for
  Multi-Object Navigation","This work addresses the challenge of navigating expansive spaces with sparse
rewards through Reinforcement Learning (RL). Using topological maps, we elevate
elementary actions to object-oriented macro actions, enabling a simple Deep
Q-Network (DQN) agent to solve otherwise practically impossible environments.","Simon Hakenes, Tobias Glasmachers",2023,http://arxiv.org/abs/2310.10250v1
"Model-Based Robust Deep Learning: Generalizing to Natural,
  Out-of-Distribution Data","While deep learning has resulted in major breakthroughs in many application
domains, the frameworks commonly used in deep learning remain fragile to
artificially-crafted and imperceptible changes in the data. In response to this
fragility, adversarial training has emerged as a principled approach for
enhancing the robustness of deep learning with respect to norm-bounded
perturbations. However, there are other sources of fragility for deep learning
that are arguably more common and less thoroughly studied. Indeed, natural
variation such as lighting or weather conditions can significantly degrade the
accuracy of trained neural networks, proving that such natural variation
presents a significant challenge for deep learning.
  In this paper, we propose a paradigm shift from perturbation-based
adversarial robustness toward model-based robust deep learning. Our objective
is to provide general training algorithms that can be used to train deep neural
networks to be robust against natural variation in data. Critical to our
paradigm is first obtaining a model of natural variation which can be used to
vary data over a range of natural conditions. Such models may be either known a
priori or else learned from data. In the latter case, we show that deep
generative models can be used to learn models of natural variation that are
consistent with realistic conditions. We then exploit such models in three
novel model-based robust training algorithms in order to enhance the robustness
of deep learning with respect to the given model. Our extensive experiments
show that across a variety of naturally-occurring conditions and across various
datasets, deep neural networks trained with our model-based algorithms
significantly outperform both standard deep learning algorithms as well as
norm-bounded robust deep learning algorithms.","Alexander Robey, Hamed Hassani, George J. Pappas",2020,http://arxiv.org/abs/2005.10247v2
"Learning Discriminative Features using Encoder-Decoder type Deep Neural
  Nets","As machine learning is applied to an increasing variety of complex problems,
which are defined by high dimensional and complex data sets, the necessity for
task oriented feature learning grows in importance. With the advancement of
Deep Learning algorithms, various successful feature learning techniques have
evolved. In this paper, we present a novel way of learning discriminative
features by training Deep Neural Nets which have Encoder or Decoder type
architecture similar to an Autoencoder. We demonstrate that our approach can
learn discriminative features which can perform better at pattern
classification tasks when the number of training samples is relatively small in
size.","Vishwajeet Singh, Killamsetti Ravi Kumar, K Eswaran",2016,http://arxiv.org/abs/1607.01354v1
Minimal Achievable Sufficient Statistic Learning,"We introduce Minimal Achievable Sufficient Statistic (MASS) Learning, a
training method for machine learning models that attempts to produce minimal
sufficient statistics with respect to a class of functions (e.g. deep networks)
being optimized over. In deriving MASS Learning, we also introduce Conserved
Differential Information (CDI), an information-theoretic quantity that - unlike
standard mutual information - can be usefully applied to
deterministically-dependent continuous random variables like the input and
output of a deep network. In a series of experiments, we show that deep
networks trained with MASS Learning achieve competitive performance on
supervised learning and uncertainty quantification benchmarks.","Milan Cvitkovic, Günther Koliander",2019,http://arxiv.org/abs/1905.07822v2
"Improving Robustness of Deep Convolutional Neural Networks via
  Multiresolution Learning","The current learning process of deep learning, regardless of any deep neural
network (DNN) architecture and/or learning algorithm used, is essentially a
single resolution training. We explore multiresolution learning and show that
multiresolution learning can significantly improve robustness of DNN models for
both 1D signal and 2D signal (image) prediction problems. We demonstrate this
improvement in terms of both noise and adversarial robustness as well as with
small training dataset size. Our results also suggest that it may not be
necessary to trade standard accuracy for robustness with multiresolution
learning, which is, interestingly, contrary to the observation obtained from
the traditional single resolution learning setting.","Hongyan Zhou, Yao Liang",2023,http://arxiv.org/abs/2309.13752v2
"Holarchic Structures for Decentralized Deep Learning - A Performance
  Analysis","Structure plays a key role in learning performance. In centralized
computational systems, hyperparameter optimization and regularization
techniques such as dropout are computational means to enhance learning
performance by adjusting the deep hierarchical structure. However, in
decentralized deep learning by the Internet of Things, the structure is an
actual network of autonomous interconnected devices such as smart phones that
interact via complex network protocols. Self-adaptation of the learning
structure is a challenge. Uncertainties such as network latency, node and link
failures or even bottlenecks by limited processing capacity and energy
availability can signif- icantly downgrade learning performance. Network
self-organization and self-management is complex, while it requires additional
computational and network resources that hinder the feasibility of
decentralized deep learning. In contrast, this paper introduces a self-adaptive
learning approach based on holarchic learning structures for exploring,
mitigating and boosting learning performance in distributed environments with
uncertainties. A large-scale performance analysis with 864000 experiments fed
with synthetic and real-world data from smart grid and smart city pilot
projects confirm the cost-effectiveness of holarchic structures for
decentralized deep learning.","Evangelos Pournaras, Srivatsan Yadhunathan, Ada Diaconescu",2018,http://arxiv.org/abs/1805.02686v2
Generalization and Regularization in DQN,"Deep reinforcement learning algorithms have shown an impressive ability to
learn complex control policies in high-dimensional tasks. However, despite the
ever-increasing performance on popular benchmarks, policies learned by deep
reinforcement learning algorithms can struggle to generalize when evaluated in
remarkably similar environments. In this paper we propose a protocol to
evaluate generalization in reinforcement learning through different modes of
Atari 2600 games. With that protocol we assess the generalization capabilities
of DQN, one of the most traditional deep reinforcement learning algorithms, and
we provide evidence suggesting that DQN overspecializes to the training
environment. We then comprehensively evaluate the impact of dropout and
$\ell_2$ regularization, as well as the impact of reusing learned
representations to improve the generalization capabilities of DQN. Despite
regularization being largely underutilized in deep reinforcement learning, we
show that it can, in fact, help DQN learn more general features. These features
can be reused and fine-tuned on similar tasks, considerably improving DQN's
sample efficiency.","Jesse Farebrother, Marlos C. Machado, Michael Bowling",2018,http://arxiv.org/abs/1810.00123v3
Machine learning and deep learning,"Today, intelligent systems that offer artificial intelligence capabilities
often rely on machine learning. Machine learning describes the capacity of
systems to learn from problem-specific training data to automate the process of
analytical model building and solve associated tasks. Deep learning is a
machine learning concept based on artificial neural networks. For many
applications, deep learning models outperform shallow machine learning models
and traditional data analysis approaches. In this article, we summarize the
fundamentals of machine learning and deep learning to generate a broader
understanding of the methodical underpinning of current intelligent systems. In
particular, we provide a conceptual distinction between relevant terms and
concepts, explain the process of automated analytical model building through
machine learning and deep learning, and discuss the challenges that arise when
implementing such intelligent systems in the field of electronic markets and
networked business. These naturally go beyond technological aspects and
highlight issues in human-machine interaction and artificial intelligence
servitization.","Christian Janiesch, Patrick Zschech, Kai Heinrich",2021,http://arxiv.org/abs/2104.05314v2
"Image Companding and Inverse Halftoning using Deep Convolutional Neural
  Networks","In this paper, we introduce deep learning technology to tackle two
traditional low-level image processing problems, companding and inverse
halftoning. We make two main contributions. First, to the best knowledge of the
authors, this is the first work that has successfully developed deep learning
based solutions to these two traditional low-level image processing problems.
This not only introduces new methods to tackle well-known image processing
problems but also demonstrates the power of deep learning in solving
traditional signal processing problems. Second, we have developed an effective
deep learning algorithm based on insights into the properties of visual quality
of images and the internal representation properties of a deep convolutional
neural network (CNN). We train a deep CNN as a nonlinear transformation
function to map a low bit depth image to higher bit depth or from a halftone
image to a continuous tone image. We also employ another pretrained deep CNN as
a feature extractor to derive visually important features to construct the
objective function for the training of the mapping CNN. We present experimental
results to demonstrate the effectiveness of the new deep learning based
solutions.","Xianxu Hou, Guoping Qiu",2017,http://arxiv.org/abs/1707.00116v2
Deep Learning for Click-Through Rate Estimation,"Click-through rate (CTR) estimation plays as a core function module in
various personalized online services, including online advertising, recommender
systems, and web search etc. From 2015, the success of deep learning started to
benefit CTR estimation performance and now deep CTR models have been widely
applied in many industrial platforms. In this survey, we provide a
comprehensive review of deep learning models for CTR estimation tasks. First,
we take a review of the transfer from shallow to deep CTR models and explain
why going deep is a necessary trend of development. Second, we concentrate on
explicit feature interaction learning modules of deep CTR models. Then, as an
important perspective on large platforms with abundant user histories, deep
behavior models are discussed. Moreover, the recently emerged automated methods
for deep CTR architecture design are presented. Finally, we summarize the
survey and discuss the future prospects of this field.","Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, Xiuqiang He",2021,http://arxiv.org/abs/2104.10584v1
Deep Learning At Scale and At Ease,"Recently, deep learning techniques have enjoyed success in various multimedia
applications, such as image classification and multi-modal data analysis. Large
deep learning models are developed for learning rich representations of complex
data. There are two challenges to overcome before deep learning can be widely
adopted in multimedia and other applications. One is usability, namely the
implementation of different models and training algorithms must be done by
non-experts without much effort especially when the model is large and complex.
The other is scalability, that is the deep learning system must be able to
provision for a huge demand of computing resources for training large models
with massive datasets. To address these two challenges, in this paper, we
design a distributed deep learning platform called SINGA which has an intuitive
programming model based on the common layer abstraction of deep learning
models. Good scalability is achieved through flexible distributed training
architecture and specific optimization techniques. SINGA runs on GPUs as well
as on CPUs, and we show that it outperforms many other state-of-the-art deep
learning systems. Our experience with developing and training deep learning
models for real-life multimedia applications in SINGA shows that the platform
is both usable and scalable.","Wei Wang, Gang Chen, Haibo Chen, Tien Tuan Anh Dinh, Jinyang Gao, Beng Chin Ooi, Kian-Lee Tan, Sheng Wang",2016,http://arxiv.org/abs/1603.07846v1
Deep Learning on Mobile Devices - A Review,"Recent breakthroughs in deep learning and artificial intelligence
technologies have enabled numerous mobile applications. While traditional
computation paradigms rely on mobile sensing and cloud computing, deep learning
implemented on mobile devices provides several advantages. These advantages
include low communication bandwidth, small cloud computing resource cost, quick
response time, and improved data privacy. Research and development of deep
learning on mobile and embedded devices has recently attracted much attention.
This paper provides a timely review of this fast-paced field to give the
researcher, engineer, practitioner, and graduate student a quick grasp on the
recent advancements of deep learning on mobile devices. In this paper, we
discuss hardware architectures for mobile deep learning, including Field
Programmable Gate Arrays, Application Specific Integrated Circuit, and recent
mobile Graphic Processing Units. We present Size, Weight, Area and Power
considerations and their relation to algorithm optimizations, such as
quantization, pruning, compression, and approximations that simplify
computation while retaining performance accuracy. We cover existing systems and
give a state-of-the-industry review of TensorFlow, MXNet, Mobile AI Compute
Engine, and Paddle-mobile deep learning platform. We discuss resources for
mobile deep learning practitioners, including tools, libraries, models, and
performance benchmarks. We present applications of various mobile sensing
modalities to industries, ranging from robotics, healthcare and multi-media,
biometrics to autonomous drive and defense. We address the key deep learning
challenges to overcome, including low quality data, and small
training/adaptation data sets. In addition, the review provides numerous
citations and links to existing code bases implementing various technologies.",Yunbin Deng,2019,http://arxiv.org/abs/1904.09274v1
NeurIPS 2020 Competition: Predicting Generalization in Deep Learning,"Understanding generalization in deep learning is arguably one of the most
important questions in deep learning. Deep learning has been successfully
adopted to a large number of problems ranging from pattern recognition to
complex decision making, but many recent researchers have raised many concerns
about deep learning, among which the most important is generalization. Despite
numerous attempts, conventional statistical learning approaches have yet been
able to provide a satisfactory explanation on why deep learning works. A recent
line of works aims to address the problem by trying to predict the
generalization performance through complexity measures. In this competition, we
invite the community to propose complexity measures that can accurately predict
generalization of models. A robust and general complexity measure would
potentially lead to a better understanding of deep learning's underlying
mechanism and behavior of deep models on unseen data, or shed light on better
generalization bounds. All these outcomes will be important for making deep
learning more robust and reliable.","Yiding Jiang, Pierre Foret, Scott Yak, Daniel M. Roy, Hossein Mobahi, Gintare Karolina Dziugaite, Samy Bengio, Suriya Gunasekar, Isabelle Guyon, Behnam Neyshabur",2020,http://arxiv.org/abs/2012.07976v1
"On the minimax optimality and superiority of deep neural network
  learning over sparse parameter spaces","Deep learning has been applied to various tasks in the field of machine
learning and has shown superiority to other common procedures such as kernel
methods. To provide a better theoretical understanding of the reasons for its
success, we discuss the performance of deep learning and other methods on a
nonparametric regression problem with a Gaussian noise. Whereas existing
theoretical studies of deep learning have been based mainly on mathematical
theories of well-known function classes such as H\""{o}lder and Besov classes,
we focus on function classes with discontinuity and sparsity, which are those
naturally assumed in practice. To highlight the effectiveness of deep learning,
we compare deep learning with a class of linear estimators representative of a
class of shallow estimators. It is shown that the minimax risk of a linear
estimator on the convex hull of a target function class does not differ from
that of the original target function class. This results in the suboptimality
of linear methods over a simple but non-convex function class, on which deep
learning can attain nearly the minimax-optimal rate. In addition to this
extreme case, we consider function classes with sparse wavelet coefficients. On
these function classes, deep learning also attains the minimax rate up to log
factors of the sample size, and linear methods are still suboptimal if the
assumed sparsity is strong. We also point out that the parameter sharing of
deep neural networks can remarkably reduce the complexity of the model in our
setting.","Satoshi Hayakawa, Taiji Suzuki",2019,http://arxiv.org/abs/1905.09195v2
"SenseFi: A Library and Benchmark on Deep-Learning-Empowered WiFi Human
  Sensing","WiFi sensing has been evolving rapidly in recent years. Empowered by
propagation models and deep learning methods, many challenging applications are
realized such as WiFi-based human activity recognition and gesture recognition.
However, in contrast to deep learning for visual recognition and natural
language processing, no sufficiently comprehensive public benchmark exists. In
this paper, we review the recent progress on deep learning enabled WiFi
sensing, and then propose a benchmark, SenseFi, to study the effectiveness of
various deep learning models for WiFi sensing. These advanced models are
compared in terms of distinct sensing tasks, WiFi platforms, recognition
accuracy, model size, computational complexity, feature transferability, and
adaptability of unsupervised learning. It is also regarded as a tutorial for
deep learning based WiFi sensing, starting from CSI hardware platform to
sensing algorithms. The extensive experiments provide us with experiences in
deep model design, learning strategy skills and training techniques for
real-world applications. To the best of our knowledge, this is the first
benchmark with an open-source library for deep learning in WiFi sensing
research. The benchmark codes are available at
https://github.com/xyanchen/WiFi-CSI-Sensing-Benchmark.","Jianfei Yang, Xinyan Chen, Dazhuo Wang, Han Zou, Chris Xiaoxuan Lu, Sumei Sun, Lihua Xie",2022,http://arxiv.org/abs/2207.07859v3
"An inclusive review on deep learning techniques and their scope in
  handwriting recognition","Deep learning expresses a category of machine learning algorithms that have
the capability to combine raw inputs into intermediate features layers. These
deep learning algorithms have demonstrated great results in different fields.
Deep learning has particularly witnessed for a great achievement of human level
performance across a number of domains in computer vision and pattern
recognition. For the achievement of state-of-the-art performances in diverse
domains, the deep learning used different architectures and these architectures
used activation functions to perform various computations between hidden and
output layers of any architecture. This paper presents a survey on the existing
studies of deep learning in handwriting recognition field. Even though the
recent progress indicates that the deep learning methods has provided valuable
means for speeding up or proving accurate results in handwriting recognition,
but following from the extensive literature survey, the present study finds
that the deep learning has yet to revolutionize more and has to resolve many of
the most pressing challenges in this field, but promising advances have been
made on the prior state of the art. Additionally, an inadequate availability of
labelled data to train presents problems in this domain. Nevertheless, the
present handwriting recognition survey foresees deep learning enabling changes
at both bench and bedside with the potential to transform several domains as
image processing, speech recognition, computer vision, machine translation,
robotics and control, medical imaging, medical information processing,
bio-informatics, natural language processing, cyber security, and many others.","Sukhdeep Singh, Sudhir Rohilla, Anuj Sharma",2024,http://arxiv.org/abs/2404.08011v1
Pre-screening breast cancer with machine learning and deep learning,"We suggest that deep learning can be used for pre-screening cancer by
analyzing demographic and anthropometric information of patients, as well as
biological markers obtained from routine blood samples and relative risks
obtained from meta-analysis and international databases. We applied feature
selection algorithms to a database of 116 women, including 52 healthy women and
64 women diagnosed with breast cancer, to identify the best pre-screening
predictors of cancer. We utilized the best predictors to perform k-fold Monte
Carlo cross-validation experiments that compare deep learning against
traditional machine learning algorithms. Our results indicate that a deep
learning model with an input-layer architecture that is fine-tuned using
feature selection can effectively distinguish between patients with and without
cancer. Additionally, compared to machine learning, deep learning has the
lowest uncertainty in its predictions. These findings suggest that deep
learning algorithms applied to cancer pre-screening offer a radiation-free,
non-invasive, and affordable complement to screening methods based on imagery.
The implementation of deep learning algorithms in cancer pre-screening offer
opportunities to identify individuals who may require imaging-based screening,
can encourage self-examination, and decrease the psychological externalities
associated with false positives in cancer screening. The integration of deep
learning algorithms for both screening and pre-screening will ultimately lead
to earlier detection of malignancy, reducing the healthcare and societal burden
associated to cancer treatment.","Rolando Gonzales Martinez, Daan-Max van Dongen",2023,http://arxiv.org/abs/2302.02406v1
"Deep Learning with CNNs: A Compact Holistic Tutorial with Focus on
  Supervised Regression (Preprint)","In this tutorial, we present a compact and holistic discussion of Deep
Learning with a focus on Convolutional Neural Networks (CNNs) and supervised
regression. While there are numerous books and articles on the individual
topics we cover, comprehensive and detailed tutorials that address Deep
Learning from a foundational yet rigorous and accessible perspective are rare.
Most resources on CNNs are either too advanced, focusing on cutting-edge
architectures, or too narrow, addressing only specific applications like image
classification.This tutorial not only summarizes the most relevant concepts but
also provides an in-depth exploration of each, offering a complete yet agile
set of ideas. Moreover, we highlight the powerful synergy between learning
theory, statistic, and machine learning, which together underpin the Deep
Learning and CNN frameworks. We aim for this tutorial to serve as an optimal
resource for students, professors, and anyone interested in understanding the
foundations of Deep Learning. Upon acceptance we will provide an accompanying
repository under
\href{https://github.com/neoglez/deep-learning-tutorial}{https://github.com/neoglez/deep-learning-tutorial}
  Keywords: Tutorial, Deep Learning, Convolutional Neural Networks, Machine
Learning.","Yansel Gonzalez Tejeda, Helmut A. Mayer",2024,http://arxiv.org/abs/2408.12308v3
Deep Learning for Computational Chemistry,"The rise and fall of artificial neural networks is well documented in the
scientific literature of both computer science and computational chemistry. Yet
almost two decades later, we are now seeing a resurgence of interest in deep
learning, a machine learning algorithm based on multilayer neural networks.
Within the last few years, we have seen the transformative impact of deep
learning in many domains, particularly in speech recognition and computer
vision, to the extent that the majority of expert practitioners in those field
are now regularly eschewing prior established models in favor of deep learning
models. In this review, we provide an introductory overview into the theory of
deep neural networks and their unique properties that distinguish them from
traditional machine learning algorithms used in cheminformatics. By providing
an overview of the variety of emerging applications of deep neural networks, we
highlight its ubiquity and broad applicability to a wide range of challenges in
the field, including QSAR, virtual screening, protein structure prediction,
quantum chemistry, materials design and property prediction. In reviewing the
performance of deep neural networks, we observed a consistent outperformance
against non-neural networks state-of-the-art models across disparate research
topics, and deep neural network based models often exceeded the ""glass ceiling""
expectations of their respective tasks. Coupled with the maturity of
GPU-accelerated computing for training deep neural networks and the exponential
growth of chemical data on which to train these networks on, we anticipate that
deep learning algorithms will be a valuable tool for computational chemistry.","Garrett B. Goh, Nathan O. Hodas, Abhinav Vishnu",2017,http://arxiv.org/abs/1701.04503v1
Face Recognition System,"Deep learning is one of the new and important branches in machine learning.
Deep learning refers to a set of algorithms that solve various problems such as
images and texts by using various machine learning algorithms in multi-layer
neural networks. Deep learning can be classified as a neural network from the
general category, but there are many changes in the concrete realization. At
the core of deep learning is feature learning, which is designed to obtain
hierarchical information through hierarchical networks, so as to solve the
important problems that previously required artificial design features. Deep
Learning is a framework that contains several important algorithms. For
different applications (images, voice, text), you need to use different network
models to achieve better results. With the development of deep learning and the
introduction of deep convolutional neural networks, the accuracy and speed of
face recognition have made great strides. However, as we said above, the
results from different networks and models are very different. In this paper,
facial features are extracted by merging and comparing multiple models, and
then a deep neural network is constructed to train and construct the combined
features. In this way, the advantages of multiple models can be combined to
mention the recognition accuracy. After getting a model with high accuracy, we
build a product model. This article compares the pure-client model with the
server-client model, analyzes the pros and cons of the two models, and analyzes
the various commercial products that are required for the server-client model.","Yang Li, Sangwhan Cha",2019,http://arxiv.org/abs/1901.02452v1
"Deep Confidence: A Computationally Efficient Framework for Calculating
  Reliable Errors for Deep Neural Networks","Deep learning architectures have proved versatile in a number of drug
discovery applications, including the modelling of in vitro compound activity.
While controlling for prediction confidence is essential to increase the trust,
interpretability and usefulness of virtual screening models in drug discovery,
techniques to estimate the reliability of the predictions generated with deep
learning networks remain largely underexplored. Here, we present Deep
Confidence, a framework to compute valid and efficient confidence intervals for
individual predictions using the deep learning technique Snapshot Ensembling
and conformal prediction. Specifically, Deep Confidence generates an ensemble
of deep neural networks by recording the network parameters throughout the
local minima visited during the optimization phase of a single neural network.
This approach serves to derive a set of base learners (i.e., snapshots) with
comparable predictive power on average, that will however generate slightly
different predictions for a given instance. The variability across base
learners and the validation residuals are in turn harnessed to compute
confidence intervals using the conformal prediction framework. Using a set of
24 diverse IC50 data sets from ChEMBL 23, we show that Snapshot Ensembles
perform on par with Random Forest (RF) and ensembles of independently trained
deep neural networks. In addition, we find that the confidence regions
predicted using the Deep Confidence framework span a narrower set of values.
Overall, Deep Confidence represents a highly versatile error prediction
framework that can be applied to any deep learning-based application at no
extra computational cost.","Isidro Cortes-Ciriano, Andreas Bender",2018,http://arxiv.org/abs/1809.09060v1
A Probabilistic Theory of Deep Learning,"A grand challenge in machine learning is the development of computational
algorithms that match or outperform humans in perceptual inference tasks that
are complicated by nuisance variation. For instance, visual object recognition
involves the unknown object position, orientation, and scale in object
recognition while speech recognition involves the unknown voice pronunciation,
pitch, and speed. Recently, a new breed of deep learning algorithms have
emerged for high-nuisance inference tasks that routinely yield pattern
recognition systems with near- or super-human capabilities. But a fundamental
question remains: Why do they work? Intuitions abound, but a coherent framework
for understanding, analyzing, and synthesizing deep learning architectures has
remained elusive. We answer this question by developing a new probabilistic
framework for deep learning based on the Deep Rendering Model: a generative
probabilistic model that explicitly captures latent nuisance variation. By
relaxing the generative model to a discriminative one, we can recover two of
the current leading deep learning systems, deep convolutional neural networks
and random decision forests, providing insights into their successes and
shortcomings, as well as a principled route to their improvement.","Ankit B. Patel, Tan Nguyen, Richard G. Baraniuk",2015,http://arxiv.org/abs/1504.00641v1
"Comparative Deep Learning of Hybrid Representations for Image
  Recommendations","In many image-related tasks, learning expressive and discriminative
representations of images is essential, and deep learning has been studied for
automating the learning of such representations. Some user-centric tasks, such
as image recommendations, call for effective representations of not only images
but also preferences and intents of users over images. Such representations are
termed \emph{hybrid} and addressed via a deep learning approach in this paper.
We design a dual-net deep network, in which the two sub-networks map input
images and preferences of users into a same latent semantic space, and then the
distances between images and users in the latent space are calculated to make
decisions. We further propose a comparative deep learning (CDL) method to train
the deep network, using a pair of images compared against one user to learn the
pattern of their relative distances. The CDL embraces much more training data
than naive deep learning, and thus achieves superior performance than the
latter, with no cost of increasing network complexity. Experimental results
with real-world data sets for image recommendations have shown the proposed
dual-net network and CDL greatly outperform other state-of-the-art image
recommendation solutions.","Chenyi Lei, Dong Liu, Weiping Li, Zheng-Jun Zha, Houqiang Li",2016,http://arxiv.org/abs/1604.01252v1
"A Survey of Techniques All Classifiers Can Learn from Deep Networks:
  Models, Optimizations, and Regularization","Deep neural networks have introduced novel and useful tools to the machine
learning community. Other types of classifiers can potentially make use of
these tools as well to improve their performance and generality. This paper
reviews the current state of the art for deep learning classifier technologies
that are being used outside of deep neural networks. Non-network classifiers
can employ many components found in deep neural network architectures. In this
paper, we review the feature learning, optimization, and regularization methods
that form a core of deep network technologies. We then survey non-neural
network learning algorithms that make innovative use of these methods to
improve classification. Because many opportunities and challenges still exist,
we discuss directions that can be pursued to expand the area of deep learning
for a variety of classification algorithms.","Alireza Ghods, Diane J Cook",2019,http://arxiv.org/abs/1909.04791v2
"Hydrocephalus verification on brain magnetic resonance images with deep
  convolutional neural networks and ""transfer learning"" technique","The hydrocephalus can be either an independent disease or a concomitant
symptom of a number of pathologies, therefore representing an urgent issue in
the present-day clinical practice. Deep Learning is an evolving technology and
the part of a broader field of Machine Learning. Deep learning is currently
actively researched in the field of radiology. The aim of this study was to
evaluate deep learning applicability to the diagnostics of hydrocephalus with
the use of MRI images. We retrospectively collected, annotated, and
preprocessed the brain MRI data of 200 patients with and without radiological
signs of hydrocephalus. We applied a state-of-the-art deep convolutional neural
network in conjunction with transfer learning method to train a hydrocephalus
classifier model. Using deep convolutional neural networks, we achieved a high
quality of machine learning model. Accuracy, sensitivity, and specificity of
hydrocephalus signs identification was 97%, 98%, and 96% respectively. In this
study, we demonstrated the capacity of deep neural networks to identify
hydrocephalus syndrome using brain MRI images. Applying transfer learning
technique, the high quality of classification was achieved although trained on
rather limited data.","Alexey Demyanchuk, Ekaterina Pushkina, Nikolay Russkikh, Dmitry Shtokalo, Sergey Mishinov",2019,http://arxiv.org/abs/1909.10473v1
"Learning Low-dimensional Manifolds for Scoring of Tissue Microarray
  Images","Tissue microarray (TMA) images have emerged as an important high-throughput
tool for cancer study and the validation of biomarkers. Efforts have been
dedicated to further improve the accuracy of TACOMA, a cutting-edge automatic
scoring algorithm for TMA images. One major advance is due to deepTacoma, an
algorithm that incorporates suitable deep representations of a group nature.
Inspired by the recent advance in semi-supervised learning and deep learning,
we propose mfTacoma to learn alternative deep representations in the context of
TMA image scoring. In particular, mfTacoma learns the low-dimensional
manifolds, a common latent structure in high dimensional data. Deep
representation learning and manifold learning typically requires large data. By
encoding deep representation of the manifolds as regularizing features,
mfTacoma effectively leverages the manifold information that is potentially
crude due to small data. Our experiments show that deep features by manifolds
outperforms two alternatives -- deep features by linear manifolds with
principal component analysis or by leveraging the group property.","Donghui Yan, Jian Zou, Zhenpeng Li",2021,http://arxiv.org/abs/2102.11396v1
A Survey of Forex and Stock Price Prediction Using Deep Learning,"The prediction of stock and foreign exchange (Forex) had always been a hot
and profitable area of study. Deep learning application had proven to yields
better accuracy and return in the field of financial prediction and
forecasting. In this survey we selected papers from the DBLP database for
comparison and analysis. We classified papers according to different deep
learning methods, which included: Convolutional neural network (CNN), Long
Short-Term Memory (LSTM), Deep neural network (DNN), Recurrent Neural Network
(RNN), Reinforcement Learning, and other deep learning methods such as HAN,
NLP, and Wavenet. Furthermore, this paper reviewed the dataset, variable,
model, and results of each article. The survey presented the results through
the most used performance metrics: RMSE, MAPE, MAE, MSE, accuracy, Sharpe
ratio, and return rate. We identified that recent models that combined LSTM
with other methods, for example, DNN, are widely researched. Reinforcement
learning and other deep learning method yielded great returns and performances.
We conclude that in recent years the trend of using deep-learning based method
for financial modeling is exponentially rising.","Zexin Hu, Yiqi Zhao, Matloob Khushi",2021,http://arxiv.org/abs/2103.09750v1
Audio representations for deep learning in sound synthesis: A review,"The rise of deep learning algorithms has led many researchers to withdraw
from using classic signal processing methods for sound generation. Deep
learning models have achieved expressive voice synthesis, realistic sound
textures, and musical notes from virtual instruments. However, the most
suitable deep learning architecture is still under investigation. The choice of
architecture is tightly coupled to the audio representations. A sound's
original waveform can be too dense and rich for deep learning models to deal
with efficiently - and complexity increases training time and computational
cost. Also, it does not represent sound in the manner in which it is perceived.
Therefore, in many cases, the raw audio has been transformed into a compressed
and more meaningful form using upsampling, feature-extraction, or even by
adopting a higher level illustration of the waveform. Furthermore, conditional
on the form chosen, additional conditioning representations, different model
architectures, and numerous metrics for evaluating the reconstructed sound have
been investigated. This paper provides an overview of audio representations
applied to sound synthesis using deep learning. Additionally, it presents the
most significant methods for developing and evaluating a sound synthesis
architecture using deep learning models, always depending on the audio
representation.","Anastasia Natsiou, Sean O'Leary",2022,http://arxiv.org/abs/2201.02490v1
"Simultaneous Detection of Multiple Appliances from Smart-meter
  Measurements via Multi-Label Consistent Deep Dictionary Learning and Deep
  Transform Learning","Currently there are several well-known approaches to non-intrusive appliance
load monitoring rule based, stochastic finite state machines, neural networks
and sparse coding. Recently several studies have proposed a new approach based
on multi label classification. Different appliances are treated as separate
classes, and the task is to identify the classes given the aggregate
smart-meter reading. Prior studies in this area have used off the shelf
algorithms like MLKNN and RAKEL to address this problem. In this work, we
propose a deep learning based technique. There are hardly any studies in deep
learning based multi label classification; two new deep learning techniques to
solve the said problem are fundamental contributions of this work. These are
deep dictionary learning and deep transform learning. Thorough experimental
results on benchmark datasets show marked improvement over existing studies.","Vanika Singhal, Jyoti Maggu, Angshul Majumdar",2019,http://arxiv.org/abs/1912.07568v1
"The Unreasonable Effectiveness of Deep Learning in Artificial
  Intelligence","Deep learning networks have been trained to recognize speech, caption
photographs and translate text between languages at high levels of performance.
Although applications of deep learning networks to real world problems have
become ubiquitous, our understanding of why they are so effective is lacking.
These empirical results should not be possible according to sample complexity
in statistics and non-convex optimization theory. However, paradoxes in the
training and effectiveness of deep learning networks are being investigated and
insights are being found in the geometry of high-dimensional spaces. A
mathematical theory of deep learning would illuminate how they function, allow
us to assess the strengths and weaknesses of different network architectures
and lead to major improvements. Deep learning has provided natural ways for
humans to communicate with digital devices and is foundational for building
artificial general intelligence. Deep learning was inspired by the architecture
of the cerebral cortex and insights into autonomy and general intelligence may
be found in other brain regions that are essential for planning and survival,
but major breakthroughs will be needed to achieve these goals.",Terrence J. Sejnowski,2020,http://arxiv.org/abs/2002.04806v1
Trends in deep learning for medical hyperspectral image analysis,"Deep learning algorithms have seen acute growth of interest in their
applications throughout several fields of interest in the last decade, with
medical hyperspectral imaging being a particularly promising domain. So far, to
the best of our knowledge, there is no review paper that discusses the
implementation of deep learning for medical hyperspectral imaging, which is
what this review paper aims to accomplish by examining publications that
currently utilize deep learning to perform effective analysis of medical
hyperspectral imagery. This paper discusses deep learning concepts that are
relevant and applicable to medical hyperspectral imaging analysis, several of
which have been implemented since the boom in deep learning. This will comprise
of reviewing the use of deep learning for classification, segmentation, and
detection in order to investigate the analysis of medical hyperspectral
imaging. Lastly, we discuss the current and future challenges pertaining to
this discipline and the possible efforts to overcome such trials.","Uzair Khan, Paheding Sidike, Colin Elkin, Vijay Devabhaktuni",2020,http://arxiv.org/abs/2011.13974v1
"Sparse Deep Learning: A New Framework Immune to Local Traps and
  Miscalibration","Deep learning has powered recent successes of artificial intelligence (AI).
However, the deep neural network, as the basic model of deep learning, has
suffered from issues such as local traps and miscalibration. In this paper, we
provide a new framework for sparse deep learning, which has the above issues
addressed in a coherent way. In particular, we lay down a theoretical
foundation for sparse deep learning and propose prior annealing algorithms for
learning sparse neural networks. The former has successfully tamed the sparse
deep neural network into the framework of statistical modeling, enabling
prediction uncertainty correctly quantified. The latter can be asymptotically
guaranteed to converge to the global optimum, enabling the validity of the
down-stream statistical inference. Numerical result indicates the superiority
of the proposed method compared to the existing ones.","Yan Sun, Wenjun Xiong, Faming Liang",2021,http://arxiv.org/abs/2110.00653v2
"Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and
  Implementation-Level Techniques","Deep learning is pervasive in our daily life, including self-driving cars,
virtual assistants, social network services, healthcare services, face
recognition, etc. However, deep neural networks demand substantial compute
resources during training and inference. The machine learning community has
mainly focused on model-level optimizations such as architectural compression
of deep learning models, while the system community has focused on
implementation-level optimization. In between, various arithmetic-level
optimization techniques have been proposed in the arithmetic community. This
article provides a survey on resource-efficient deep learning techniques in
terms of model-, arithmetic-, and implementation-level techniques and
identifies the research gaps for resource-efficient deep learning techniques
across the three different level techniques. Our survey clarifies the influence
from higher to lower-level techniques based on our resource-efficiency metric
definition and discusses the future trend for resource-efficient deep learning
research.","JunKyu Lee, Lev Mukhanov, Amir Sabbagh Molahosseini, Umar Minhas, Yang Hua, Jesus Martinez del Rincon, Kiril Dichev, Cheol-Ho Hong, Hans Vandierendonck",2021,http://arxiv.org/abs/2112.15131v1
Towards understanding deep learning with the natural clustering prior,"The prior knowledge (a.k.a. priors) integrated into the design of a machine
learning system strongly influences its generalization abilities. In the
specific context of deep learning, some of these priors are poorly understood
as they implicitly emerge from the successful heuristics and tentative
approximations of biological brains involved in deep learning design. Through
the lens of supervised image classification problems, this thesis investigates
the implicit integration of a natural clustering prior composed of three
statements: (i) natural images exhibit a rich clustered structure, (ii) image
classes are composed of multiple clusters and (iii) each cluster contains
examples from a single class. The decomposition of classes into multiple
clusters implies that supervised deep learning systems could benefit from
unsupervised clustering to define appropriate decision boundaries. Hence, this
thesis attempts to identify implicit clustering abilities, mechanisms and
hyperparameters in deep learning systems and evaluate their relevance for
explaining the generalization abilities of these systems. We do so through an
extensive empirical study of the training dynamics as well as the neuron- and
layer-level representations of deep neural networks. The resulting collection
of experiments provides preliminary evidence for the relevance of the natural
clustering prior for understanding deep learning.",Simon Carbonnelle,2022,http://arxiv.org/abs/2203.08174v1
Fault-Tolerant Deep Learning: A Hierarchical Perspective,"With the rapid advancements of deep learning in the past decade, it can be
foreseen that deep learning will be continuously deployed in more and more
safety-critical applications such as autonomous driving and robotics. In this
context, reliability turns out to be critical to the deployment of deep
learning in these applications and gradually becomes a first-class citizen
among the major design metrics like performance and energy efficiency.
Nevertheless, the back-box deep learning models combined with the diverse
underlying hardware faults make resilient deep learning extremely challenging.
In this special session, we conduct a comprehensive survey of fault-tolerant
deep learning design approaches with a hierarchical perspective and investigate
these approaches from model layer, architecture layer, circuit layer, and cross
layer respectively.","Cheng Liu, Zhen Gao, Siting Liu, Xuefei Ning, Huawei Li, Xiaowei Li",2022,http://arxiv.org/abs/2204.01942v1
"Achieve Optimal Adversarial Accuracy for Adversarial Deep Learning using
  Stackelberg Game","Adversarial deep learning is to train robust DNNs against adversarial
attacks, which is one of the major research focuses of deep learning. Game
theory has been used to answer some of the basic questions about adversarial
deep learning such as the existence of a classifier with optimal robustness and
the existence of optimal adversarial samples for a given class of classifiers.
In most previous work, adversarial deep learning was formulated as a
simultaneous game and the strategy spaces are assumed to be certain probability
distributions in order for the Nash equilibrium to exist. But, this assumption
is not applicable to the practical situation. In this paper, we give answers to
these basic questions for the practical case where the classifiers are DNNs
with a given structure, by formulating the adversarial deep learning as
sequential games. The existence of Stackelberg equilibria for these games are
proved. Furthermore, it is shown that the equilibrium DNN has the largest
adversarial accuracy among all DNNs with the same structure, when
Carlini-Wagner's margin loss is used. Trade-off between robustness and accuracy
in adversarial deep learning is also studied from game theoretical aspect.","Xiao-Shan Gao, Shuang Liu, Lijia Yu",2022,http://arxiv.org/abs/2207.08137v1
"Interpretations Cannot Be Trusted: Stealthy and Effective Adversarial
  Perturbations against Interpretable Deep Learning","Deep learning methods have gained increased attention in various applications
due to their outstanding performance. For exploring how this high performance
relates to the proper use of data artifacts and the accurate problem
formulation of a given task, interpretation models have become a crucial
component in developing deep learning-based systems. Interpretation models
enable the understanding of the inner workings of deep learning models and
offer a sense of security in detecting the misuse of artifacts in the input
data. Similar to prediction models, interpretation models are also susceptible
to adversarial inputs. This work introduces two attacks, AdvEdge and
AdvEdge$^{+}$, that deceive both the target deep learning model and the coupled
interpretation model. We assess the effectiveness of proposed attacks against
two deep learning model architectures coupled with four interpretation models
that represent different categories of interpretation models. Our experiments
include the attack implementation using various attack frameworks. We also
explore the potential countermeasures against such attacks. Our analysis shows
the effectiveness of our attacks in terms of deceiving the deep learning models
and their interpreters, and highlights insights to improve and circumvent the
attacks.","Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Eric Chan-Tin, Tamer Abuhmed",2022,http://arxiv.org/abs/2211.15926v1
Deep Learning in Deterministic Computational Mechanics,"The rapid growth of deep learning research, including within the field of
computational mechanics, has resulted in an extensive and diverse body of
literature. To help researchers identify key concepts and promising
methodologies within this field, we provide an overview of deep learning in
deterministic computational mechanics. Five main categories are identified and
explored: simulation substitution, simulation enhancement, discretizations as
neural networks, generative approaches, and deep reinforcement learning. This
review focuses on deep learning methods rather than applications for
computational mechanics, thereby enabling researchers to explore this field
more effectively. As such, the review is not necessarily aimed at researchers
with extensive knowledge of deep learning -- instead, the primary audience is
researchers at the verge of entering this field or those who attempt to gain an
overview of deep learning in computational mechanics. The discussed concepts
are, therefore, explained as simple as possible.","Leon Herrmann, Stefan Kollmannsberger",2023,http://arxiv.org/abs/2309.15421v1
"Deep learning applied to EEG data with different montages using spatial
  attention","The ability of Deep Learning to process and extract relevant information in
complex brain dynamics from raw EEG data has been demonstrated in various
recent works. Deep learning models, however, have also been shown to perform
best on large corpora of data. When processing EEG, a natural approach is to
combine EEG datasets from different experiments to train large deep-learning
models. However, most EEG experiments use custom channel montages, requiring
the data to be transformed into a common space. Previous methods have used the
raw EEG signal to extract features of interest and focused on using a common
feature space across EEG datasets. While this is a sensible approach, it
underexploits the potential richness of EEG raw data. Here, we explore using
spatial attention applied to EEG electrode coordinates to perform channel
harmonization of raw EEG data, allowing us to train deep learning on EEG data
using different montages. We test this model on a gender classification task.
We first show that spatial attention increases model performance. Then, we show
that a deep learning model trained on data using different channel montages
performs significantly better than deep learning models trained on fixed 23-
and 128-channel data montages.","Dung Truong, Muhammad Abdullah Khalid, Arnaud Delorme",2023,http://arxiv.org/abs/2310.10550v1
"Deep-Graph-Sprints: Accelerated Representation Learning in
  Continuous-Time Dynamic Graphs","Continuous-time dynamic graphs (CTDGs) are essential for modeling
interconnected, evolving systems. Traditional methods for extracting knowledge
from these graphs often depend on feature engineering or deep learning. Feature
engineering is limited by the manual and time-intensive nature of crafting
features, while deep learning approaches suffer from high inference latency,
making them impractical for real-time applications. This paper introduces
Deep-Graph-Sprints (DGS), a novel deep learning architecture designed for
efficient representation learning on CTDGs with low-latency inference
requirements. We benchmark DGS against state-of-the-art (SOTA) feature
engineering and graph neural network methods using five diverse datasets. The
results indicate that DGS achieves competitive performance while inference
speed improves between 4x and 12x compared to other deep learning approaches on
our benchmark datasets. Our method effectively bridges the gap between deep
representation learning and low-latency application requirements for CTDGs.","Ahmad Naser Eddin, Jacopo Bono, David Aparício, Hugo Ferreira, Pedro Ribeiro, Pedro Bizarro",2024,http://arxiv.org/abs/2407.07712v3
On Learnable Parameters of Optimal and Suboptimal Deep Learning Models,"We scrutinize the structural and operational aspects of deep learning models,
particularly focusing on the nuances of learnable parameters (weight)
statistics, distribution, node interaction, and visualization. By establishing
correlations between variance in weight patterns and overall network
performance, we investigate the varying (optimal and suboptimal) performances
of various deep-learning models. Our empirical analysis extends across widely
recognized datasets such as MNIST, Fashion-MNIST, and CIFAR-10, and various
deep learning models such as deep neural networks (DNNs), convolutional neural
networks (CNNs), and vision transformer (ViT), enabling us to pinpoint
characteristics of learnable parameters that correlate with successful
networks. Through extensive experiments on the diverse architectures of deep
learning models, we shed light on the critical factors that influence the
functionality and efficiency of DNNs. Our findings reveal that successful
networks, irrespective of datasets or models, are invariably similar to other
successful networks in their converged weights statistics and distribution,
while poor-performing networks vary in their weights. In addition, our research
shows that the learnable parameters of widely varied deep learning models such
as DNN, CNN, and ViT exhibit similar learning characteristics.","Ziwei Zheng, Huizhi Liang, Vaclav Snasel, Vito Latora, Panos Pardalos, Giuseppe Nicosia, Varun Ojha",2024,http://arxiv.org/abs/2408.11720v1
An Overview of Deep Learning Architectures in Few-Shot Learning Domain,"Since 2012, Deep learning has revolutionized Artificial Intelligence and has
achieved state-of-the-art outcomes in different domains, ranging from Image
Classification to Speech Generation. Though it has many potentials, our current
architectures come with the pre-requisite of large amounts of data. Few-Shot
Learning (also known as one-shot learning) is a sub-field of machine learning
that aims to create such models that can learn the desired objective with less
data, similar to how humans learn. In this paper, we have reviewed some of the
well-known deep learning-based approaches towards few-shot learning. We have
discussed the recent achievements, challenges, and possibilities of improvement
of few-shot learning based deep learning architectures. Our aim for this paper
is threefold: (i) Give a brief introduction to deep learning architectures for
few-shot learning with pointers to core references. (ii) Indicate how deep
learning has been applied to the low-data regime, from data preparation to
model training. and, (iii) Provide a starting point for people interested in
experimenting and perhaps contributing to the field of few-shot learning by
pointing out some useful resources and open-source code. Our code is available
at Github: https://github.com/shruti-jadon/Hands-on-One-Shot-Learning.","Shruti Jadon, Aryan Jadon",2020,http://arxiv.org/abs/2008.06365v4
Deep Asymmetric Multi-task Feature Learning,"We propose Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) which can
learn deep representations shared across multiple tasks while effectively
preventing negative transfer that may happen in the feature sharing process.
Specifically, we introduce an asymmetric autoencoder term that allows reliable
predictors for the easy tasks to have high contribution to the feature learning
while suppressing the influences of unreliable predictors for more difficult
tasks. This allows the learning of less noisy representations, and enables
unreliable predictors to exploit knowledge from the reliable predictors via the
shared latent features. Such asymmetric knowledge transfer through shared
features is also more scalable and efficient than inter-task asymmetric
transfer. We validate our Deep-AMTFL model on multiple benchmark datasets for
multitask learning and image classification, on which it significantly
outperforms existing symmetric and asymmetric multitask learning models, by
effectively preventing negative transfer in deep feature learning.","Hae Beom Lee, Eunho Yang, Sung Ju Hwang",2017,http://arxiv.org/abs/1708.00260v3
"Clones in Deep Learning Code: What, Where, and Why?","Deep Learning applications are becoming increasingly popular. Developers of
deep learning systems strive to write more efficient code. Deep learning
systems are constantly evolving, imposing tighter development timelines and
increasing complexity, which may lead to bad design decisions. A copy-paste
approach is widely used among deep learning developers because they rely on
common frameworks and duplicate similar tasks. Developers often fail to
properly propagate changes to all clones fragments during a maintenance
activity. To our knowledge, no study has examined code cloning practices in
deep learning development. Given the negative impacts of clones on software
quality reported in the studies on traditional systems, it is very important to
understand the characteristics and potential impacts of code clones on deep
learning systems. To this end, we use the NiCad tool to detect clones from 59
Python, 14 C# and 6 Java-based deep learning systems and an equal number of
traditional software systems. We then analyze the frequency and distribution of
code clones in deep learning and traditional systems. We do further analysis of
the distribution of code clones using location-based taxonomy. We also study
the correlation between bugs and code clones to assess the impacts of clones on
the quality of the studied systems. Finally, we introduce a code clone taxonomy
related to deep learning programs and identify the deep learning system
development phases in which cloning has the highest risk of faults. Our results
show that code cloning is a frequent practice in deep learning systems and that
deep learning developers often clone code from files in distant repositories in
the system. In addition, we found that code cloning occurs more frequently
during DL model construction. And that hyperparameters setting is the phase
during which cloning is the riskiest, since it often leads to faults.","Hadhemi Jebnoun, Md Saidur Rahman, Foutse Khomh, Biruk Asmare Muse",2021,http://arxiv.org/abs/2107.13614v1
Implicit Regularization in Deep Learning,"In an attempt to better understand generalization in deep learning, we study
several possible explanations. We show that implicit regularization induced by
the optimization method is playing a key role in generalization and success of
deep learning models. Motivated by this view, we study how different complexity
measures can ensure generalization and explain how optimization algorithms can
implicitly regularize complexity measures. We empirically investigate the
ability of these measures to explain different observed phenomena in deep
learning. We further study the invariances in neural networks, suggest
complexity measures and optimization algorithms that have similar invariances
to those in neural networks and evaluate them on a number of learning tasks.",Behnam Neyshabur,2017,http://arxiv.org/abs/1709.01953v2
Deep Inverse Optimization,"Given a set of observations generated by an optimization process, the goal of
inverse optimization is to determine likely parameters of that process. We cast
inverse optimization as a form of deep learning. Our method, called deep
inverse optimization, is to unroll an iterative optimization process and then
use backpropagation to learn parameters that generate the observations. We
demonstrate that by backpropagating through the interior point algorithm we can
learn the coefficients determining the cost vector and the constraints,
independently or jointly, for both non-parametric and parametric linear
programs, starting from one or multiple observations. With this approach,
inverse optimization can leverage concepts and algorithms from deep learning.","Yingcong Tan, Andrew Delong, Daria Terekhov",2018,http://arxiv.org/abs/1812.00804v1
A short review on Applications of Deep learning for Cyber security,"Deep learning is an advanced model of traditional machine learning. This has
the capability to extract optimal feature representation from raw input
samples. This has been applied towards various use cases in cyber security such
as intrusion detection, malware classification, android malware detection, spam
and phishing detection and binary analysis. This paper outlines the survey of
all the works related to deep learning based solutions for various cyber
security use cases. Keywords: Deep learning, intrusion detection, malware
detection, Android malware detection, spam & phishing detection, traffic
analysis, binary analysis.","Mohammed Harun Babu R, Vinayakumar R, Soman KP",2018,http://arxiv.org/abs/1812.06292v2
"Distributed Layer-Partitioned Training for Privacy-Preserved Deep
  Learning","Deep Learning techniques have achieved remarkable results in many domains.
Often, training deep learning models requires large datasets, which may require
sensitive information to be uploaded to the cloud to accelerate training. To
adequately protect sensitive information, we propose distributed
layer-partitioned training with step-wise activation functions for
privacy-preserving deep learning. Experimental results attest our method to be
simple and effective.","Chun-Hsien Yu, Chun-Nan Chou, Emily Chang",2019,http://arxiv.org/abs/1904.06049v1
Time Series Forecasting With Deep Learning: A Survey,"Numerous deep learning architectures have been developed to accommodate the
diversity of time series datasets across different domains. In this article, we
survey common encoder and decoder designs used in both one-step-ahead and
multi-horizon time series forecasting -- describing how temporal information is
incorporated into predictions by each model. Next, we highlight recent
developments in hybrid deep learning models, which combine well-studied
statistical models with neural network components to improve pure methods in
either category. Lastly, we outline some ways in which deep learning can also
facilitate decision support with time series data.","Bryan Lim, Stefan Zohren",2020,http://arxiv.org/abs/2004.13408v2
DeepEMO: Deep Learning for Speech Emotion Recognition,"We proposed the industry level deep learning approach for speech emotion
recognition task. In industry, carefully proposed deep transfer learning
technology shows real results due to mostly low amount of training data
availability, machine training cost, and specialized learning on dedicated AI
tasks. The proposed speech recognition framework, called DeepEMO, consists of
two main pipelines such that preprocessing to extract efficient main features
and deep transfer learning model to train and recognize. Main source code is in
https://github.com/enkhtogtokh/deepemo repository","Enkhtogtokh Togootogtokh, Christian Klasen",2021,http://arxiv.org/abs/2109.04081v1
ChainerMN: Scalable Distributed Deep Learning Framework,"One of the keys for deep learning to have made a breakthrough in various
fields was to utilize high computing powers centering around GPUs. Enabling the
use of further computing abilities by distributed processing is essential not
only to make the deep learning bigger and faster but also to tackle unsolved
challenges. We present the design, implementation, and evaluation of ChainerMN,
the distributed deep learning framework we have developed. We demonstrate that
ChainerMN can scale the learning process of the ResNet-50 model to the ImageNet
dataset up to 128 GPUs with the parallel efficiency of 90%.","Takuya Akiba, Keisuke Fukuda, Shuji Suzuki",2017,http://arxiv.org/abs/1710.11351v1
ImJoy: an open-source computational platform for the deep learning era,"Deep learning methods have shown extraordinary potential for analyzing very
diverse biomedical data, but their dissemination beyond developers is hindered
by important computational hurdles. We introduce ImJoy (https://imjoy.io/), a
flexible and open-source browser-based platform designed to facilitate
widespread reuse of deep learning solutions in biomedical research. We
highlight ImJoy's main features and illustrate its functionalities with deep
learning plugins for mobile and interactive image analysis and genomics.","Wei Ouyang, Florian Mueller, Martin Hjelmare, Emma Lundberg, Christophe Zimmer",2019,http://arxiv.org/abs/1905.13105v1
Holistic Adversarial Robustness of Deep Learning Models,"Adversarial robustness studies the worst-case performance of a machine
learning model to ensure safety and reliability. With the proliferation of
deep-learning-based technology, the potential risks associated with model
development and deployment can be amplified and become dreadful
vulnerabilities. This paper provides a comprehensive overview of research
topics and foundational principles of research methods for adversarial
robustness of deep learning models, including attacks, defenses, verification,
and novel applications.","Pin-Yu Chen, Sijia Liu",2022,http://arxiv.org/abs/2202.07201v3
Structure-based drug design with geometric deep learning,"Structure-based drug design uses three-dimensional geometric information of
macromolecules, such as proteins or nucleic acids, to identify suitable
ligands. Geometric deep learning, an emerging concept of neural-network-based
machine learning, has been applied to macromolecular structures. This review
provides an overview of the recent applications of geometric deep learning in
bioorganic and medicinal chemistry, highlighting its potential for
structure-based drug discovery and design. Emphasis is placed on molecular
property prediction, ligand binding site and pose prediction, and
structure-based de novo molecular design. The current challenges and
opportunities are highlighted, and a forecast of the future of geometric deep
learning for drug discovery is presented.","Clemens Isert, Kenneth Atz, Gisbert Schneider",2022,http://arxiv.org/abs/2210.11250v1
"Opportunities in deep learning methods development for computational
  biology","Advances in molecular technologies underlie an enormous growth in the size of
data sets pertaining to biology and biomedicine. These advances parallel those
in the deep learning subfield of machine learning. Components in the
differentiable programming toolbox that makes deep learning possible are
allowing computer scientists to address an increasingly large array of problems
with flexible and effective tools. However many of these tools have not fully
proliferated into the computational biology and bioinformatics fields. In this
perspective we survey some of these advances and highlight exemplary examples
of their utilization in the biosciences, with the goal of increasing awareness
among practitioners of emerging opportunities to blend expert knowledge with
newly emerging deep learning architectural tools.","Alex Jihun Lee, Reza Abbasi-Asl",2024,http://arxiv.org/abs/2406.08686v1
Deep Transfer Learning for Person Re-identification,"Person re-identification (Re-ID) poses a unique challenge to deep learning:
how to learn a deep model with millions of parameters on a small training set
of few or no labels. In this paper, a number of deep transfer learning models
are proposed to address the data sparsity problem. First, a deep network
architecture is designed which differs from existing deep Re-ID models in that
(a) it is more suitable for transferring representations learned from large
image classification datasets, and (b) classification loss and verification
loss are combined, each of which adopts a different dropout strategy. Second, a
two-stepped fine-tuning strategy is developed to transfer knowledge from
auxiliary datasets. Third, given an unlabelled Re-ID dataset, a novel
unsupervised deep transfer learning model is developed based on co-training.
The proposed models outperform the state-of-the-art deep Re-ID models by large
margins: we achieve Rank-1 accuracy of 85.4\%, 83.7\% and 56.3\% on CUHK03,
Market1501, and VIPeR respectively, whilst on VIPeR, our unsupervised model
(45.1\%) beats most supervised models.","Mengyue Geng, Yaowei Wang, Tao Xiang, Yonghong Tian",2016,http://arxiv.org/abs/1611.05244v2
Forward Thinking: Building Deep Random Forests,"The success of deep neural networks has inspired many to wonder whether other
learners could benefit from deep, layered architectures. We present a general
framework called forward thinking for deep learning that generalizes the
architectural flexibility and sophistication of deep neural networks while also
allowing for (i) different types of learning functions in the network, other
than neurons, and (ii) the ability to adaptively deepen the network as needed
to improve results. This is done by training one layer at a time, and once a
layer is trained, the input data are mapped forward through the layer to create
a new learning problem. The process is then repeated, transforming the data
through multiple layers, one at a time, rendering a new dataset, which is
expected to be better behaved, and on which a final output layer can achieve
good performance. In the case where the neurons of deep neural nets are
replaced with decision trees, we call the result a Forward Thinking Deep Random
Forest (FTDRF). We demonstrate a proof of concept by applying FTDRF on the
MNIST dataset. We also provide a general mathematical formulation that allows
for other types of deep learning problems to be considered.","Kevin Miller, Chris Hettinger, Jeffrey Humpherys, Tyler Jarvis, David Kartchner",2017,http://arxiv.org/abs/1705.07366v1
Universality of Deep Convolutional Neural Networks,"Deep learning has been widely applied and brought breakthroughs in speech
recognition, computer vision, and many other domains. The involved deep neural
network architectures and computational issues have been well studied in
machine learning. But there lacks a theoretical foundation for understanding
the approximation or generalization ability of deep learning methods generated
by the network architectures such as deep convolutional neural networks having
convolutional structures. Here we show that a deep convolutional neural network
(CNN) is universal, meaning that it can be used to approximate any continuous
function to an arbitrary accuracy when the depth of the neural network is large
enough. This answers an open question in learning theory. Our quantitative
estimate, given tightly in terms of the number of free parameters to be
computed, verifies the efficiency of deep CNNs in dealing with large
dimensional data. Our study also demonstrates the role of convolutions in deep
CNNs.",Ding-Xuan Zhou,2018,http://arxiv.org/abs/1805.10769v2
"Limitations of Deep Neural Networks: a discussion of G. Marcus' critical
  appraisal of deep learning","Deep neural networks have triggered a revolution in artificial intelligence,
having been applied with great results in medical imaging, semi-autonomous
vehicles, ecommerce, genetics research, speech recognition, particle physics,
experimental art, economic forecasting, environmental science, industrial
manufacturing, and a wide variety of applications in nearly every field. This
sudden success, though, may have intoxicated the research community and blinded
them to the potential pitfalls of assigning deep learning a higher status than
warranted. Also, research directed at alleviating the weaknesses of deep
learning may seem less attractive to scientists and engineers, who focus on the
low-hanging fruit of finding more and more applications for deep learning
models, thus letting short-term benefits hamper long-term scientific progress.
Gary Marcus wrote a paper entitled Deep Learning: A Critical Appraisal, and
here we discuss Marcus' core ideas, as well as attempt a general assessment of
the subject. This study examines some of the limitations of deep neural
networks, with the intention of pointing towards potential paths for future
research, and of clearing up some metaphysical misconceptions, held by numerous
researchers, that may misdirect them.",Stefanos Tsimenidis,2020,http://arxiv.org/abs/2012.15754v1
Deep Learning Algorithms Used in Intrusion Detection Systems -- A Review,"The increase in network attacks has necessitated the development of robust
and efficient intrusion detection systems (IDS) capable of identifying
malicious activities in real-time. In the last five years, deep learning
algorithms have emerged as powerful tools in this domain, offering enhanced
detection capabilities compared to traditional methods. This review paper
studies recent advancements in the application of deep learning techniques,
including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN),
Deep Belief Networks (DBN), Deep Neural Networks (DNN), Long Short-Term Memory
(LSTM), autoencoders (AE), Multi-Layer Perceptrons (MLP), Self-Normalizing
Networks (SNN) and hybrid models, within network intrusion detection systems.
we delve into the unique architectures, training models, and classification
methodologies tailored for network traffic analysis and anomaly detection.
Furthermore, we analyze the strengths and limitations of each deep learning
approach in terms of detection accuracy, computational efficiency, scalability,
and adaptability to evolving threats. Additionally, this paper highlights
prominent datasets and benchmarking frameworks commonly utilized for evaluating
the performance of deep learning-based IDS. This review will provide
researchers and industry practitioners with valuable insights into the
state-of-the-art deep learning algorithms for enhancing the security framework
of network environments through intrusion detection.","Richard Kimanzi, Peter Kimanga, Dedan Cherori, Patrick K. Gikunda",2024,http://arxiv.org/abs/2402.17020v1
"Classifiers Based on Deep Sparse Coding Architectures are Robust to Deep
  Learning Transferable Examples","Although deep learning has shown great success in recent years, researchers
have discovered a critical flaw where small, imperceptible changes in the input
to the system can drastically change the output classification. These attacks
are exploitable in nearly all of the existing deep learning classification
frameworks. However, the susceptibility of deep sparse coding models to
adversarial examples has not been examined. Here, we show that classifiers
based on a deep sparse coding model whose classification accuracy is
competitive with a variety of deep neural network models are robust to
adversarial examples that effectively fool those same deep learning models. We
demonstrate both quantitatively and qualitatively that the robustness of deep
sparse coding models to adversarial examples arises from two key properties.
First, because deep sparse coding models learn general features corresponding
to generators of the dataset as a whole, rather than highly discriminative
features for distinguishing specific classes, the resulting classifiers are
less dependent on idiosyncratic features that might be more easily exploited.
Second, because deep sparse coding models utilize fixed point attractor
dynamics with top-down feedback, it is more difficult to find small changes to
the input that drive the resulting representations out of the correct attractor
basin.","Jacob M. Springer, Charles S. Strauss, Austin M. Thresher, Edward Kim, Garrett T. Kenyon",2018,http://arxiv.org/abs/1811.07211v2
Deep Reinforcement Learning using Cyclical Learning Rates,"Deep Reinforcement Learning (DRL) methods often rely on the meticulous tuning
of hyperparameters to successfully resolve problems. One of the most
influential parameters in optimization procedures based on stochastic gradient
descent (SGD) is the learning rate. We investigate cyclical learning and
propose a method for defining a general cyclical learning rate for various DRL
problems. In this paper we present a method for cyclical learning applied to
complex DRL problems. Our experiments show that, utilizing cyclical learning
achieves similar or even better results than highly tuned fixed learning rates.
This paper presents the first application of cyclical learning rates in DRL
settings and is a step towards overcoming manual hyperparameter tuning.","Ralf Gulde, Marc Tuscher, Akos Csiszar, Oliver Riedel, Alexander Verl",2020,http://arxiv.org/abs/2008.01171v1
"Deep Learning: Generalization Requires Deep Compositional Feature Space
  Design","Generalization error defines the discriminability and the representation
power of a deep model. In this work, we claim that feature space design using
deep compositional function plays a significant role in generalization along
with explicit and implicit regularizations. Our claims are being established
with several image classification experiments. We show that the information
loss due to convolution and max pooling can be marginalized with the
compositional design, improving generalization performance. Also, we will show
that learning rate decay acts as an implicit regularizer in deep model
training.",Mrinal Haloi,2017,http://arxiv.org/abs/1706.01983v2
Deep Learning in Robotics: A Review of Recent Research,"Advances in deep learning over the last decade have led to a flurry of
research in the application of deep artificial neural networks to robotic
systems, with at least thirty papers published on the subject between 2014 and
the present. This review discusses the applications, benefits, and limitations
of deep learning vis-\`a-vis physical robotic systems, using contemporary
research as exemplars. It is intended to communicate recent advances to the
wider robotics community and inspire additional interest in and application of
deep learning in robotics.","Harry A. Pierson, Michael S. Gashler",2017,http://arxiv.org/abs/1707.07217v1
Exploring Variational Deep Q Networks,"This study provides both analysis and a refined, research-ready
implementation of Tang and Kucukelbir's Variational Deep Q Network, a novel
approach to maximising the efficiency of exploration in complex learning
environments using Variational Bayesian Inference. Alongside reference
implementations of both Traditional and Double Deep Q Networks, a small novel
contribution is presented - the Double Variational Deep Q Network, which
incorporates improvements to increase the stability and robustness of
inference-based learning. Finally, an evaluation and discussion of the
effectiveness of these approaches is discussed in the wider context of Bayesian
Deep Learning.",A. H. Bell-Thomas,2020,http://arxiv.org/abs/2008.01641v1
A SOM-based Gradient-Free Deep Learning Method with Convergence Analysis,"As gradient descent method in deep learning causes a series of questions,
this paper proposes a novel gradient-free deep learning structure. By adding a
new module into traditional Self-Organizing Map and introducing residual into
the map, a Deep Valued Self-Organizing Map network is constructed. And analysis
about the convergence performance of such a deep Valued Self-Organizing Map
network is proved in this paper, which gives an inequality about the designed
parameters with the dimension of inputs and the loss of prediction.","Shaosheng Xu, Jinde Cao, Yichao Cao, Tong Wang",2021,http://arxiv.org/abs/2101.05612v2
"Practical applicability of deep neural networks for overlapping speaker
  separation","This paper examines the applicability in realistic scenarios of two deep
learning based solutions to the overlapping speaker separation problem.
Firstly, we present experiments that show that these methods are applicable for
a broad range of languages. Further experimentation indicates limited
performance loss for untrained languages, when these have common features with
the trained language(s). Secondly, it investigates how the methods deal with
realistic background noise and proposes some modifications to better cope with
these disturbances. The deep learning methods that will be examined are deep
clustering and deep attractor networks.","Pieter Appeltans, Jeroen Zegers, Hugo Van hamme",2019,http://arxiv.org/abs/1912.09261v1
Deep Probabilistic Programming Languages: A Qualitative Study,"Deep probabilistic programming languages try to combine the advantages of
deep learning with those of probabilistic programming languages. If successful,
this would be a big step forward in machine learning and programming languages.
Unfortunately, as of now, this new crop of languages is hard to use and
understand. This paper addresses this problem directly by explaining deep
probabilistic programming languages and indirectly by characterizing their
current strengths and weaknesses.","Guillaume Baudart, Martin Hirzel, Louis Mandel",2018,http://arxiv.org/abs/1804.06458v1
Convolutional Deep Exponential Families,"We describe convolutional deep exponential families (CDEFs) in this paper.
CDEFs are built based on deep exponential families, deep probabilistic models
that capture the hierarchical dependence between latent variables. CDEFs
greatly reduce the number of free parameters by tying the weights of DEFs. Our
experiments show that CDEFs are able to uncover time correlations with a small
amount of data.","Chengkuan Hong, Christian R. Shelton",2021,http://arxiv.org/abs/2110.14800v1
Learning Deep Energy Models: Contrastive Divergence vs. Amortized MLE,"We propose a number of new algorithms for learning deep energy models and
demonstrate their properties. We show that our SteinCD performs well in term of
test likelihood, while SteinGAN performs well in terms of generating realistic
looking images. Our results suggest promising directions for learning better
models by combining GAN-style methods with traditional energy-based learning.","Qiang Liu, Dilin Wang",2017,http://arxiv.org/abs/1707.00797v1
Online Deep Learning: Learning Deep Neural Networks on the Fly,"Deep Neural Networks (DNNs) are typically trained by backpropagation in a
batch learning setting, which requires the entire training data to be made
available prior to the learning task. This is not scalable for many real-world
scenarios where new data arrives sequentially in a stream form. We aim to
address an open challenge of ""Online Deep Learning"" (ODL) for learning DNNs on
the fly in an online setting. Unlike traditional online learning that often
optimizes some convex objective function with respect to a shallow model (e.g.,
a linear/kernel-based hypothesis), ODL is significantly more challenging since
the optimization of the DNN objective function is non-convex, and regular
backpropagation does not work well in practice, especially for online learning
settings. In this paper, we present a new online deep learning framework that
attempts to tackle the challenges by learning DNN models of adaptive depth from
a sequence of training data in an online learning setting. In particular, we
propose a novel Hedge Backpropagation (HBP) method for online updating the
parameters of DNN effectively, and validate the efficacy of our method on
large-scale data sets, including both stationary and concept drifting
scenarios.","Doyen Sahoo, Quang Pham, Jing Lu, Steven C. H. Hoi",2017,http://arxiv.org/abs/1711.03705v1
Quantum-Classical Machine learning by Hybrid Tensor Networks,"Tensor networks (TN) have found a wide use in machine learning, and in
particular, TN and deep learning bear striking similarities. In this work, we
propose the quantum-classical hybrid tensor networks (HTN) which combine tensor
networks with classical neural networks in a uniform deep learning framework to
overcome the limitations of regular tensor networks in machine learning. We
first analyze the limitations of regular tensor networks in the applications of
machine learning involving the representation power and architecture
scalability. We conclude that in fact the regular tensor networks are not
competent to be the basic building blocks of deep learning. Then, we discuss
the performance of HTN which overcome all the deficiency of regular tensor
networks for machine learning. In this sense, we are able to train HTN in the
deep learning way which is the standard combination of algorithms such as Back
Propagation and Stochastic Gradient Descent. We finally provide two applicable
cases to show the potential applications of HTN, including quantum states
classification and quantum-classical autoencoder. These cases also demonstrate
the great potentiality to design various HTN in deep learning way.","Ding Liu, Jiaqi Yao, Zekun Yao, Quan Zhang",2020,http://arxiv.org/abs/2005.09428v2
"Performance of Deep Learning models with transfer learning for
  multiple-step-ahead forecasts in monthly time series","Deep Learning and transfer learning models are being used to generate time
series forecasts; however, there is scarce evidence about their performance
prediction that it is more evident for monthly time series. The purpose of this
paper is to compare Deep Learning models with transfer learning and without
transfer learning and other traditional methods used for monthly forecasts to
answer three questions about the suitability of Deep Learning and Transfer
Learning to generate predictions of time series. Time series of M4 and M3
competitions were used for the experiments. The results suggest that deep
learning models based on TCN, LSTM, and CNN with transfer learning tend to
surpass the performance prediction of other traditional methods. On the other
hand, TCN and LSTM, trained directly on the target time series, got similar or
better performance than traditional methods for some forecast horizons.","Martín Solís, Luis-Alexander Calvo-Valverde",2022,http://arxiv.org/abs/2203.11196v2
Impact of Physical Activity on Sleep:A Deep Learning Based Exploration,"The importance of sleep is paramount for maintaining physical, emotional and
mental wellbeing. Though the relationship between sleep and physical activity
is known to be important, it is not yet fully understood. The explosion in
popularity of actigraphy and wearable devices, provides a unique opportunity to
understand this relationship. Leveraging this information source requires new
tools to be developed to facilitate data-driven research for sleep and activity
patient-recommendations.
  In this paper we explore the use of deep learning to build sleep quality
prediction models based on actigraphy data. We first use deep learning as a
pure model building device by performing human activity recognition (HAR) on
raw sensor data, and using deep learning to build sleep prediction models. We
compare the deep learning models with those build using classical approaches,
i.e. logistic regression, support vector machines, random forest and adaboost.
Secondly, we employ the advantage of deep learning with its ability to handle
high dimensional datasets. We explore several deep learning models on the raw
wearable sensor output without performing HAR or any other feature extraction.
  Our results show that using a convolutional neural network on the raw
wearables output improves the predictive value of sleep quality from physical
activity, by an additional 8% compared to state-of-the-art non-deep learning
approaches, which itself shows a 15% improvement over current practice.
Moreover, utilizing deep learning on raw data eliminates the need for data
pre-processing and simplifies the overall workflow to analyze actigraphy data
for sleep and physical activity research.","Aarti Sathyanarayana, Shafiq Joty, Luis Fernandez-Luque, Ferda Ofli, Jaideep Srivastava, Ahmed Elmagarmid, Shahrad Taheri, Teresa Arora",2016,http://arxiv.org/abs/1607.07034v1
Deep Learning in EEG: Advance of the Last Ten-Year Critical Period,"Deep learning has achieved excellent performance in a wide range of domains,
especially in speech recognition and computer vision. Relatively less work has
been done for EEG, but there is still significant progress attained in the last
decade. Due to the lack of a comprehensive and topic widely covered survey for
deep learning in EEG, we attempt to summarize recent progress to provide an
overview, as well as perspectives for future developments. We first briefly
mention the artifacts removal for EEG signal and then introduce deep learning
models that have been utilized in EEG processing and classification.
Subsequently, the applications of deep learning in EEG are reviewed by
categorizing them into groups such as brain-computer interface, disease
detection, and emotion recognition. They are followed by the discussion, in
which the pros and cons of deep learning are presented and future directions
and challenges for deep learning in EEG are proposed. We hope that this paper
could serve as a summary of past work for deep learning in EEG and the
beginning of further developments and achievements of EEG studies based on deep
learning.","Shu Gong, Kaibo Xing, Andrzej Cichocki, Junhua Li",2020,http://arxiv.org/abs/2011.11128v3
Bringing AI To Edge: From Deep Learning's Perspective,"Edge computing and artificial intelligence (AI), especially deep learning for
nowadays, are gradually intersecting to build a novel system, called edge
intelligence. However, the development of edge intelligence systems encounters
some challenges, and one of these challenges is the \textit{computational gap}
between computation-intensive deep learning algorithms and less-capable edge
systems. Due to the computational gap, many edge intelligence systems cannot
meet the expected performance requirements. To bridge the gap, a plethora of
deep learning techniques and optimization methods are proposed in the past
years: light-weight deep learning models, network compression, and efficient
neural architecture search. Although some reviews or surveys have partially
covered this large body of literature, we lack a systematic and comprehensive
review to discuss all aspects of these deep learning techniques which are
critical for edge intelligence implementation. As various and diverse methods
which are applicable to edge systems are proposed intensively, a holistic
review would enable edge computing engineers and community to know the
state-of-the-art deep learning techniques which are instrumental for edge
intelligence and to facilitate the development of edge intelligence systems.
This paper surveys the representative and latest deep learning techniques that
are useful for edge intelligence systems, including hand-crafted models, model
compression, hardware-aware neural architecture search and adaptive deep
learning models. Finally, based on observations and simple experiments we
conducted, we discuss some future directions.","Di Liu, Hao Kong, Xiangzhong Luo, Weichen Liu, Ravi Subramaniam",2020,http://arxiv.org/abs/2011.14808v1
"Impact of Deep Learning Libraries on Online Adaptive Lightweight Time
  Series Anomaly Detection","Providing online adaptive lightweight time series anomaly detection without
human intervention and domain knowledge is highly valuable. Several such
anomaly detection approaches have been introduced in the past years, but all of
them were only implemented in one deep learning library. With the development
of deep learning libraries, it is unclear how different deep learning libraries
impact these anomaly detection approaches since there is no such evaluation
available. Randomly choosing a deep learning library to implement an anomaly
detection approach might not be able to show the true performance of the
approach. It might also mislead users in believing one approach is better than
another. Therefore, in this paper, we investigate the impact of deep learning
libraries on online adaptive lightweight time series anomaly detection by
implementing two state-of-the-art anomaly detection approaches in three
well-known deep learning libraries and evaluating how these two approaches are
individually affected by the three deep learning libraries. A series of
experiments based on four real-world open-source time series datasets were
conducted. The results provide a good reference to select an appropriate deep
learning library for online adaptive lightweight anomaly detection.","Ming-Chang Lee, Jia-Chun Lin",2023,http://arxiv.org/abs/2305.00595v2
Model-Based Deep Learning,"Signal processing traditionally relies on classical statistical modeling
techniques. Such model-based methods utilize mathematical formulations that
represent the underlying physics, prior information and additional domain
knowledge. Simple classical models are useful but sensitive to inaccuracies and
may lead to poor performance when real systems display complex or dynamic
behavior. More recently, deep learning approaches that use deep neural networks
are becoming increasingly popular. Deep learning systems do not rely on
mathematical modeling, and learn their mapping from data, which allows them to
operate in complex environments. However, they lack the interpretability and
reliability of model-based methods, typically require large training sets to
obtain good performance, and tend to be computationally complex. Model-based
signal processing methods and data-centric deep learning each have their pros
and cons. These paradigms can be characterized as edges of a continuous
spectrum varying in specificity and parameterization. The methodologies that
lie in the middle ground of this spectrum, thus integrating model-based signal
processing with deep learning, are referred to as model-based deep learning,
and are the focus here. This monograph provides a tutorial style presentation
of model-based deep learning methodologies. These are families of algorithms
that combine principled mathematical models with data-driven systems to benefit
from the advantages of both approaches. Such model-based deep learning methods
exploit both partial domain knowledge, via mathematical structures designed for
specific problems, as well as learning from limited data. We accompany our
presentation with running examples, in super-resolution, dynamic systems, and
array processing. We show how they are expressed using the provided
characterization and specialized in each of the detailed methodologies.","Nir Shlezinger, Yonina C. Eldar",2023,http://arxiv.org/abs/2306.04469v1
Cross-Layer Optimization for Fault-Tolerant Deep Learning,"Fault-tolerant deep learning accelerator is the basis for highly reliable
deep learning processing and critical to deploy deep learning in
safety-critical applications such as avionics and robotics. Since deep learning
is known to be computing- and memory-intensive, traditional fault-tolerant
approaches based on redundant computing will incur substantial overhead
including power consumption and chip area. To this end, we propose to
characterize deep learning vulnerability difference across both neurons and
bits of each neuron, and leverage the vulnerability difference to enable
selective protection of the deep learning processing components from the
perspective of architecture layer and circuit layer respectively. At the same
time, we observe the correlation between model quantization and bit protection
overhead of the underlying processing elements of deep learning accelerators,
and propose to reduce the bit protection overhead by adding additional
quantization constrain without compromising the model accuracy. Finally, we
employ Bayesian optimization strategy to co-optimize the correlated cross-layer
design parameters at algorithm layer, architecture layer, and circuit layer to
minimize the hardware resource consumption while fulfilling multiple user
constraints including reliability, accuracy, and performance of the deep
learning processing at the same time.","Qing Zhang, Cheng Liu, Bo Liu, Haitong Huang, Ying Wang, Huawei Li, Xiaowei Li",2023,http://arxiv.org/abs/2312.13754v1
"Improving Interpretability of Deep Active Learning for Flood Inundation
  Mapping Through Class Ambiguity Indices Using Multi-spectral Satellite
  Imagery","Flood inundation mapping is a critical task for responding to the increasing
risk of flooding linked to global warming. Significant advancements of deep
learning in recent years have triggered its extensive applications, including
flood inundation mapping. To cope with the time-consuming and labor-intensive
data labeling process in supervised learning, deep active learning strategies
are one of the feasible approaches. However, there remains limited exploration
into the interpretability of how deep active learning strategies operate, with
a specific focus on flood inundation mapping in the field of remote sensing. In
this study, we introduce a novel framework of Interpretable Deep Active
Learning for Flood inundation Mapping (IDAL-FIM), specifically in terms of
class ambiguity of multi-spectral satellite images. In the experiments, we
utilize Sen1Floods11 dataset, and adopt U-Net with MC-dropout. In addition, we
employ five acquisition functions, which are the random, K-means, BALD,
entropy, and margin acquisition functions. Based on the experimental results,
we demonstrate that two proposed class ambiguity indices are effective
variables to interpret the deep active learning by establishing statistically
significant correlation with the predictive uncertainty of the deep learning
model at the tile level. Then, we illustrate the behaviors of deep active
learning through visualizing two-dimensional density plots and providing
interpretations regarding the operation of deep active learning, in flood
inundation mapping.","Hyunho Lee, Wenwen Li",2024,http://arxiv.org/abs/2404.19043v1
"Quantile deep learning models for multi-step ahead time series
  prediction","Uncertainty quantification is crucial in time series prediction, and quantile
regression offers a valuable mechanism for uncertainty quantification which is
useful for extreme value forecasting. Although deep learning models have been
prominent in multi-step ahead prediction, the development and evaluation of
quantile deep learning models have been limited. We present a novel quantile
regression deep learning framework for multi-step time series prediction. In
this way, we elevate the capabilities of deep learning models by incorporating
quantile regression, thus providing a more nuanced understanding of predictive
values. We provide an implementation of prominent deep learning models for
multi-step ahead time series prediction and evaluate their performance under
high volatility and extreme conditions. We include multivariate and univariate
modelling, strategies and provide a comparison with conventional deep learning
models from the literature. Our models are tested on two cryptocurrencies:
Bitcoin and Ethereum, using daily close-price data and selected benchmark time
series datasets. The results show that integrating a quantile loss function
with deep learning provides additional predictions for selected quantiles
without a loss in the prediction accuracy when compared to the literature. Our
quantile model has the ability to handle volatility more effectively and
provides additional information for decision-making and uncertainty
quantification through the use of quantiles when compared to conventional deep
learning models.","Jimmy Cheung, Smruthi Rangarajan, Amelia Maddocks, Xizhe Chen, Rohitash Chandra",2024,http://arxiv.org/abs/2411.15674v1
"Applications of Deep Reinforcement Learning in Communications and
  Networking: A Survey","This paper presents a comprehensive literature review on applications of deep
reinforcement learning in communications and networking. Modern networks, e.g.,
Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become
more decentralized and autonomous. In such networks, network entities need to
make decisions locally to maximize the network performance under uncertainty of
network environment. Reinforcement learning has been efficiently used to enable
the network entities to obtain the optimal policy including, e.g., decisions or
actions, given their states when the state and action spaces are small.
However, in complex and large-scale networks, the state and action spaces are
usually large, and the reinforcement learning may not be able to find the
optimal policy in reasonable time. Therefore, deep reinforcement learning, a
combination of reinforcement learning with deep learning, has been developed to
overcome the shortcomings. In this survey, we first give a tutorial of deep
reinforcement learning from fundamental concepts to advanced models. Then, we
review deep reinforcement learning approaches proposed to address emerging
issues in communications and networking. The issues include dynamic network
access, data rate control, wireless caching, data offloading, network security,
and connectivity preservation which are all important to next generation
networks such as 5G and beyond. Furthermore, we present applications of deep
reinforcement learning for traffic routing, resource sharing, and data
collection. Finally, we highlight important challenges, open issues, and future
research directions of applying deep reinforcement learning.","Nguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang, Ying-Chang Liang, Dong In Kim",2018,http://arxiv.org/abs/1810.07862v1
"Can Deep Learning Predict Risky Retail Investors? A Case Study in
  Financial Risk Behavior Forecasting","The paper examines the potential of deep learning to support decisions in
financial risk management. We develop a deep learning model for predicting
whether individual spread traders secure profits from future trades. This task
embodies typical modeling challenges faced in risk and behavior forecasting.
Conventional machine learning requires data that is representative of the
feature-target relationship and relies on the often costly development,
maintenance, and revision of handcrafted features. Consequently, modeling
highly variable, heterogeneous patterns such as trader behavior is challenging.
Deep learning promises a remedy. Learning hierarchical distributed
representations of the data in an automatic manner (e.g. risk taking behavior),
it uncovers generative features that determine the target (e.g., trader's
profitability), avoids manual feature engineering, and is more robust toward
change (e.g. dynamic market conditions). The results of employing a deep
network for operational risk forecasting confirm the feature learning
capability of deep learning, provide guidance on designing a suitable network
architecture and demonstrate the superiority of deep learning over machine
learning and rule-based benchmarks.","Yaodong Yang, Alisa Kolesnikova, Stefan Lessmann, Tiejun Ma, Ming-Chien Sung, Johnnie E. V. Johnson",2018,http://arxiv.org/abs/1812.06175v3
"Deep Learning to Address Candidate Generation and Cold Start Challenges
  in Recommender Systems: A Research Survey","Among the machine learning applications to business, recommender systems
would take one of the top places when it comes to success and adoption. They
help the user in accelerating the process of search while helping businesses
maximize sales. Post phenomenal success in computer vision and speech
recognition, deep learning methods are beginning to get applied to recommender
systems. Current survey papers on deep learning in recommender systems provide
a historical overview and taxonomy of recommender systems based on type. Our
paper addresses the gaps of providing a taxonomy of deep learning approaches to
address recommender systems problems in the areas of cold start and candidate
generation in recommender systems. We outline different challenges in
recommender systems into those related to the recommendations themselves
(include relevance, speed, accuracy and scalability), those related to the
nature of the data (cold start problem, imbalance and sparsity) and candidate
generation. We then provide a taxonomy of deep learning techniques to address
these challenges. Deep learning techniques are mapped to the different
challenges in recommender systems providing an overview of how deep learning
techniques can be used to address them. We contribute a taxonomy of deep
learning techniques to address the cold start and candidate generation problems
in recommender systems. Cold Start is addressed through additional features
(for audio, images, text) and by learning hidden user and item representations.
Candidate generation has been addressed by separate networks, RNNs,
autoencoders and hybrid methods. We also summarize the advantages and
limitations of these techniques while outlining areas for future research.","Kiran Rama, Pradeep Kumar, Bharat Bhasker",2019,http://arxiv.org/abs/1907.08674v1
Is deep learning a useful tool for the pure mathematician?,"A personal and informal account of what a pure mathematician might expect
when using tools from deep learning in their research.",Geordie Williamson,2023,http://arxiv.org/abs/2304.12602v2
Geometric deep learning approach to knot theory,"In this paper, we introduce a novel way to use geometric deep learning for
knot data by constructing a functor that takes knots to graphs and using graph
neural networks. We will attempt to predict several knot invariants with this
approach. This approach demonstrates high generalization capabilities.",Lennart Jaretzki,2023,http://arxiv.org/abs/2305.16808v1
Deep Reinforcement Learning for Adaptive Learning Systems,"In this paper, we formulate the adaptive learning problem---the problem of
how to find an individualized learning plan (called policy) that chooses the
most appropriate learning materials based on learner's latent traits---faced in
adaptive learning systems as a Markov decision process (MDP). We assume latent
traits to be continuous with an unknown transition model. We apply a model-free
deep reinforcement learning algorithm---the deep Q-learning algorithm---that
can effectively find the optimal learning policy from data on learners'
learning process without knowing the actual transition model of the learners'
continuous latent traits. To efficiently utilize available data, we also
develop a transition model estimator that emulates the learner's learning
process using neural networks. The transition model estimator can be used in
the deep Q-learning algorithm so that it can more efficiently discover the
optimal learning policy for a learner. Numerical simulation studies verify that
the proposed algorithm is very efficient in finding a good learning policy,
especially with the aid of a transition model estimator, it can find the
optimal learning policy after training using a small number of learners.","Xiao Li, Hanchen Xu, Jinming Zhang, Hua-hua Chang",2020,http://arxiv.org/abs/2004.08410v1
"Algebraically-Informed Deep Networks (AIDN): A Deep Learning Approach to
  Represent Algebraic Structures","One of the central problems in the interface of deep learning and mathematics
is that of building learning systems that can automatically uncover underlying
mathematical laws from observed data. In this work, we make one step towards
building a bridge between algebraic structures and deep learning, and introduce
\textbf{AIDN}, \textit{Algebraically-Informed Deep Networks}. \textbf{AIDN} is
a deep learning algorithm to represent any finitely-presented algebraic object
with a set of deep neural networks. The deep networks obtained via
\textbf{AIDN} are \textit{algebraically-informed} in the sense that they
satisfy the algebraic relations of the presentation of the algebraic structure
that serves as the input to the algorithm. Our proposed network can robustly
compute linear and non-linear representations of most finitely-presented
algebraic structures such as groups, associative algebras, and Lie algebras. We
evaluate our proposed approach and demonstrate its applicability to algebraic
and geometric objects that are significant in low-dimensional topology. In
particular, we study solutions for the Yang-Baxter equations and their
applications on braid groups. Further, we study the representations of the
Temperley-Lieb algebra. Finally, we show, using the Reshetikhin-Turaev
construction, how our proposed deep learning approach can be utilized to
construct new link invariants. We believe the proposed approach would tread a
path toward a promising future research in deep learning applied to algebraic
and geometric structures.","Mustafa Hajij, Ghada Zamzmi, Matthew Dawson, Greg Muller",2020,http://arxiv.org/abs/2012.01141v3
"Breaking the Curse of Dimensionality in Deep Neural Networks by Learning
  Invariant Representations","Artificial intelligence, particularly the subfield of machine learning, has
seen a paradigm shift towards data-driven models that learn from and adapt to
data. This has resulted in unprecedented advancements in various domains such
as natural language processing and computer vision, largely attributed to deep
learning, a special class of machine learning models. Deep learning arguably
surpasses traditional approaches by learning the relevant features from raw
data through a series of computational layers.
  This thesis explores the theoretical foundations of deep learning by studying
the relationship between the architecture of these models and the inherent
structures found within the data they process. In particular, we ask What
drives the efficacy of deep learning algorithms and allows them to beat the
so-called curse of dimensionality-i.e. the difficulty of generally learning
functions in high dimensions due to the exponentially increasing need for data
points with increased dimensionality? Is it their ability to learn relevant
representations of the data by exploiting their structure? How do different
architectures exploit different data structures? In order to address these
questions, we push forward the idea that the structure of the data can be
effectively characterized by its invariances-i.e. aspects that are irrelevant
for the task at hand.
  Our methodology takes an empirical approach to deep learning, combining
experimental studies with physics-inspired toy models. These simplified models
allow us to investigate and interpret the complex behaviors we observe in deep
learning systems, offering insights into their inner workings, with the
far-reaching goal of bridging the gap between theory and practice.",Leonardo Petrini,2023,http://arxiv.org/abs/2310.16154v1
"Every Model Learned by Gradient Descent Is Approximately a Kernel
  Machine","Deep learning's successes are often attributed to its ability to
automatically discover new representations of the data, rather than relying on
handcrafted features like other learning methods. We show, however, that deep
networks learned by the standard gradient descent algorithm are in fact
mathematically approximately equivalent to kernel machines, a learning method
that simply memorizes the data and uses it directly for prediction via a
similarity function (the kernel). This greatly enhances the interpretability of
deep network weights, by elucidating that they are effectively a superposition
of the training examples. The network architecture incorporates knowledge of
the target function into the kernel. This improved understanding should lead to
better learning algorithms.",Pedro Domingos,2020,http://arxiv.org/abs/2012.00152v1
Deep Learning with Nonparametric Clustering,"Clustering is an essential problem in machine learning and data mining. One
vital factor that impacts clustering performance is how to learn or design the
data representation (or features). Fortunately, recent advances in deep
learning can learn unsupervised features effectively, and have yielded state of
the art performance in many classification problems, such as character
recognition, object recognition and document categorization. However, little
attention has been paid to the potential of deep learning for unsupervised
clustering problems. In this paper, we propose a deep belief network with
nonparametric clustering. As an unsupervised method, our model first leverages
the advantages of deep learning for feature representation and dimension
reduction. Then, it performs nonparametric clustering under a maximum margin
framework -- a discriminative clustering model and can be trained online
efficiently in the code space. Lastly model parameters are refined in the deep
belief network. Thus, this model can learn features for clustering and infer
model complexity in an unified framework. The experimental results show the
advantage of our approach over competitive baselines.",Gang Chen,2015,http://arxiv.org/abs/1501.03084v1
RMDL: Random Multimodel Deep Learning for Classification,"The continually increasing number of complex datasets each year necessitates
ever improving machine learning methods for robust and accurate categorization
of these data. This paper introduces Random Multimodel Deep Learning (RMDL): a
new ensemble, deep learning approach for classification. Deep learning models
have achieved state-of-the-art results across many domains. RMDL solves the
problem of finding the best deep learning structure and architecture while
simultaneously improving robustness and accuracy through ensembles of deep
learning architectures. RDML can accept as input a variety data to include
text, video, images, and symbolic. This paper describes RMDL and shows test
results for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB,
and 20newsgroup. These test results show that RDML produces consistently better
performance than standard methods over a broad range of data types and
classification problems.","Kamran Kowsari, Mojtaba Heidarysafa, Donald E. Brown, Kiana Jafari Meimandi, Laura E. Barnes",2018,http://arxiv.org/abs/1805.01890v2
Deep learning for in vitro prediction of pharmaceutical formulations,"Current pharmaceutical formulation development still strongly relies on the
traditional trial-and-error approach by individual experiences of
pharmaceutical scientists, which is laborious, time-consuming and costly.
Recently, deep learning has been widely applied in many challenging domains
because of its important capability of automatic feature extraction. The aim of
this research is to use deep learning to predict pharmaceutical formulations.
In this paper, two different types of dosage forms were chosen as model
systems. Evaluation criteria suitable for pharmaceutics were applied to
assessing the performance of the models. Moreover, an automatic dataset
selection algorithm was developed for selecting the representative data as
validation and test datasets. Six machine learning methods were compared with
deep learning. The result shows the accuracies of both two deep neural networks
were above 80% and higher than other machine learning models, which showed good
prediction in pharmaceutical formulations. In summary, deep learning with the
automatic data splitting algorithm and the evaluation criteria suitable for
pharmaceutical formulation data was firstly developed for the prediction of
pharmaceutical formulations. The cross-disciplinary integration of
pharmaceutics and artificial intelligence may shift the paradigm of
pharmaceutical researches from experience-dependent studies to data-driven
methodologies.","Yilong Yang, Zhuyifan Ye, Yan Su, Qianqian Zhao, Xiaoshan Li, Defang Ouyang",2018,http://arxiv.org/abs/1809.02069v1
Deep Quality-Value (DQV) Learning,"We introduce a novel Deep Reinforcement Learning (DRL) algorithm called Deep
Quality-Value (DQV) Learning. DQV uses temporal-difference learning to train a
Value neural network and uses this network for training a second Quality-value
network that learns to estimate state-action values. We first test DQV's update
rules with Multilayer Perceptrons as function approximators on two classic RL
problems, and then extend DQV with the use of Deep Convolutional Neural
Networks, `Experience Replay' and `Target Neural Networks' for tackling four
games of the Atari Arcade Learning environment. Our results show that DQV
learns significantly faster and better than Deep Q-Learning and Double Deep
Q-Learning, suggesting that our algorithm can potentially be a better
performing synchronous temporal difference algorithm than what is currently
present in DRL.","Matthia Sabatelli, Gilles Louppe, Pierre Geurts, Marco A. Wiering",2018,http://arxiv.org/abs/1810.00368v2
An Introduction to Deep Reinforcement Learning,"Deep reinforcement learning is the combination of reinforcement learning (RL)
and deep learning. This field of research has been able to solve a wide range
of complex decision-making tasks that were previously out of reach for a
machine. Thus, deep RL opens up many new applications in domains such as
healthcare, robotics, smart grids, finance, and many more. This manuscript
provides an introduction to deep reinforcement learning models, algorithms and
techniques. Particular focus is on the aspects related to generalization and
how deep RL can be used for practical applications. We assume the reader is
familiar with basic machine learning concepts.","Vincent Francois-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare, Joelle Pineau",2018,http://arxiv.org/abs/1811.12560v2
Lecture Notes: Neural Network Architectures,"These lecture notes provide an overview of Neural Network architectures from
a mathematical point of view. Especially, Machine Learning with Neural Networks
is seen as an optimization problem. Covered are an introduction to Neural
Networks and the following architectures: Feedforward Neural Network,
Convolutional Neural Network, ResNet, and Recurrent Neural Network.",Evelyn Herberg,2023,http://arxiv.org/abs/2304.05133v2
"Neural Network Processing Neural Networks: An efficient way to learn
  higher order functions","Functions are rich in meaning and can be interpreted in a variety of ways.
Neural networks were proven to be capable of approximating a large class of
functions[1]. In this paper, we propose a new class of neural networks called
""Neural Network Processing Neural Networks"" (NNPNNs), which inputs neural
networks and numerical values, instead of just numerical values. Thus enabling
neural networks to represent and process rich structures.",Firat Tuna,2019,http://arxiv.org/abs/1911.05640v2
"Guaranteed Quantization Error Computation for Neural Network Model
  Compression","Neural network model compression techniques can address the computation issue
of deep neural networks on embedded devices in industrial systems. The
guaranteed output error computation problem for neural network compression with
quantization is addressed in this paper. A merged neural network is built from
a feedforward neural network and its quantized version to produce the exact
output difference between two neural networks. Then, optimization-based methods
and reachability analysis methods are applied to the merged neural network to
compute the guaranteed quantization error. Finally, a numerical example is
proposed to validate the applicability and effectiveness of the proposed
approach.","Wesley Cooke, Zihao Mo, Weiming Xiang",2023,http://arxiv.org/abs/2304.13812v1
Graph Structure of Neural Networks,"Neural networks are often represented as graphs of connections between
neurons. However, despite their wide use, there is currently little
understanding of the relationship between the graph structure of the neural
network and its predictive performance. Here we systematically investigate how
does the graph structure of neural networks affect their predictive
performance. To this end, we develop a novel graph-based representation of
neural networks called relational graph, where layers of neural network
computation correspond to rounds of message exchange along the graph structure.
Using this representation we show that: (1) a ""sweet spot"" of relational graphs
leads to neural networks with significantly improved predictive performance;
(2) neural network's performance is approximately a smooth function of the
clustering coefficient and average path length of its relational graph; (3) our
findings are consistent across many different tasks and datasets; (4) the sweet
spot can be identified efficiently; (5) top-performing neural networks have
graph structure surprisingly similar to those of real biological neural
networks. Our work opens new directions for the design of neural architectures
and the understanding on neural networks in general.","Jiaxuan You, Jure Leskovec, Kaiming He, Saining Xie",2020,http://arxiv.org/abs/2007.06559v2
"Hybrid Quantum-Classical Neural Networks for Downlink Beamforming
  Optimization","This paper investigates quantum machine learning to optimize the beamforming
in a multiuser multiple-input single-output downlink system. We aim to combine
the power of quantum neural networks and the success of classical deep neural
networks to enhance the learning performance. Specifically, we propose two
hybrid quantum-classical neural networks to maximize the sum rate of a downlink
system. The first one proposes a quantum neural network employing parameterized
quantum circuits that follows a classical convolutional neural network. The
classical neural network can be jointly trained with the quantum neural network
or pre-trained leading to a fine-tuning transfer learning method. The second
one designs a quantum convolutional neural network to better extract features
followed by a classical deep neural network. Our results demonstrate the
feasibility of the proposed hybrid neural networks, and reveal that the first
method can achieve similar sum rate performance compared to a benchmark
classical neural network with significantly less training parameters; while the
second method can achieve higher sum rate especially in presence of many users
still with less training parameters. The robustness of the proposed methods is
verified using both software simulators and hardware emulators considering
noisy intermediate-scale quantum devices.","Juping Zhang, Gan Zheng, Toshiaki Koike-Akino, Kai-Kit Wong, Fraser Burton",2024,http://arxiv.org/abs/2408.04747v1
Cortex Neural Network: learning with Neural Network groups,"Neural Network has been successfully applied to many real-world problems,
such as image recognition and machine translation. However, for the current
architecture of neural networks, it is hard to perform complex cognitive tasks,
for example, to process the image and audio inputs together. Cortex, as an
important architecture in the brain, is important for animals to perform the
complex cognitive task. We view the architecture of Cortex in the brain as a
missing part in the design of the current artificial neural network. In this
paper, we purpose Cortex Neural Network (CrtxNN). The Cortex Neural Network is
an upper architecture of neural networks which motivated from cerebral cortex
in the brain to handle different tasks in the same learning system. It is able
to identify different tasks and solve them with different methods. In our
implementation, the Cortex Neural Network is able to process different
cognitive tasks and perform reflection to get a higher accuracy. We provide a
series of experiments to examine the capability of the cortex architecture on
traditional neural networks. Our experiments proved its ability on the Cortex
Neural Network can reach accuracy by 98.32% on MNIST and 62% on CIFAR10 at the
same time, which can promisingly reduce the loss by 40%.",Liyao Gao,2018,http://arxiv.org/abs/1804.03313v1
Assessing Intelligence in Artificial Neural Networks,"The purpose of this work was to develop of metrics to assess network
architectures that balance neural network size and task performance. To this
end, the concept of neural efficiency is introduced to measure neural layer
utilization, and a second metric called artificial intelligence quotient (aIQ)
was created to balance neural network performance and neural network
efficiency. To study aIQ and neural efficiency, two simple neural networks were
trained on MNIST: a fully connected network (LeNet-300-100) and a convolutional
neural network (LeNet-5). The LeNet-5 network with the highest aIQ was 2.32%
less accurate but contained 30,912 times fewer parameters than the highest
accuracy network. Both batch normalization and dropout layers were found to
increase neural efficiency. Finally, high aIQ networks are shown to be
memorization and overtraining resistant, capable of learning proper digit
classification with an accuracy of 92.51% even when 75% of the class labels are
randomized. These results demonstrate the utility of aIQ and neural efficiency
as metrics for balancing network performance and size.","Nicholas J. Schaub, Nathan Hotaling",2020,http://arxiv.org/abs/2006.02909v1
Rational Neural Network Controllers,"Neural networks have shown great success in many machine learning related
tasks, due to their ability to act as general function approximators. Recent
work has demonstrated the effectiveness of neural networks in control systems
(known as neural feedback loops), most notably by using a neural network as a
controller. However, one of the big challenges of this approach is that neural
networks have been shown to be sensitive to adversarial attacks. This means
that, unless they are designed properly, they are not an ideal candidate for
controllers due to issues with robustness and uncertainty, which are pivotal
aspects of control systems. There has been initial work on robustness to both
analyse and design dynamical systems with neural network controllers. However,
one prominent issue with these methods is that they use existing neural network
architectures tailored for traditional machine learning tasks. These structures
may not be appropriate for neural network controllers and it is important to
consider alternative architectures. This paper considers rational neural
networks and presents novel rational activation functions, which can be used
effectively in robustness problems for neural feedback loops. Rational
activation functions are replaced by a general rational neural network
structure, which is convex in the neural network's parameters. A method is
proposed to recover a stabilising controller from a Sum of Squares feasibility
test. This approach is then applied to a refined rational neural network which
is more compatible with Sum of Squares programming. Numerical examples show
that this method can successfully recover stabilising rational neural network
controllers for neural feedback loops with non-linear plants with noise and
parametric uncertainty.","Matthew Newton, Antonis Papachristodoulou",2023,http://arxiv.org/abs/2307.06287v1
Asymptotic Theory of Expectile Neural Networks,"Neural networks are becoming an increasingly important tool in applications.
However, neural networks are not widely used in statistical genetics. In this
paper, we propose a new neural networks method called expectile neural
networks. When the size of parameter is too large, the standard maximum
likelihood procedures may not work. We use sieve method to constrain parameter
space. And we prove its consistency and normality under nonparametric
regression framework.","Jinghang Lin, Xiaoxi Shen, Qing Lu",2020,http://arxiv.org/abs/2011.01218v1
"Combining Recurrent and Convolutional Neural Networks for Relation
  Classification","This paper investigates two different neural architectures for the task of
relation classification: convolutional neural networks and recurrent neural
networks. For both models, we demonstrate the effect of different architectural
choices. We present a new context representation for convolutional neural
networks for relation classification (extended middle context). Furthermore, we
propose connectionist bi-directional recurrent neural networks and introduce
ranking loss for their optimization. Finally, we show that combining
convolutional and recurrent neural networks using a simple voting scheme is
accurate enough to improve results. Our neural models achieve state-of-the-art
results on the SemEval 2010 relation classification task.","Ngoc Thang Vu, Heike Adel, Pankaj Gupta, Hinrich Schütze",2016,http://arxiv.org/abs/1605.07333v1
"A Comprehensive Review of Spiking Neural Networks: Interpretation,
  Optimization, Efficiency, and Best Practices","Biological neural networks continue to inspire breakthroughs in neural
network performance. And yet, one key area of neural computation that has been
under-appreciated and under-investigated is biologically plausible,
energy-efficient spiking neural networks, whose potential is especially
attractive for low-power, mobile, or otherwise hardware-constrained settings.
We present a literature review of recent developments in the interpretation,
optimization, efficiency, and accuracy of spiking neural networks. Key
contributions include identification, discussion, and comparison of
cutting-edge methods in spiking neural network optimization, energy-efficiency,
and evaluation, starting from first principles so as to be accessible to new
practitioners.","Kai Malcolm, Josue Casco-Rodriguez",2023,http://arxiv.org/abs/2303.10780v2
"Design and development of opto-neural processors for simulation of
  neural networks trained in image detection for potential implementation in
  hybrid robotics","Neural networks have been employed for a wide range of processing
applications like image processing, motor control, object detection and many
others. Living neural networks offer advantages of lower power consumption,
faster processing, and biological realism. Optogenetics offers high spatial and
temporal control over biological neurons and presents potential in training
live neural networks. This work proposes a simulated living neural network
trained indirectly by backpropagating STDP based algorithms using precision
activation by optogenetics achieving accuracy comparable to traditional neural
network training algorithms.",Sanjana Shetty,2024,http://arxiv.org/abs/2401.10289v1
Convex Formulation of Overparameterized Deep Neural Networks,"Analysis of over-parameterized neural networks has drawn significant
attention in recentyears. It was shown that such systems behave like convex
systems under various restrictedsettings, such as for two-level neural
networks, and when learning is only restricted locally inthe so-called neural
tangent kernel space around specialized initializations. However, there areno
theoretical techniques that can analyze fully trained deep neural networks
encountered inpractice. This paper solves this fundamental problem by
investigating such overparameterizeddeep neural networks when fully trained. We
generalize a new technique called neural feature repopulation, originally
introduced in (Fang et al., 2019a) for two-level neural networks, to analyze
deep neural networks. It is shown that under suitable representations,
overparameterized deep neural networks are inherently convex, and when
optimized, the system can learn effective features suitable for the underlying
learning task under mild conditions. This new analysis is consistent with
empirical observations that deep neural networks are capable of learning
efficient feature representations. Therefore, the highly unexpected result of
this paper can satisfactorily explain the practical success of deep neural
networks. Empirical studies confirm that predictions of our theory are
consistent with results observed in practice.","Cong Fang, Yihong Gu, Weizhong Zhang, Tong Zhang",2019,http://arxiv.org/abs/1911.07626v1
"Approximate Bisimulation Relations for Neural Networks and Application
  to Assured Neural Network Compression","In this paper, we propose a concept of approximate bisimulation relation for
feedforward neural networks. In the framework of approximate bisimulation
relation, a novel neural network merging method is developed to compute the
approximate bisimulation error between two neural networks based on
reachability analysis of neural networks. The developed method is able to
quantitatively measure the distance between the outputs of two neural networks
with the same inputs. Then, we apply the approximate bisimulation relation
results to perform neural networks model reduction and compute the compression
precision, i.e., assured neural networks compression. At last, using the
assured neural network compression, we accelerate the verification processes of
ACAS Xu neural networks to illustrate the effectiveness and advantages of our
proposed approximate bisimulation approach.","Weiming Xiang, Zhongzhu Shao",2022,http://arxiv.org/abs/2202.01214v1
"Optimal rates of approximation by shallow ReLU$^k$ neural networks and
  applications to nonparametric regression","We study the approximation capacity of some variation spaces corresponding to
shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth
functions are contained in these spaces with finite variation norms. For
functions with less smoothness, the approximation rates in terms of the
variation norm are established. Using these results, we are able to prove the
optimal approximation rates in terms of the number of neurons for shallow
ReLU$^k$ neural networks. It is also shown how these results can be used to
derive approximation bounds for deep neural networks and convolutional neural
networks (CNNs). As applications, we study convergence rates for nonparametric
regression using three ReLU neural network models: shallow neural network,
over-parameterized neural network, and CNN. In particular, we show that shallow
neural networks can achieve the minimax optimal rates for learning H\""older
functions, which complements recent results for deep neural networks. It is
also proven that over-parameterized (deep or shallow) neural networks can
achieve nearly optimal rates for nonparametric regression.","Yunfei Yang, Ding-Xuan Zhou",2023,http://arxiv.org/abs/2304.01561v3
"Understanding Vector-Valued Neural Networks and Their Relationship with
  Real and Hypercomplex-Valued Neural Networks","Despite the many successful applications of deep learning models for
multidimensional signal and image processing, most traditional neural networks
process data represented by (multidimensional) arrays of real numbers. The
intercorrelation between feature channels is usually expected to be learned
from the training data, requiring numerous parameters and careful training. In
contrast, vector-valued neural networks are conceived to process arrays of
vectors and naturally consider the intercorrelation between feature channels.
Consequently, they usually have fewer parameters and often undergo more robust
training than traditional neural networks. This paper aims to present a broad
framework for vector-valued neural networks, referred to as V-nets. In this
context, hypercomplex-valued neural networks are regarded as vector-valued
models with additional algebraic properties. Furthermore, this paper explains
the relationship between vector-valued and traditional neural networks.
Precisely, a vector-valued neural network can be obtained by placing
restrictions on a real-valued model to consider the intercorrelation between
feature channels. Finally, we show how V-nets, including hypercomplex-valued
neural networks, can be implemented in current deep-learning libraries as
real-valued networks.",Marcos Eduardo Valle,2023,http://arxiv.org/abs/2309.07716v2
Nonlinear Systems Identification Using Deep Dynamic Neural Networks,"Neural networks are known to be effective function approximators. Recently,
deep neural networks have proven to be very effective in pattern recognition,
classification tasks and human-level control to model highly nonlinear
realworld systems. This paper investigates the effectiveness of deep neural
networks in the modeling of dynamical systems with complex behavior. Three deep
neural network structures are trained on sequential data, and we investigate
the effectiveness of these networks in modeling associated characteristics of
the underlying dynamical systems. We carry out similar evaluations on select
publicly available system identification datasets. We demonstrate that deep
neural networks are effective model estimators from input-output data","Olalekan Ogunmolu, Xuejun Gu, Steve Jiang, Nicholas Gans",2016,http://arxiv.org/abs/1610.01439v1
Geometric Decomposition of Feed Forward Neural Networks,"There have been several attempts to mathematically understand neural networks
and many more from biological and computational perspectives. The field has
exploded in the last decade, yet neural networks are still treated much like a
black box. In this work we describe a structure that is inherent to a feed
forward neural network. This will provide a framework for future work on neural
networks to improve training algorithms, compute the homology of the network,
and other applications. Our approach takes a more geometric point of view and
is unlike other attempts to mathematically understand neural networks that rely
on a functional perspective.",Sven Cattell,2016,http://arxiv.org/abs/1612.02522v1
Neural Networks Architecture Evaluation in a Quantum Computer,"In this work, we propose a quantum algorithm to evaluate neural networks
architectures named Quantum Neural Network Architecture Evaluation (QNNAE). The
proposed algorithm is based on a quantum associative memory and the learning
algorithm for artificial neural networks. Unlike conventional algorithms for
evaluating neural network architectures, QNNAE does not depend on
initialization of weights. The proposed algorithm has a binary output and
results in 0 with probability proportional to the performance of the network.
And its computational cost is equal to the computational cost to train a neural
network.","Adenilton José da Silva, Rodolfo Luan F. de Oliveira",2017,http://arxiv.org/abs/1711.04759v1
Building Compact and Robust Deep Neural Networks with Toeplitz Matrices,"Deep neural networks are state-of-the-art in a wide variety of tasks,
however, they exhibit important limitations which hinder their use and
deployment in real-world applications. When developing and training neural
networks, the accuracy should not be the only concern, neural networks must
also be cost-effective and reliable. Although accurate, large neural networks
often lack these properties. This thesis focuses on the problem of training
neural networks which are not only accurate but also compact, easy to train,
reliable and robust to adversarial examples. To tackle these problems, we
leverage the properties of structured matrices from the Toeplitz family to
build compact and secure neural networks.",Alexandre Araujo,2021,http://arxiv.org/abs/2109.00959v1
Application of Neural Network in Optimization of Chemical Process,"Artificial neural network (ANN) has been widely used due to its strong
nonlinear mapping ability, fault tolerance and self-learning ability. This
article summarizes the development history of artificial neural networks,
introduces three common neural network types, BP neural network, RBF neural
network and convolutional neural network, and focuses on the practical
application in chemical process optimization, especially the results achieved
in multi-objective control optimization and process parameter improvement.","Fei Liang, Taowen Zhang",2021,http://arxiv.org/abs/2110.04942v1
Compact Matrix Quantum Group Equivariant Neural Networks,"We derive the existence of a new type of neural network, called a compact
matrix quantum group equivariant neural network, that learns from data that has
an underlying quantum symmetry. We apply the Woronowicz formulation of
Tannaka-Krein duality to characterise the weight matrices that appear in these
neural networks for any easy compact matrix quantum group. We show that compact
matrix quantum group equivariant neural networks contain, as a subclass, all
compact matrix group equivariant neural networks. Moreover, we obtain
characterisations of the weight matrices for many compact matrix group
equivariant neural networks that have not previously appeared in the machine
learning literature.",Edward Pearce-Crump,2023,http://arxiv.org/abs/2311.06358v1
"Universal Approximation Theorem for Vector- and Hypercomplex-Valued
  Neural Networks","The universal approximation theorem states that a neural network with one
hidden layer can approximate continuous functions on compact sets with any
desired precision. This theorem supports using neural networks for various
applications, including regression and classification tasks. Furthermore, it is
valid for real-valued neural networks and some hypercomplex-valued neural
networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural
networks. However, hypercomplex-valued neural networks are a type of
vector-valued neural network defined on an algebra with additional algebraic or
geometric properties. This paper extends the universal approximation theorem
for a wide range of vector-valued neural networks, including
hypercomplex-valued models as particular instances. Precisely, we introduce the
concept of non-degenerate algebra and state the universal approximation theorem
for neural networks defined on such algebras.","Marcos Eduardo Valle, Wington L. Vital, Guilherme Vieira",2024,http://arxiv.org/abs/2401.02277v2
Detecting Neural Trojans Through Merkle Trees,"Deep neural networks are utilized in a growing number of industries. Much of
the current literature focuses on the applications of deep neural networks
without discussing the security of the network itself. One security issue
facing deep neural networks is neural trojans. Through a neural trojan, a
malicious actor may force the deep neural network to act in unintended ways.
Several potential defenses have been proposed, but they are computationally
expensive, complex, or unusable in commercial applications. We propose Merkle
trees as a novel way to detect and isolate neural trojans.",Joshua Strubel,2023,http://arxiv.org/abs/2306.05368v1
"Why Quantization Improves Generalization: NTK of Binary Weight Neural
  Networks","Quantized neural networks have drawn a lot of attention as they reduce the
space and computational complexity during the inference. Moreover, there has
been folklore that quantization acts as an implicit regularizer and thus can
improve the generalizability of neural networks, yet no existing work
formalizes this interesting folklore. In this paper, we take the binary weights
in a neural network as random variables under stochastic rounding, and study
the distribution propagation over different layers in the neural network. We
propose a quasi neural network to approximate the distribution propagation,
which is a neural network with continuous parameters and smooth activation
function. We derive the neural tangent kernel (NTK) for this quasi neural
network, and show that the eigenvalue of NTK decays at approximately
exponential rate, which is comparable to that of Gaussian kernel with
randomized scale. This in turn indicates that the Reproducing Kernel Hilbert
Space (RKHS) of a binary weight neural network covers a strict subset of
functions compared with the one with real value weights. We use experiments to
verify that the quasi neural network we proposed can well approximate binary
weight neural network. Furthermore, binary weight neural network gives a lower
generalization gap compared with real value weight neural network, which is
similar to the difference between Gaussian kernel and Laplace kernel.","Kaiqi Zhang, Ming Yin, Yu-Xiang Wang",2022,http://arxiv.org/abs/2206.05916v1
Bayesian Neural Networks: Essentials,"Bayesian neural networks utilize probabilistic layers that capture
uncertainty over weights and activations, and are trained using Bayesian
inference. Since these probabilistic layers are designed to be drop-in
replacement of their deterministic counter parts, Bayesian neural networks
provide a direct and natural way to extend conventional deep neural networks to
support probabilistic deep learning. However, it is nontrivial to understand,
design and train Bayesian neural networks due to their complexities. We discuss
the essentials of Bayesian neural networks including duality (deep neural
networks, probabilistic models), approximate Bayesian inference, Bayesian
priors, Bayesian posteriors, and deep variational learning. We use TensorFlow
Probability APIs and code examples for illustration. The main problem with
Bayesian neural networks is that the architecture of deep neural networks makes
it quite redundant, and costly, to account for uncertainty for a large number
of successive layers. Hybrid Bayesian neural networks, which use few
probabilistic layers judicially positioned in the networks, provide a practical
solution.",Daniel T. Chang,2021,http://arxiv.org/abs/2106.13594v1
Fourier Neural Networks for Function Approximation,"The success of Neural networks in providing miraculous results when applied
to a wide variety of tasks is astonishing. Insight in the working can be
obtained by studying the universal approximation property of neural networks.
It is proved extensively that neural networks are universal approximators.
Further it is proved that deep Neural networks are better approximators. It is
specifically proved that for a narrow neural network to approximate a function
which is otherwise implemented by a deep Neural network, the network take
exponentially large number of neurons. In this work, we have implemented
existing methodologies for a variety of synthetic functions and identified
their deficiencies. Further, we examined that Fourier neural network is able to
perform fairly good with only two layers in the neural network. A modified
Fourier Neural network which has sinusoidal activation and two hidden layer is
proposed and the results are tabulated.","R Subhash Chandra Bose, Kakarla Yaswanth",2021,http://arxiv.org/abs/2111.08438v1
"Genetic cellular neural networks for generating three-dimensional
  geometry","There are a number of ways to procedurally generate interesting
three-dimensional shapes, and a method where a cellular neural network is
combined with a mesh growth algorithm is presented here. The aim is to create a
shape from a genetic code in such a way that a crude search can find
interesting shapes. Identical neural networks are placed at each vertex of a
mesh which can communicate with neural networks on neighboring vertices. The
output of the neural networks determine how the mesh grows, allowing
interesting shapes to be produced emergently, mimicking some of the complexity
of biological organism development. Since the neural networks' parameters can
be freely mutated, the approach is amenable for use in a genetic algorithm.",Hugo Martay,2016,http://arxiv.org/abs/1603.08551v1
Survey of Dropout Methods for Deep Neural Networks,"Dropout methods are a family of stochastic techniques used in neural network
training or inference that have generated significant research interest and are
widely used in practice. They have been successfully applied in neural network
regularization, model compression, and in measuring the uncertainty of neural
network outputs. While original formulated for dense neural network layers,
recent advances have made dropout methods also applicable to convolutional and
recurrent neural network layers. This paper summarizes the history of dropout
methods, their various applications, and current areas of research interest.
Important proposed methods are described in additional detail.","Alex Labach, Hojjat Salehinejad, Shahrokh Valaee",2019,http://arxiv.org/abs/1904.13310v2
"General Regression Neural Networks, Radial Basis Function Neural
  Networks, Support Vector Machines, and Feedforward Neural Networks","The aim of this project is to develop a code to discover the optimal sigma
value that maximum the F1 score and the optimal sigma value that maximizes the
accuracy and to find out if they are the same. Four algorithms which can be
used to solve this problem are: Genetic Regression Neural Networks (GRNNs),
Radial Based Function (RBF) Neural Networks (RBFNNs), Support Vector Machines
(SVMs) and Feedforward Neural Network (FFNNs).","Alison Jenkins, Vinika Gupta, Mary Lenoir",2019,http://arxiv.org/abs/1911.07115v1
On neural network kernels and the storage capacity problem,"In this short note, we reify the connection between work on the storage
capacity problem in wide two-layer treelike neural networks and the
rapidly-growing body of literature on kernel limits of wide neural networks.
Concretely, we observe that the ""effective order parameter"" studied in the
statistical mechanics literature is exactly equivalent to the infinite-width
Neural Network Gaussian Process Kernel. This correspondence connects the
expressivity and trainability of wide two-layer neural networks.","Jacob A. Zavatone-Veth, Cengiz Pehlevan",2022,http://arxiv.org/abs/2201.04669v1
Deep Neural Networks - A Brief History,Introduction to deep neural networks and their history.,Krzysztof J. Cios,2017,http://arxiv.org/abs/1701.05549v1
GPU Acceleration of Sparse Neural Networks,"In this paper, we use graphics processing units(GPU) to accelerate sparse and
arbitrary structured neural networks. Sparse networks have nodes in the network
that are not fully connected with nodes in preceding and following layers, and
arbitrary structure neural networks have different number of nodes in each
layers. Sparse Neural networks with arbitrary structures are generally created
in the processes like neural network pruning and evolutionary machine learning
strategies. We show that we can gain significant speedup for full activation of
such neural networks using graphical processing units. We do a prepossessing
step to determine dependency groups for all the nodes in a network, and use
that information to guide the progression of activation in the neural network.
Then we compute activation for each nodes in its own separate thread in the
GPU, which allows for massive parallelization. We use CUDA framework to
implement our approach and compare the results of sequential and GPU
implementations. Our results show that the activation of sparse neural networks
lends very well to GPU acceleration and can help speed up machine learning
strategies which generate such networks or other processes that have similar
structure.","Aavaas Gajurel, Sushil J. Louis, Frederick C Harris",2020,http://arxiv.org/abs/2005.04347v1
Neural Network Pruning as Spectrum Preserving Process,"Neural networks have achieved remarkable performance in various application
domains. Nevertheless, a large number of weights in pre-trained deep neural
networks prohibit them from being deployed on smartphones and embedded systems.
It is highly desirable to obtain lightweight versions of neural networks for
inference in edge devices. Many cost-effective approaches were proposed to
prune dense and convolutional layers that are common in deep neural networks
and dominant in the parameter space. However, a unified theoretical foundation
for the problem mostly is missing. In this paper, we identify the close
connection between matrix spectrum learning and neural network training for
dense and convolutional layers and argue that weight pruning is essentially a
matrix sparsification process to preserve the spectrum. Based on the analysis,
we also propose a matrix sparsification algorithm tailored for neural network
pruning that yields better pruning result. We carefully design and conduct
experiments to support our arguments. Hence we provide a consolidated viewpoint
for neural network pruning and enhance the interpretability of deep neural
networks by identifying and preserving the critical neural weights.","Shibo Yao, Dantong Yu, Ioannis Koutis",2023,http://arxiv.org/abs/2307.08982v1
On Hiding Neural Networks Inside Neural Networks,"Modern neural networks often contain significantly more parameters than the
size of their training data. We show that this excess capacity provides an
opportunity for embedding secret machine learning models within a trained
neural network. Our novel framework hides the existence of a secret neural
network with arbitrary desired functionality within a carrier network. We prove
theoretically that the secret network's detection is computationally infeasible
and demonstrate empirically that the carrier network does not compromise the
secret network's disguise. Our paper introduces a previously unknown
steganographic technique that can be exploited by adversaries if left
unchecked.","Chuan Guo, Ruihan Wu, Kilian Q. Weinberger",2020,http://arxiv.org/abs/2002.10078v3
"Deep physical neural networks enabled by a backpropagation algorithm for
  arbitrary physical systems","Deep neural networks have become a pervasive tool in science and engineering.
However, modern deep neural networks' growing energy requirements now
increasingly limit their scaling and broader use. We propose a radical
alternative for implementing deep neural network models: Physical Neural
Networks. We introduce a hybrid physical-digital algorithm called Physics-Aware
Training to efficiently train sequences of controllable physical systems to act
as deep neural networks. This method automatically trains the functionality of
any sequence of real physical systems, directly, using backpropagation, the
same technique used for modern deep neural networks. To illustrate their
generality, we demonstrate physical neural networks with three diverse physical
systems-optical, mechanical, and electrical. Physical neural networks may
facilitate unconventional machine learning hardware that is orders of magnitude
faster and more energy efficient than conventional electronic processors.","Logan G. Wright, Tatsuhiro Onodera, Martin M. Stein, Tianyu Wang, Darren T. Schachter, Zoey Hu, Peter L. McMahon",2021,http://arxiv.org/abs/2104.13386v1
Consistency of Neural Networks with Regularization,"Neural networks have attracted a lot of attention due to its success in
applications such as natural language processing and computer vision. For large
scale data, due to the tremendous number of parameters in neural networks,
overfitting is an issue in training neural networks. To avoid overfitting, one
common approach is to penalize the parameters especially the weights in neural
networks. Although neural networks has demonstrated its advantages in many
applications, the theoretical foundation of penalized neural networks has not
been well-established. Our goal of this paper is to propose the general
framework of neural networks with regularization and prove its consistency.
Under certain conditions, the estimated neural network will converge to true
underlying function as the sample size increases. The method of sieves and the
theory on minimal neural networks are used to overcome the issue of
unidentifiability for the parameters. Two types of activation functions:
hyperbolic tangent function(Tanh) and rectified linear unit(ReLU) have been
taken into consideration. Simulations have been conducted to verify the
validation of theorem of consistency.","Xiaoxi Shen, Jinghang Lin",2022,http://arxiv.org/abs/2207.01538v1
"Understanding Weight Similarity of Neural Networks via Chain
  Normalization Rule and Hypothesis-Training-Testing","We present a weight similarity measure method that can quantify the weight
similarity of non-convex neural networks. To understand the weight similarity
of different trained models, we propose to extract the feature representation
from the weights of neural networks. We first normalize the weights of neural
networks by introducing a chain normalization rule, which is used for weight
representation learning and weight similarity measure. We extend the
traditional hypothesis-testing method to a hypothesis-training-testing
statistical inference method to validate the hypothesis on the weight
similarity of neural networks. With the chain normalization rule and the new
statistical inference, we study the weight similarity measure on Multi-Layer
Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural
Network (RNN), and find that the weights of an identical neural network
optimized with the Stochastic Gradient Descent (SGD) algorithm converge to a
similar local solution in a metric space. The weight similarity measure
provides more insight into the local solutions of neural networks. Experiments
on several datasets consistently validate the hypothesis of weight similarity
measure.","Guangcong Wang, Guangrun Wang, Wenqi Liang, Jianhuang Lai",2022,http://arxiv.org/abs/2208.04369v1
Graph Metanetworks for Processing Diverse Neural Architectures,"Neural networks efficiently encode learned information within their
parameters. Consequently, many tasks can be unified by treating neural networks
themselves as input data. When doing so, recent studies demonstrated the
importance of accounting for the symmetries and geometry of parameter spaces.
However, those works developed architectures tailored to specific networks such
as MLPs and CNNs without normalization layers, and generalizing such
architectures to other types of networks can be challenging. In this work, we
overcome these challenges by building new metanetworks - neural networks that
take weights from other neural networks as input. Put simply, we carefully
build graphs representing the input neural networks and process the graphs
using graph neural networks. Our approach, Graph Metanetworks (GMNs),
generalizes to neural architectures where competing methods struggle, such as
multi-head attention layers, normalization layers, convolutional layers, ResNet
blocks, and group-equivariant linear layers. We prove that GMNs are expressive
and equivariant to parameter permutation symmetries that leave the input neural
network functions unchanged. We validate the effectiveness of our method on
several metanetwork tasks over diverse neural network architectures.","Derek Lim, Haggai Maron, Marc T. Law, Jonathan Lorraine, James Lucas",2023,http://arxiv.org/abs/2312.04501v2
Deep Neural Networks for Pattern Recognition,"In the field of pattern recognition research, the method of using deep neural
networks based on improved computing hardware recently attracted attention
because of their superior accuracy compared to conventional methods. Deep
neural networks simulate the human visual system and achieve human equivalent
accuracy in image classification, object detection, and segmentation. This
chapter introduces the basic structure of deep neural networks that simulate
human neural networks. Then we identify the operational processes and
applications of conditional generative adversarial networks, which are being
actively researched based on the bottom-up and top-down mechanisms, the most
important functions of the human visual perception process. Finally, recent
developments in training strategies for effective learning of complex deep
neural networks are addressed.","Kyongsik Yun, Alexander Huyen, Thomas Lu",2018,http://arxiv.org/abs/1809.09645v1
"Evidence, Definitions and Algorithms regarding the Existence of
  Cohesive-Convergence Groups in Neural Network Optimization","Understanding the convergence process of neural networks is one of the most
complex and crucial issues in the field of machine learning. Despite the close
association of notable successes in this domain with the convergence of
artificial neural networks, this concept remains predominantly theoretical. In
reality, due to the non-convex nature of the optimization problems that
artificial neural networks tackle, very few trained networks actually achieve
convergence. To expand recent research efforts on artificial-neural-network
convergence, this paper will discuss a different approach based on observations
of cohesive-convergence groups emerging during the optimization process of an
artificial neural network.",Thien An L. Nguyen,2024,http://arxiv.org/abs/2403.05610v1
"Hybrid deep neural network based prediction method for unsteady flows
  with moving boundaries","A novel hybrid deep neural network architecture is designed to capture the
spatial-temporal features of unsteady flows around moving boundaries directly
from high-dimensional unsteady flow fields data. The hybrid deep neural network
is constituted by the convolutional neural network (CNN), improved
convolutional Long-Short Term Memory neural network (ConvLSTM) and
deconvolutional neural network (DeCNN). Flow fields at future time step can be
predicted through flow fields by previous time steps and boundary positions at
those steps by the novel hybrid deep neural network. Unsteady wake flows around
a forced oscillation cylinder with various amplitudes are calculated to
establish the datasets as training samples for training the hybrid deep neural
networks. The trained hybrid deep neural networks are then tested by predicting
the unsteady flow fields around a forced oscillation cylinder with new
amplitude. The effect of neural network structure parameters on prediction
accuracy was analyzed. The hybrid deep neural network, constituted by the best
parameter combination, is used to predict the flow fields in the future time.
The predicted flow fields are in good agreement with those calculated directly
by computational fluid dynamic solver, which means that this kind of deep
neural network can capture accurate spatial-temporal information from the
spatial-temporal series of unsteady flows around moving boundaries. The result
shows the potential capability of this kind novel hybrid deep neural network in
flow control for vibrating cylinder, where the fast calculation of
high-dimensional nonlinear unsteady flow around moving boundaries is needed.","Renkun Han, Zhong Zhang, Yixing Wang, Ziyang Liu, Yang Zhang, Gang Chen",2020,http://arxiv.org/abs/2006.00690v1
Feedforward Neural Networks for Caching: Enough or Too Much?,"We propose a caching policy that uses a feedforward neural network (FNN) to
predict content popularity. Our scheme outperforms popular eviction policies
like LRU or ARC, but also a new policy relying on the more complex recurrent
neural networks. At the same time, replacing the FNN predictor with a naive
linear estimator does not degrade caching performance significantly,
questioning then the role of neural networks for these applications.","Vladyslav Fedchenko, Giovanni Neglia, Bruno Ribeiro",2018,http://arxiv.org/abs/1810.06930v1
Neural SDE: Stabilizing Neural ODE Networks with Stochastic Noise,"Neural Ordinary Differential Equation (Neural ODE) has been proposed as a
continuous approximation to the ResNet architecture. Some commonly used
regularization mechanisms in discrete neural networks (e.g. dropout, Gaussian
noise) are missing in current Neural ODE networks. In this paper, we propose a
new continuous neural network framework called Neural Stochastic Differential
Equation (Neural SDE) network, which naturally incorporates various commonly
used regularization mechanisms based on random noise injection. Our framework
can model various types of noise injection frequently used in discrete networks
for regularization purpose, such as dropout and additive/multiplicative noise
in each block. We provide theoretical analysis explaining the improved
robustness of Neural SDE models against input perturbations/adversarial
attacks. Furthermore, we demonstrate that the Neural SDE network can achieve
better generalization than the Neural ODE and is more resistant to adversarial
and non-adversarial input perturbations.","Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, Cho-Jui Hsieh",2019,http://arxiv.org/abs/1906.02355v1
The Representation Theory of Neural Networks,"In this work, we show that neural networks can be represented via the
mathematical theory of quiver representations. More specifically, we prove that
a neural network is a quiver representation with activation functions, a
mathematical object that we represent using a network quiver. Also, we show
that network quivers gently adapt to common neural network concepts such as
fully-connected layers, convolution operations, residual connections, batch
normalization, pooling operations and even randomly wired neural networks. We
show that this mathematical representation is by no means an approximation of
what neural networks are as it exactly matches reality. This interpretation is
algebraic and can be studied with algebraic methods. We also provide a quiver
representation model to understand how a neural network creates representations
from the data. We show that a neural network saves the data as quiver
representations, and maps it to a geometrical space called the moduli space,
which is given in terms of the underlying oriented graph of the network, i.e.,
its quiver. This results as a consequence of our defined objects and of
understanding how the neural network computes a prediction in a combinatorial
and algebraic way. Overall, representing neural networks through the quiver
representation theory leads to 9 consequences and 4 inquiries for future
research that we believe are of great interest to better understand what neural
networks are and how they work.","Marco Antonio Armenta, Pierre-Marc Jodoin",2020,http://arxiv.org/abs/2007.12213v2
"Deep Kronecker neural networks: A general framework for neural networks
  with adaptive activation functions","We propose a new type of neural networks, Kronecker neural networks (KNNs),
that form a general framework for neural networks with adaptive activation
functions. KNNs employ the Kronecker product, which provides an efficient way
of constructing a very wide network while keeping the number of parameters low.
Our theoretical analysis reveals that under suitable conditions, KNNs induce a
faster decay of the loss than that by the feed-forward networks. This is also
empirically verified through a set of computational examples. Furthermore,
under certain technical assumptions, we establish global convergence of
gradient descent for KNNs. As a specific case, we propose the Rowdy activation
function that is designed to get rid of any saturation region by injecting
sinusoidal fluctuations, which include trainable parameters. The proposed Rowdy
activation function can be employed in any neural network architecture like
feed-forward neural networks, Recurrent neural networks, Convolutional neural
networks etc. The effectiveness of KNNs with Rowdy activation is demonstrated
through various computational experiments including function approximation
using feed-forward neural networks, solution inference of partial differential
equations using the physics-informed neural networks, and standard deep
learning benchmark problems using convolutional and fully-connected neural
networks.","Ameya D. Jagtap, Yeonjong Shin, Kenji Kawaguchi, George Em Karniadakis",2021,http://arxiv.org/abs/2105.09513v2
Can a powerful neural network be a teacher for a weaker neural network?,"The transfer learning technique is widely used to learning in one context and
applying it to another, i.e. the capacity to apply acquired knowledge and
skills to new situations. But is it possible to transfer the learning from a
deep neural network to a weaker neural network? Is it possible to improve the
performance of a weak neural network using the knowledge acquired by a more
powerful neural network? In this work, during the training process of a weak
network, we add a loss function that minimizes the distance between the
features previously learned from a strong neural network with the features that
the weak network must try to learn. To demonstrate the effectiveness and
robustness of our approach, we conducted a large number of experiments using
three known datasets and demonstrated that a weak neural network can increase
its performance if its learning process is driven by a more powerful neural
network.","Nicola Landro, Ignazio Gallo, Riccardo La Grassa",2020,http://arxiv.org/abs/2005.00393v2
"Message Passing Variational Autoregressive Network for Solving
  Intractable Ising Models","Many deep neural networks have been used to solve Ising models, including
autoregressive neural networks, convolutional neural networks, recurrent neural
networks, and graph neural networks. Learning a probability distribution of
energy configuration or finding the ground states of a disordered, fully
connected Ising model is essential for statistical mechanics and NP-hard
problems. Despite tremendous efforts, a neural network architecture with the
ability to high-accurately solve these fully connected and extremely
intractable problems on larger systems is still lacking. Here we propose a
variational autoregressive architecture with a message passing mechanism, which
can effectively utilize the interactions between spin variables. The new
network trained under an annealing framework outperforms existing methods in
solving several prototypical Ising spin Hamiltonians, especially for larger
spin systems at low temperatures. The advantages also come from the great
mitigation of mode collapse during the training process of deep neural
networks. Considering these extremely difficult problems to be solved, our
method extends the current computational limits of unsupervised neural networks
to solve combinatorial optimization problems.","Qunlong Ma, Zhi Ma, Jinlong Xu, Hairui Zhang, Ming Gao",2024,http://arxiv.org/abs/2404.06225v1
Dynamics of Deep Neural Networks and Neural Tangent Hierarchy,"The evolution of a deep neural network trained by the gradient descent can be
described by its neural tangent kernel (NTK) as introduced in [20], where it
was proven that in the infinite width limit the NTK converges to an explicit
limiting kernel and it stays constant during training. The NTK was also
implicit in some other recent papers [6,13,14]. In the overparametrization
regime, a fully-trained deep neural network is indeed equivalent to the kernel
regression predictor using the limiting NTK. And the gradient descent achieves
zero training loss for a deep overparameterized neural network. However, it was
observed in [5] that there is a performance gap between the kernel regression
using the limiting NTK and the deep neural networks. This performance gap is
likely to originate from the change of the NTK along training due to the finite
width effect. The change of the NTK along the training is central to describe
the generalization features of deep neural networks.
  In the current paper, we study the dynamic of the NTK for finite width deep
fully-connected neural networks. We derive an infinite hierarchy of ordinary
differential equations, the neural tangent hierarchy (NTH) which captures the
gradient descent dynamic of the deep neural network. Moreover, under certain
conditions on the neural network width and the data set dimension, we prove
that the truncated hierarchy of NTH approximates the dynamic of the NTK up to
arbitrary precision. This description makes it possible to directly study the
change of the NTK for deep neural networks, and sheds light on the observation
that deep neural networks outperform kernel regressions using the corresponding
limiting NTK.","Jiaoyang Huang, Horng-Tzer Yau",2019,http://arxiv.org/abs/1909.08156v1
"Novel Kernel Models and Exact Representor Theory for Neural Networks
  Beyond the Over-Parameterized Regime","This paper presents two models of neural-networks and their training
applicable to neural networks of arbitrary width, depth and topology, assuming
only finite-energy neural activations; and a novel representor theory for
neural networks in terms of a matrix-valued kernel. The first model is exact
(un-approximated) and global, casting the neural network as an elements in a
reproducing kernel Banach space (RKBS); we use this model to provide tight
bounds on Rademacher complexity. The second model is exact and local, casting
the change in neural network function resulting from a bounded change in
weights and biases (ie. a training step) in reproducing kernel Hilbert space
(RKHS) in terms of a local-intrinsic neural kernel (LiNK). This local model
provides insight into model adaptation through tight bounds on Rademacher
complexity of network adaptation. We also prove that the neural tangent kernel
(NTK) is a first-order approximation of the LiNK kernel. Finally, and noting
that the LiNK does not provide a representor theory for technical reasons, we
present an exact novel representor theory for layer-wise neural network
training with unregularized gradient descent in terms of a local-extrinsic
neural kernel (LeNK). This representor theory gives insight into the role of
higher-order statistics in neural network training and the effect of kernel
evolution in neural-network kernel models. Throughout the paper (a) feedforward
ReLU networks and (b) residual networks (ResNet) are used as illustrative
examples.","Alistair Shilton, Sunil Gupta, Santu Rana, Svetha Venkatesh",2024,http://arxiv.org/abs/2405.15254v1
Descriptive complexity for neural networks via Boolean networks,"We investigate the descriptive complexity of a class of neural networks with
unrestricted topologies and piecewise polynomial activation functions. We
consider the general scenario where the networks run for an unlimited number of
rounds and floating-point numbers are used to simulate reals. We characterize
these neural networks with a recursive rule-based logic for Boolean networks.
In particular, we show that the sizes of the neural networks and the
corresponding Boolean rule formulae are polynomially related. In fact, in the
direction from Boolean rules to neural networks, the blow-up is only linear.
Our translations result in a time delay, which is the number of rounds that it
takes for an object's translation to simulate a single round of the object. In
the translation from neural networks to Boolean rules, the time delay of the
resulting formula is polylogarithmic in the neural network size. In the
converse translation, the time delay of the neural network is linear in the
formula size. As a corollary, by restricting our logic, we obtain a similar
characterization for classical feedforward neural networks. We also obtain
translations between the rule-based logic for Boolean networks, the
diamond-free fragment of modal substitution calculus and a class of recursive
Boolean circuits where the number of input and output gates match. Ultimately,
our translations offer a method of translating a given neural network into an
equivalent neural network with different activation functions, including linear
activation functions!","Veeti Ahvonen, Damian Heiman, Antti Kuusisto",2023,http://arxiv.org/abs/2308.06277v3
"Neural Nets via Forward State Transformation and Backward Loss
  Transformation","This article studies (multilayer perceptron) neural networks with an emphasis
on the transformations involved --- both forward and backward --- in order to
develop a semantical/logical perspective that is in line with standard program
semantics. The common two-pass neural network training algorithms make this
viewpoint particularly fitting. In the forward direction, neural networks act
as state transformers. In the reverse direction, however, neural networks
change losses of outputs to losses of inputs, thereby acting like a
(real-valued) predicate transformer. In this way, backpropagation is functorial
by construction, as shown earlier in recent other work. We illustrate this
perspective by training a simple instance of a neural network.","Bart Jacobs, David Sprunger",2018,http://arxiv.org/abs/1803.09356v1
A Primer on Neural Network Models for Natural Language Processing,"Over the past few years, neural networks have re-emerged as powerful
machine-learning models, yielding state-of-the-art results in fields such as
image recognition and speech processing. More recently, neural network models
started to be applied also to textual natural language signals, again with very
promising results. This tutorial surveys neural network models from the
perspective of natural language processing research, in an attempt to bring
natural-language researchers up to speed with the neural techniques. The
tutorial covers input encoding for natural language tasks, feed-forward
networks, convolutional networks, recurrent networks and recursive networks, as
well as the computation graph abstraction for automatic gradient computation.",Yoav Goldberg,2015,http://arxiv.org/abs/1510.00726v1
Power Law in Sparsified Deep Neural Networks,"The power law has been observed in the degree distributions of many
biological neural networks. Sparse deep neural networks, which learn an
economical representation from the data, resemble biological neural networks in
many ways. In this paper, we study if these artificial networks also exhibit
properties of the power law. Experimental results on two popular deep learning
models, namely, multilayer perceptrons and convolutional neural networks, are
affirmative. The power law is also naturally related to preferential
attachment. To study the dynamical properties of deep networks in continual
learning, we propose an internal preferential attachment model to explain how
the network topology evolves. Experimental results show that with the arrival
of a new task, the new connections made follow this preferential attachment
process.","Lu Hou, James T. Kwok",2018,http://arxiv.org/abs/1805.01891v1
On the Relative Expressiveness of Bayesian and Neural Networks,"A neural network computes a function. A central property of neural networks
is that they are ""universal approximators:"" for a given continuous function,
there exists a neural network that can approximate it arbitrarily well, given
enough neurons (and some additional assumptions). In contrast, a Bayesian
network is a model, but each of its queries can be viewed as computing a
function. In this paper, we identify some key distinctions between the
functions computed by neural networks and those by marginal Bayesian network
queries, showing that the former are more expressive than the latter. Moreover,
we propose a simple augmentation to Bayesian networks (a testing operator),
which enables their marginal queries to become ""universal approximators.""","Arthur Choi, Ruocheng Wang, Adnan Darwiche",2018,http://arxiv.org/abs/1812.08957v1
Kernel-based Translations of Convolutional Networks,"Convolutional Neural Networks, as most artificial neural networks, are
commonly viewed as methods different in essence from kernel-based methods. We
provide a systematic translation of Convolutional Neural Networks (ConvNets)
into their kernel-based counterparts, Convolutional Kernel Networks (CKNs), and
demonstrate that this perception is unfounded both formally and empirically. We
show that, given a Convolutional Neural Network, we can design a corresponding
Convolutional Kernel Network, easily trainable using a new stochastic gradient
algorithm based on an accurate gradient computation, that performs on par with
its Convolutional Neural Network counterpart. We present experimental results
supporting our claims on landmark ConvNet architectures comparing each ConvNet
to its CKN counterpart over several parameter settings.","Corinne Jones, Vincent Roulet, Zaid Harchaoui",2019,http://arxiv.org/abs/1903.08131v1
A Survey on Graph Classification and Link Prediction based on GNN,"Traditional convolutional neural networks are limited to handling Euclidean
space data, overlooking the vast realm of real-life scenarios represented as
graph data, including transportation networks, social networks, and reference
networks. The pivotal step in transferring convolutional neural networks to
graph data analysis and processing lies in the construction of graph
convolutional operators and graph pooling operators. This comprehensive review
article delves into the world of graph convolutional neural networks. Firstly,
it elaborates on the fundamentals of graph convolutional neural networks.
Subsequently, it elucidates the graph neural network models based on attention
mechanisms and autoencoders, summarizing their application in node
classification, graph classification, and link prediction along with the
associated datasets.","Xingyu Liu, Juan Chen, Quan Wen",2023,http://arxiv.org/abs/2307.00865v1
Context-adaptive neural network based prediction for image compression,"This paper describes a set of neural network architectures, called Prediction
Neural Networks Set (PNNS), based on both fully-connected and convolutional
neural networks, for intra image prediction. The choice of neural network for
predicting a given image block depends on the block size, hence does not need
to be signalled to the decoder. It is shown that, while fully-connected neural
networks give good performance for small block sizes, convolutional neural
networks provide better predictions in large blocks with complex textures.
Thanks to the use of masks of random sizes during training, the neural networks
of PNNS well adapt to the available context that may vary, depending on the
position of the image block to be predicted. When integrating PNNS into a H.265
codec, PSNR-rate performance gains going from 1.46% to 5.20% are obtained.
These gains are on average 0.99% larger than those of prior neural network
based methods. Unlike the H.265 intra prediction modes, which are each
specialized in predicting a specific texture, the proposed PNNS can model a
large set of complex textures.","Thierry Dumas, Aline Roumy, Christine Guillemot",2018,http://arxiv.org/abs/1807.06244v2
"Evolving Self-taught Neural Networks: The Baldwin Effect and the
  Emergence of Intelligence","The so-called Baldwin Effect generally says how learning, as a form of
ontogenetic adaptation, can influence the process of phylogenetic adaptation,
or evolution. This idea has also been taken into computation in which evolution
and learning are used as computational metaphors, including evolving neural
networks. This paper presents a technique called evolving self-taught neural
networks - neural networks that can teach themselves without external
supervision or reward. The self-taught neural network is intrinsically
motivated. Moreover, the self-taught neural network is the product of the
interplay between evolution and learning. We simulate a multi-agent system in
which neural networks are used to control autonomous agents. These agents have
to forage for resources and compete for their own survival. Experimental
results show that the interaction between evolution and the ability to teach
oneself in self-taught neural networks outperform evolution and self-teaching
alone. More specifically, the emergence of an intelligent foraging strategy is
also demonstrated through that interaction. Indications for future work on
evolving neural networks are also presented.",Nam Le,2019,http://arxiv.org/abs/1906.08854v1
"Stable Learning Using Spiking Neural Networks Equipped With Affine
  Encoders and Decoders","We study the learning problem associated with spiking neural networks.
Specifically, we focus on spiking neural networks composed of simple spiking
neurons having only positive synaptic weights, equipped with an affine encoder
and decoder. These neural networks are shown to depend continuously on their
parameters, which facilitates classical covering number-based generalization
statements and supports stable gradient-based training. We demonstrate that the
positivity of the weights continues to enable a wide range of expressivity
results, including rate-optimal approximation of smooth functions and
dimension-independent approximation of Barron regular functions. In particular,
we show in theory and simulations that affine spiking neural networks are
capable of approximating shallow ReLU neural networks. Furthermore, we apply
these neural networks to standard machine learning benchmarks, reaching
competitive results. Finally, and remarkably, we observe that from a
generalization perspective, contrary to feedforward neural networks or previous
results for general spiking neural networks, the depth has little to no adverse
effect on the generalization capabilities.","A. Martina Neuman, Dominik Dold, Philipp Christian Petersen",2024,http://arxiv.org/abs/2404.04549v2
Simultaneous Weight and Architecture Optimization for Neural Networks,"Neural networks are trained by choosing an architecture and training the
parameters. The choice of architecture is often by trial and error or with
Neural Architecture Search (NAS) methods. While NAS provides some automation,
it often relies on discrete steps that optimize the architecture and then train
the parameters. We introduce a novel neural network training framework that
fundamentally transforms the process by learning architecture and parameters
simultaneously with gradient descent. With the appropriate setting of the loss
function, it can discover sparse and compact neural networks for given
datasets. Central to our approach is a multi-scale encoder-decoder, in which
the encoder embeds pairs of neural networks with similar functionalities close
to each other (irrespective of their architectures and weights). To train a
neural network with a given dataset, we randomly sample a neural network
embedding in the embedding space and then perform gradient descent using our
custom loss function, which incorporates a sparsity penalty to encourage
compactness. The decoder generates a neural network corresponding to the
embedding. Experiments demonstrate that our framework can discover sparse and
compact neural networks maintaining a high performance.","Zitong Huang, Mansooreh Montazerin, Ajitesh Srivastava",2024,http://arxiv.org/abs/2410.08339v1
"Exploring the Imposition of Synaptic Precision Restrictions For
  Evolutionary Synthesis of Deep Neural Networks","A key contributing factor to incredible success of deep neural networks has
been the significant rise on massively parallel computing devices allowing
researchers to greatly increase the size and depth of deep neural networks,
leading to significant improvements in modeling accuracy. Although deeper,
larger, or complex deep neural networks have shown considerable promise, the
computational complexity of such networks is a major barrier to utilization in
resource-starved scenarios. We explore the synaptogenesis of deep neural
networks in the formation of efficient deep neural network architectures within
an evolutionary deep intelligence framework, where a probabilistic generative
modeling strategy is introduced to stochastically synthesize increasingly
efficient yet effective offspring deep neural networks over generations,
mimicking evolutionary processes such as heredity, random mutation, and natural
selection in a probabilistic manner. In this study, we primarily explore the
imposition of synaptic precision restrictions and its impact on the
evolutionary synthesis of deep neural networks to synthesize more efficient
network architectures tailored for resource-starved scenarios. Experimental
results show significant improvements in synaptic efficiency (~10X decrease for
GoogLeNet-based DetectNet) and inference speed (>5X increase for
GoogLeNet-based DetectNet) while preserving modeling accuracy.","Mohammad Javad Shafiee, Francis Li, Alexander Wong",2017,http://arxiv.org/abs/1707.00095v1
"Scalable Training of Artificial Neural Networks with Adaptive Sparse
  Connectivity inspired by Network Science","Through the success of deep learning in various domains, artificial neural
networks are currently among the most used artificial intelligence methods.
Taking inspiration from the network properties of biological neural networks
(e.g. sparsity, scale-freeness), we argue that (contrary to general practice)
artificial neural networks, too, should not have fully-connected layers. Here
we propose sparse evolutionary training of artificial neural networks, an
algorithm which evolves an initial sparse topology (Erd\H{o}s-R\'enyi random
graph) of two consecutive layers of neurons into a scale-free topology, during
learning. Our method replaces artificial neural networks fully-connected layers
with sparse ones before training, reducing quadratically the number of
parameters, with no decrease in accuracy. We demonstrate our claims on
restricted Boltzmann machines, multi-layer perceptrons, and convolutional
neural networks for unsupervised and supervised learning on 15 datasets. Our
approach has the potential to enable artificial neural networks to scale up
beyond what is currently possible.","Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H. Nguyen, Madeleine Gibescu, Antonio Liotta",2017,http://arxiv.org/abs/1707.04780v2
Mean Field Analysis of Neural Networks: A Law of Large Numbers,"Machine learning, and in particular neural network models, have
revolutionized fields such as image, text, and speech recognition. Today, many
important real-world applications in these areas are driven by neural networks.
There are also growing applications in engineering, robotics, medicine, and
finance. Despite their immense success in practice, there is limited
mathematical understanding of neural networks. This paper illustrates how
neural networks can be studied via stochastic analysis, and develops approaches
for addressing some of the technical challenges which arise. We analyze
one-layer neural networks in the asymptotic regime of simultaneously (A) large
network sizes and (B) large numbers of stochastic gradient descent training
iterations. We rigorously prove that the empirical distribution of the neural
network parameters converges to the solution of a nonlinear partial
differential equation. This result can be considered a law of large numbers for
neural networks. In addition, a consequence of our analysis is that the trained
parameters of the neural network asymptotically become independent, a property
which is commonly called ""propagation of chaos"".","Justin Sirignano, Konstantinos Spiliopoulos",2018,http://arxiv.org/abs/1805.01053v4
Deep Convolutional Spiking Neural Networks for Image Classification,"Spiking neural networks are biologically plausible counterparts of the
artificial neural networks, artificial neural networks are usually trained with
stochastic gradient descent and spiking neural networks are trained with spike
timing dependant plasticity. Training deep convolutional neural networks is a
memory and power intensive job. Spiking networks could potentially help in
reducing the power usage. There is a large pool of tools for one to chose to
train artificial neural networks of any size, on the other hand all the
available tools to simulate spiking neural networks are geared towards
computational neuroscience applications and they are not suitable for real life
applications. In this work we focus on implementing a spiking CNN using
Tensorflow to examine behaviour of the network and empirically study the effect
of various parameters on learning capabilities and also study catastrophic
forgetting in the spiking CNN and weight initialization problem in R-STDP using
MNIST and N-MNIST data sets.","Ruthvik Vaila, John Chiasson, Vishal Saxena",2019,http://arxiv.org/abs/1903.12272v2
"Univariate ReLU neural network and its application in nonlinear system
  identification","ReLU (rectified linear units) neural network has received significant
attention since its emergence. In this paper, a univariate ReLU (UReLU) neural
network is proposed to both modelling the nonlinear dynamic system and
revealing insights about the system. Specifically, the neural network consists
of neurons with linear and UReLU activation functions, and the UReLU functions
are defined as the ReLU functions respect to each dimension. The UReLU neural
network is a single hidden layer neural network, and the structure is
relatively simple. The initialization of the neural network employs the
decoupling method, which provides a good initialization and some insight into
the nonlinear system. Compared with normal ReLU neural network, the number of
parameters of UReLU network is less, but it still provide a good approximation
of the nonlinear dynamic system. The performance of the UReLU neural network is
shown through a Hysteretic benchmark system: the Bouc-Wen system. Simulation
results verify the effectiveness of the proposed method.","Xinglong Liang, Jun Xu",2020,http://arxiv.org/abs/2003.02666v1
Making Neural Networks FAIR,"Research on neural networks has gained significant momentum over the past few
years. Because training is a resource-intensive process and training data
cannot always be made available to everyone, there has been a trend to reuse
pre-trained neural networks. As such, neural networks themselves have become
research data. In this paper, we first present the neural network ontology
FAIRnets Ontology, an ontology to make existing neural network models findable,
accessible, interoperable, and reusable according to the FAIR principles. Our
ontology allows us to model neural networks on a meta-level in a structured
way, including the representation of all network layers and their
characteristics. Secondly, we have modeled over 18,400 neural networks from
GitHub based on this ontology, which we provide to the public as a knowledge
graph called FAIRnets, ready to be used for recommending suitable neural
networks to data scientists.","Anna Nguyen, Tobias Weller, Michael Färber, York Sure-Vetter",2019,http://arxiv.org/abs/1907.11569v4
An SMT-Based Approach for Verifying Binarized Neural Networks,"Deep learning has emerged as an effective approach for creating modern
software systems, with neural networks often surpassing hand-crafted systems.
Unfortunately, neural networks are known to suffer from various safety and
security issues. Formal verification is a promising avenue for tackling this
difficulty, by formally certifying that networks are correct. We propose an
SMT-based technique for verifying Binarized Neural Networks - a popular kind of
neural network, where some weights have been binarized in order to render the
neural network more memory and energy efficient, and quicker to evaluate. One
novelty of our technique is that it allows the verification of neural networks
that include both binarized and non-binarized components. Neural network
verification is computationally very difficult, and so we propose here various
optimizations, integrated into our SMT procedure as deduction steps, as well as
an approach for parallelizing verification queries. We implement our technique
as an extension to the Marabou framework, and use it to evaluate the approach
on popular binarized neural network architectures.","Guy Amir, Haoze Wu, Clark Barrett, Guy Katz",2020,http://arxiv.org/abs/2011.02948v2
Feedforward Sequential Memory Neural Networks without Recurrent Feedback,"We introduce a new structure for memory neural networks, called feedforward
sequential memory networks (FSMN), which can learn long-term dependency without
using recurrent feedback. The proposed FSMN is a standard feedforward neural
networks equipped with learnable sequential memory blocks in the hidden layers.
In this work, we have applied FSMN to several language modeling (LM) tasks.
Experimental results have shown that the memory blocks in FSMN can learn
effective representations of long history. Experiments have shown that FSMN
based language models can significantly outperform not only feedforward neural
network (FNN) based LMs but also the popular recurrent neural network (RNN)
LMs.","ShiLiang Zhang, Hui Jiang, Si Wei, LiRong Dai",2015,http://arxiv.org/abs/1510.02693v1
Flow of Information in Feed-Forward Deep Neural Networks,"Feed-forward deep neural networks have been used extensively in various
machine learning applications. Developing a precise understanding of the
underling behavior of neural networks is crucial for their efficient
deployment. In this paper, we use an information theoretic approach to study
the flow of information in a neural network and to determine how entropy of
information changes between consecutive layers. Moreover, using the Information
Bottleneck principle, we develop a constrained optimization problem that can be
used in the training process of a deep neural network. Furthermore, we
determine a lower bound for the level of data representation that can be
achieved in a deep neural network with an acceptable level of distortion.","Pejman Khadivi, Ravi Tandon, Naren Ramakrishnan",2016,http://arxiv.org/abs/1603.06220v1
"Fixed-point optimization of deep neural networks with adaptive step size
  retraining","Fixed-point optimization of deep neural networks plays an important role in
hardware based design and low-power implementations. Many deep neural networks
show fairly good performance even with 2- or 3-bit precision when quantized
weights are fine-tuned by retraining. We propose an improved fixedpoint
optimization algorithm that estimates the quantization step size dynamically
during the retraining. In addition, a gradual quantization scheme is also
tested, which sequentially applies fixed-point optimizations from high- to
low-precision. The experiments are conducted for feed-forward deep neural
networks (FFDNNs), convolutional neural networks (CNNs), and recurrent neural
networks (RNNs).","Sungho Shin, Yoonho Boo, Wonyong Sung",2017,http://arxiv.org/abs/1702.08171v1
Ablation of a Robot's Brain: Neural Networks Under a Knife,"It is still not fully understood exactly how neural networks are able to
solve the complex tasks that have recently pushed AI research forward. We
present a novel method for determining how information is structured inside a
neural network. Using ablation (a neuroscience technique for cutting away parts
of a brain to determine their function), we approach several neural network
architectures from a biological perspective. Through an analysis of this
method's results, we examine important similarities between biological and
artificial neural networks to search for the implicit knowledge locked away in
the network's weights.","Peter E. Lillian, Richard Meyes, Tobias Meisen",2018,http://arxiv.org/abs/1812.05687v2
Fourier Neural Networks: A Comparative Study,"We review neural network architectures which were motivated by Fourier series
and integrals and which are referred to as Fourier neural networks. These
networks are empirically evaluated in synthetic and real-world tasks. Neither
of them outperforms the standard neural network with sigmoid activation
function in the real-world tasks. All neural networks, both Fourier and the
standard one, empirically demonstrate lower approximation error than the
truncated Fourier series when it comes to an approximation of a known function
of multiple variables.","Abylay Zhumekenov, Malika Uteuliyeva, Olzhas Kabdolov, Rustem Takhanov, Zhenisbek Assylbekov, Alejandro J. Castro",2019,http://arxiv.org/abs/1902.03011v1
"Statistical Tests and Confidential Intervals as Thresholds for Quantum
  Neural Networks","Some basic quantum neural networks were analyzed and constructed in the
recent work of the author \cite{dndiep3}, published in International Journal of
Theoretical Physics (2020). In particular the Least Quare Problem (LSP) and the
Linear Regression Problem (LRP) was discussed. In this second paper we continue
to analyze and construct the least square quantum neural network (LS-QNN), the
polynomial interpolation quantum neural network (PI-QNN), the polynomial
regression quantum neural network (PR-QNN) and chi-squared quantum neural
network ($\chi^2$-QNN). We use the corresponding solution or tests as the
threshold for the corresponding training rules.",Do Ngoc Diep,2020,http://arxiv.org/abs/2001.11844v1
Power Series Expansion Neural Network,"In this paper, we develop a new neural network family based on power series
expansion, which is proved to achieve a better approximation accuracy in
comparison with existing neural networks. This new set of neural networks
embeds the power series expansion (PSE) into the neural network structure. Then
it can improve the representation ability while preserving comparable
computational cost by increasing the degree of PSE instead of increasing the
depth or width. Both theoretical approximation and numerical results show the
advantages of this new neural network.","Qipin Chen, Wenrui Hao, Juncai He",2021,http://arxiv.org/abs/2102.13221v2
Symbiotic Hybrid Neural Network Watchdog For Outlier Detection,"Neural networks are largely black boxes. A neural network trained to classify
fruit may classify a picture of a giraffe as a banana. A neural network
watchdog's job is to identify such inputs, allowing a classifier to disregard
such data. We investigate whether the watchdog should be separate from the
neural network or symbiotically attached. We present empirical evidence that
the symbiotic watchdog performs better than when the neural networks are
disjoint.","Justin Bui, Robert J. Marks II",2021,http://arxiv.org/abs/2103.00582v2
"Neural Networks, Hypersurfaces, and Radon Transforms","Connections between integration along hypersufaces, Radon transforms, and
neural networks are exploited to highlight an integral geometric mathematical
interpretation of neural networks. By analyzing the properties of neural
networks as operators on probability distributions for observed data, we show
that the distribution of outputs for any node in a neural network can be
interpreted as a nonlinear projection along hypersurfaces defined by level
surfaces over the input data space. We utilize these descriptions to provide
new interpretation for phenomena such as nonlinearity, pooling, activation
functions, and adversarial examples in neural network-based learning problems.","Soheil Kolouri, Xuwang Yin, Gustavo K. Rohde",2019,http://arxiv.org/abs/1907.02220v1
Stock Price Prediction using Dynamic Neural Networks,"This paper will analyze and implement a time series dynamic neural network to
predict daily closing stock prices. Neural networks possess unsurpassed
abilities in identifying underlying patterns in chaotic, non-linear, and
seemingly random data, thus providing a mechanism to predict stock price
movements much more precisely than many current techniques. Contemporary
methods for stock analysis, including fundamental, technical, and regression
techniques, are conversed and paralleled with the performance of neural
networks. Also, the Efficient Market Hypothesis (EMH) is presented and
contrasted with Chaos theory using neural networks. This paper will refute the
EMH and support Chaos theory. Finally, recommendations for using neural
networks in stock price prediction will be presented.",David Noel,2023,http://arxiv.org/abs/2306.12969v1
Equivariant neural networks and piecewise linear representation theory,"Equivariant neural networks are neural networks with symmetry. Motivated by
the theory of group representations, we decompose the layers of an equivariant
neural network into simple representations. The nonlinear activation functions
lead to interesting nonlinear equivariant maps between simple representations.
For example, the rectified linear unit (ReLU) gives rise to piecewise linear
maps. We show that these considerations lead to a filtration of equivariant
neural networks, generalizing Fourier series. This observation might provide a
useful tool for interpreting equivariant neural networks.","Joel Gibson, Daniel Tubbenhauer, Geordie Williamson",2024,http://arxiv.org/abs/2408.00949v2
"An Efficient Method for Solving Lane Emden Equation using Legendre
  Neural Network","The aim of this manuscript is to address non-linear differential equations of
the Lane Emden equation of second order using the shifted Legendre neural
network (SLNN) method. Here all the equations are classified as singular
initial value problems. To manage the singularity challenge, we employ an
artificial neural network method. The approach utilizes a neural network of a
single layer, where the hidden layer is omitted by enlarge the input using
shifted Legendre polynomials. We apply a feed forward neural network model
along with the principle of error back propagation. The effectiveness of the
Legendre Neural Network model is demonstrated through LaneEmden equations.","Vijay Kumar Patel, Vivek Sharma, Nitin Kumar, Anoop Tiwari",2024,http://arxiv.org/abs/2410.05409v1
Interpreting Neural Networks through Mahalanobis Distance,"This paper introduces a theoretical framework that connects neural network
linear layers with the Mahalanobis distance, offering a new perspective on
neural network interpretability. While previous studies have explored
activation functions primarily for performance optimization, our work
interprets these functions through statistical distance measures, a less
explored area in neural network research. By establishing this connection, we
provide a foundation for developing more interpretable neural network models,
which is crucial for applications requiring transparency. Although this work is
theoretical and does not include empirical data, the proposed distance-based
interpretation has the potential to enhance model robustness, improve
generalization, and provide more intuitive explanations of neural network
decisions.",Alan Oursland,2024,http://arxiv.org/abs/2410.19352v1
Neural Networks Perform Sufficient Dimension Reduction,"This paper investigates the connection between neural networks and sufficient
dimension reduction (SDR), demonstrating that neural networks inherently
perform SDR in regression tasks under appropriate rank regularizations.
Specifically, the weights in the first layer span the central mean subspace. We
establish the statistical consistency of the neural network-based estimator for
the central mean subspace, underscoring the suitability of neural networks in
addressing SDR-related challenges. Numerical experiments further validate our
theoretical findings, and highlight the underlying capability of neural
networks to facilitate SDR compared to the existing methods. Additionally, we
discuss an extension to unravel the central subspace, broadening the scope of
our investigation.","Shuntuo Xu, Zhou Yu",2024,http://arxiv.org/abs/2412.19033v1
"A theoretical analysis on the resolution of parametric PDEs via neural
  networks designed with Strassen algorithm","We construct a Neural Network that approximates the matrix multiplication
operator for any activation functions for which there exist a Neural Network
which can approximate the scalar multiplication function. In particular, we use
the Strassen algorithm for reducing the number of weights and layers needed for
such Neural Network. This allows us to define another Neural Network for
approximating the inverse matrix operator. Also, by relying on the Galerkin
method, we apply those Neural Networks for resolving parametric elliptic PDEs
for a whole set of parameters at the same time. Finally, we discuss the
improvements with respect to prior results.","Gonzalo Romera, Jon Asier Bárcena-Petisco",2025,http://arxiv.org/abs/2501.06539v1
"Optimization of the Woodcock Particle Tracking Method Using Neural
  Network","The acceptance rate in Woodcock tracking algorithm is generalized to an
arbitrary position-dependent variable $q(x)$. A neural network is used to
optimize $q(x)$, and the FOM value is used as the loss function. This idea
comes from physics informed neural network(PINN), where a neural network is
used to represent the solution of differential equations. Here the neural
network $q(x)$ should solve the functional equations that optimize FOM. For a
1d transmission problem with Gaussian absorption cross section, we observe a
significant improvement of the FOM value compared to the constant $q$ case and
the original Woodcock method. Generalizations of the neural network
Woodcock(NNW) method to 3d voxel models are waiting to be explored.",Bingnan Zhang,2025,http://arxiv.org/abs/2502.13620v1
Quantum Codes from Neural Networks,"We examine the usefulness of applying neural networks as a variational state
ansatz for many-body quantum systems in the context of quantum
information-processing tasks. In the neural network state ansatz, the complex
amplitude function of a quantum state is computed by a neural network. The
resulting multipartite entanglement structure captured by this ansatz has
proven rich enough to describe the ground states and unitary dynamics of
various physical systems of interest. In the present paper, we initiate the
study of neural network states in quantum information-processing tasks. We
demonstrate that neural network states are capable of efficiently representing
quantum codes for quantum information transmission and quantum error
correction, supplying further evidence for the usefulness of neural network
states to describe multipartite entanglement. In particular, we show the
following main results: a) Neural network states yield quantum codes with a
high coherent information for two important quantum channels, the generalized
amplitude damping channel and the dephrasure channel. These codes outperform
all other known codes for these channels, and cannot be found using a direct
parametrization of the quantum state. b) For the depolarizing channel, the
neural network state ansatz reliably finds the best known codes given by
repetition codes. c) Neural network states can be used to represent absolutely
maximally entangled states, a special type of quantum error-correcting codes.
In all three cases, the neural network state ansatz provides an efficient and
versatile means as a variational parametrization of these highly entangled
states.","Johannes Bausch, Felix Leditzky",2018,http://arxiv.org/abs/1806.08781v2
"Are Deep Neural Networks ""Robust""?","Separating outliers from inliers is the definition of robustness in computer
vision. This essay delineates how deep neural networks are different than
typical robust estimators. Deep neural networks not robust by this traditional
definition.",Peter Meer,2020,http://arxiv.org/abs/2008.12650v1
"On the validity of kernel approximations for orthogonally-initialized
  neural networks","In this note we extend kernel function approximation results for neural
networks with Gaussian-distributed weights to single-layer networks initialized
using Haar-distributed random orthogonal matrices (with possible rescaling).
This is accomplished using recent results from random matrix theory.",James Martens,2021,http://arxiv.org/abs/2104.05878v1
Extending Answer Set Programs with Neural Networks,"The integration of low-level perception with high-level reasoning is one of
the oldest problems in Artificial Intelligence. Recently, several proposals
were made to implement the reasoning process in complex neural network
architectures. While these works aim at extending neural networks with the
capability of reasoning, a natural question that we consider is: can we extend
answer set programs with neural networks to allow complex and high-level
reasoning on neural network outputs? As a preliminary result, we propose
NeurASP -- a simple extension of answer set programs by embracing neural
networks where neural network outputs are treated as probability distributions
over atomic facts in answer set programs. We show that NeurASP can not only
improve the perception accuracy of a pre-trained neural network, but also help
to train a neural network better by giving restrictions through logic rules.
However, training with NeurASP would take much more time than pure neural
network training due to the internal use of a symbolic reasoning engine. For
future work, we plan to investigate the potential ways to solve the scalability
issue of NeurASP. One potential way is to embed logic programs directly in
neural networks. On this route, we plan to first design a SAT solver using
neural networks, then extend such a solver to allow logic programs.",Zhun Yang,2020,http://arxiv.org/abs/2009.10256v1
Optimizing over an ensemble of neural networks,"We study optimization problems where the objective function is modeled
through feedforward neural networks with rectified linear unit (ReLU)
activation. Recent literature has explored the use of a single neural network
to model either uncertain or complex elements within an objective function.
However, it is well known that ensembles of neural networks produce more stable
predictions and have better generalizability than models with single neural
networks, which motivates the investigation of ensembles of neural networks
rather than single neural networks in decision-making pipelines. We study how
to incorporate a neural network ensemble as the objective function of an
optimization model and explore computational approaches for the ensuing
problem. We present a mixed-integer linear program based on existing popular
big-M formulations for optimizing over a single neural network. We develop a
two-phase approach for our model that combines preprocessing procedures to
tighten bounds for critical neurons in the neural networks with a Lagrangian
relaxation-based branch-and-bound approach. Experimental evaluations of our
solution methods suggest that using ensembles of neural networks yields more
stable and higher quality solutions, compared to single neural networks, and
that our optimization algorithm outperforms (the adaption of) a
state-of-the-art approach in terms of computational time and optimality gaps.","Keliang Wang, Leonardo Lozano, Carlos Cardonha, David Bergman",2021,http://arxiv.org/abs/2112.07007v2
"Quantifying the generalization error in deep learning in terms of data
  distribution and neural network smoothness","The accuracy of deep learning, i.e., deep neural networks, can be
characterized by dividing the total error into three main types: approximation
error, optimization error, and generalization error. Whereas there are some
satisfactory answers to the problems of approximation and optimization, much
less is known about the theory of generalization. Most existing theoretical
works for generalization fail to explain the performance of neural networks in
practice. To derive a meaningful bound, we study the generalization error of
neural networks for classification problems in terms of data distribution and
neural network smoothness. We introduce the cover complexity (CC) to measure
the difficulty of learning a data set and the inverse of the modulus of
continuity to quantify neural network smoothness. A quantitative bound for
expected accuracy/error is derived by considering both the CC and neural
network smoothness. Although most of the analysis is general and not specific
to neural networks, we validate our theoretical assumptions and results
numerically for neural networks by several data sets of images. The numerical
results confirm that the expected error of trained networks scaled with the
square root of the number of classes has a linear relationship with respect to
the CC. We also observe a clear consistency between test loss and neural
network smoothness during the training process. In addition, we demonstrate
empirically that the neural network smoothness decreases when the network size
increases whereas the smoothness is insensitive to training dataset size.","Pengzhan Jin, Lu Lu, Yifa Tang, George Em Karniadakis",2019,http://arxiv.org/abs/1905.11427v3
"DizzyRNN: Reparameterizing Recurrent Neural Networks for Norm-Preserving
  Backpropagation","The vanishing and exploding gradient problems are well-studied obstacles that
make it difficult for recurrent neural networks to learn long-term time
dependencies. We propose a reparameterization of standard recurrent neural
networks to update linear transformations in a provably norm-preserving way
through Givens rotations. Additionally, we use the absolute value function as
an element-wise non-linearity to preserve the norm of backpropagated signals
over the entire network. We show that this reparameterization reduces the
number of parameters and maintains the same algorithmic complexity as a
standard recurrent neural network, while outperforming standard recurrent
neural networks with orthogonal initializations and Long Short-Term Memory
networks on the copy problem.","Victor Dorobantu, Per Andre Stromhaug, Jess Renteria",2016,http://arxiv.org/abs/1612.04035v1
Predictive networking and optimization for flow-based networks,"Artificial Neural Networks (ANNs) were used to classify neural network flows
by flow size. After training the neural network was able to predict the size of
a flows with 87% accuracy with a Feed Forward Neural Network. This demonstrates
that flow based routers can prioritize candidate flows with a predicted large
number of packets for priority insertion into hardware content-addressable
memory.",Michael Arnold,2017,http://arxiv.org/abs/1707.06729v1
Quantum Graph Neural Networks,"We introduce Quantum Graph Neural Networks (QGNN), a new class of quantum
neural network ansatze which are tailored to represent quantum processes which
have a graph structure, and are particularly suitable to be executed on
distributed quantum systems over a quantum network. Along with this general
class of ansatze, we introduce further specialized architectures, namely,
Quantum Graph Recurrent Neural Networks (QGRNN) and Quantum Graph Convolutional
Neural Networks (QGCNN). We provide four example applications of QGNNs:
learning Hamiltonian dynamics of quantum systems, learning how to create
multipartite entanglement in a quantum network, unsupervised learning for
spectral clustering, and supervised learning for graph isomorphism
classification.","Guillaume Verdon, Trevor McCourt, Enxhell Luzhnica, Vikash Singh, Stefan Leichenauer, Jack Hidary",2019,http://arxiv.org/abs/1909.12264v1
"FusionAccel: A General Re-configurable Deep Learning Inference
  Accelerator on FPGA for Convolutional Neural Networks","The deep learning accelerator is one of the methods to accelerate deep
learning network computations, which is mainly based on convolutional neural
network acceleration. To address the fact that concurrent convolutional neural
network accelerators are not solely open-source and the exclusiveness of
platforms, FusionAccel, a scalable convolutional neural network accelerator
hardware architecture with supporting software is proposed. It can adapt to
different network structures and can be reconstructed before compilation and
reconfigured at runtime. This paper realizes this RTL convolutional neural
network accelerator design and functional verifications on a Xilinx Spartan-6
FPGA. The result is identical to that of Caffe-CPU. Since the entire project is
based on RTL, it can be migrated to ASIC after replacing some FPGA-specific
IPs.",Shi Shi,2019,http://arxiv.org/abs/1907.02217v1
"Understanding Neural Networks with Logarithm Determinant Entropy
  Estimator","Understanding the informative behaviour of deep neural networks is challenged
by misused estimators and the complexity of network structure, which leads to
inconsistent observations and diversified interpretation. Here we propose the
LogDet estimator -- a reliable matrix-based entropy estimator that approximates
Shannon differential entropy. We construct informative measurements based on
LogDet estimator, verify our method with comparable experiments and utilize it
to analyse neural network behaviour. Our results demonstrate the LogDet
estimator overcomes the drawbacks that emerge from highly diverse and
degenerated distribution thus is reliable to estimate entropy in neural
networks. The Network analysis results also find a functional distinction
between shallow and deeper layers, which can help understand the compression
phenomenon in the Information bottleneck theory of neural networks.","Zhanghao Zhouyin, Ding Liu",2021,http://arxiv.org/abs/2105.03705v1
Neural Networks are Decision Trees,"In this manuscript, we show that any neural network with any activation
function can be represented as a decision tree. The representation is
equivalence and not an approximation, thus keeping the accuracy of the neural
network exactly as is. We believe that this work provides better understanding
of neural networks and paves the way to tackle their black-box nature. We share
equivalent trees of some neural networks and show that besides providing
interpretability, tree representation can also achieve some computational
advantages for small networks. The analysis holds both for fully connected and
convolutional networks, which may or may not also include skip connections
and/or normalizations.",Caglar Aytekin,2022,http://arxiv.org/abs/2210.05189v3
"Recurrent Neural Networks as Electrical Networks, a formalization","Since the 1980s, and particularly with the Hopfield model, recurrent neural
networks or RNN became a topic of great interest. The first works of neural
networks consisted of simple systems of a few neurons that were commonly
simulated through analogue electronic circuits. The passage from the equations
to the circuits was carried out directly without justification and subsequent
formalisation. The present work shows a way to formally obtain the equivalence
between an analogue circuit and a neural network and formalizes the connection
between both systems. We also show which are the properties that these
electrical networks must satisfy. We can have confidence that the
representation in terms of circuits is mathematically equivalent to the
equations that represent the network.","Mariano Caruso, Cecilia Jarne",2023,http://arxiv.org/abs/2304.06487v1
Web Neural Network with Complete DiGraphs,"This paper introduces a new neural network model that aims to mimic the
biological brain more closely by structuring the network as a complete directed
graph that processes continuous data for each timestep. Current neural networks
have structures that vaguely mimic the brain structure, such as neurons,
convolutions, and recurrence. The model proposed in this paper adds additional
structural properties by introducing cycles into the neuron connections and
removing the sequential nature commonly seen in other network layers.
Furthermore, the model has continuous input and output, inspired by spiking
neural networks, which allows the network to learn a process of classification,
rather than simply returning the final result.",Frank Li,2024,http://arxiv.org/abs/2401.04134v1
Deep State Space Recurrent Neural Networks for Time Series Forecasting,"We explore various neural network architectures for modeling the dynamics of
the cryptocurrency market. Traditional linear models often fall short in
accurately capturing the unique and complex dynamics of this market. In
contrast, Deep Neural Networks (DNNs) have demonstrated considerable
proficiency in time series forecasting. This papers introduces novel neural
network framework that blend the principles of econometric state space models
with the dynamic capabilities of Recurrent Neural Networks (RNNs). We propose
state space models using Long Short Term Memory (LSTM), Gated Residual Units
(GRU) and Temporal Kolmogorov-Arnold Networks (TKANs). According to the
results, TKANs, inspired by Kolmogorov-Arnold Networks (KANs) and LSTM,
demonstrate promising outcomes.",Hugo Inzirillo,2024,http://arxiv.org/abs/2407.15236v1
Extent of error control in neural networks,"The article sets and solves the task to control an error of the artificial
neural network with variable signal conductivity. This kind of neural networks
was especially developed to construct timetables. Behavior of such a neural
network can be described as dynamic system control problem. The authors gave as
the results of the solving the ANN feedback control problem.","Alexander Ignatenkov, Alexey Olshansky",2016,http://arxiv.org/abs/1608.04682v1
LAYERS: Yet another Neural Network toolkit,"Layers is an open source neural network toolkit aim at providing an easy way
to implement modern neural networks. The main user target are students and to
this end layers provides an easy scriptting language that can be early adopted.
The user has to focus only on design details as network totpology and parameter
tunning.","Roberto Paredes, José-Miguel Benedí",2016,http://arxiv.org/abs/1610.01430v2
A Study on Neural Network Language Modeling,"An exhaustive study on neural network language modeling (NNLM) is performed
in this paper. Different architectures of basic neural network language models
are described and examined. A number of different improvements over basic
neural network language models, including importance sampling, word classes,
caching and bidirectional recurrent neural network (BiRNN), are studied
separately, and the advantages and disadvantages of every technique are
evaluated. Then, the limits of neural network language modeling are explored
from the aspects of model architecture and knowledge representation. Part of
the statistical information from a word sequence will loss when it is processed
word by word in a certain order, and the mechanism of training neural network
by updating weight matrixes and vectors imposes severe restrictions on any
significant enhancement of NNLM. For knowledge representation, the knowledge
represented by neural network language models is the approximate probabilistic
distribution of word sequences from a certain training data set rather than the
knowledge of a language itself or the information conveyed by word sequences in
a natural language. Finally, some directions for improving neural network
language modeling further is discussed.",Dengliang Shi,2017,http://arxiv.org/abs/1708.07252v1
Equivariant neural networks and equivarification,"We provide a process to modify a neural network to an equivariant one, which
we call equivarification. As an illustration, we build an equivariant neural
network for image classification by equivarifying a convolutional neural
network.","Erkao Bao, Linqi Song",2019,http://arxiv.org/abs/1906.07172v4
ReachNN: Reachability Analysis of Neural-Network Controlled Systems,"Applying neural networks as controllers in dynamical systems has shown great
promises. However, it is critical yet challenging to verify the safety of such
control systems with neural-network controllers in the loop. Previous methods
for verifying neural network controlled systems are limited to a few specific
activation functions. In this work, we propose a new reachability analysis
approach based on Bernstein polynomials that can verify neural-network
controlled systems with a more general form of activation functions, i.e., as
long as they ensure that the neural networks are Lipschitz continuous.
Specifically, we consider abstracting feedforward neural networks with
Bernstein polynomials for a small subset of inputs. To quantify the error
introduced by abstraction, we provide both theoretical error bound estimation
based on the theory of Bernstein polynomials and more practical sampling based
error bound estimation, following a tight Lipschitz constant estimation
approach based on forward reachability analysis. Compared with previous
methods, our approach addresses a much broader set of neural networks,
including heterogeneous neural networks that contain multiple types of
activation functions. Experiment results on a variety of benchmarks show the
effectiveness of our approach.","Chao Huang, Jiameng Fan, Wenchao Li, Xin Chen, Qi Zhu",2019,http://arxiv.org/abs/1906.10654v1
"A differential neural network learns stochastic differential equations
  and the Black-Scholes equation for pricing multi-asset options","Neural networks with sufficiently smooth activation functions can approximate
values and derivatives of any smooth function, and they are differentiable
themselves. We improve the approximation capability of neural networks by
utilizing the differentiability of neural networks; the gradient and Hessian of
neural networks are used to train the neural networks to satisfy the
differential equations of the problems of interest. Several activation
functions are also compared in term of effective differentiation of neural
networks. We apply the differential neural networks to the pricing of financial
options, where stochastic differential equations and the Black-Scholes partial
differential equation represent the relation of price of option and underlying
assets, and the first and second derivatives, Greeks, of option play important
roles in financial engineering. The proposed neural network learns -- (a) the
sample paths of option prices generated by stochastic differential equations
and (b) the Black-Scholes equation at each time and asset price. Option pricing
experiments were performed on multi-asset options such as exchange and basket
options. Experimental results show that the proposed method gives accurate
option values and Greeks; sufficiently smooth activation functions and the
constraint of Black-Scholes equation contribute significantly for accurate
option pricing.",Sang-Mun Chi,2020,http://arxiv.org/abs/2007.00937v1
Towards Repairing Neural Networks Correctly,"Neural networks are increasingly applied to support decision making in
safety-critical applications (like autonomous cars, unmanned aerial vehicles
and face recognition based authentication). While many impressive static
verification techniques have been proposed to tackle the correctness problem of
neural networks, it is possible that static verification may never be
sufficiently scalable to handle real-world neural networks. In this work, we
propose a runtime verification method to ensure the correctness of neural
networks. Given a neural network and a desirable safety property, we adopt
state-of-the-art static verification techniques to identify strategically
locations to introduce additional gates which ""correct"" neural network
behaviors at runtime. Experiment results show that our approach effectively
generates neural networks which are guaranteed to satisfy the properties,
whilst being consistent with the original neural network most of the time.","Guoliang Dong, Jun Sun, Jingyi Wang, Xinyu Wang, Ting Dai",2020,http://arxiv.org/abs/2012.01872v2
Noise-robust classification with hypergraph neural network,"This paper presents a novel version of the hypergraph neural network method.
This method is utilized to solve the noisy label learning problem. First, we
apply the PCA dimensional reduction technique to the feature matrices of the
image datasets in order to reduce the ""noise"" and the redundant features in the
feature matrices of the image datasets and to reduce the runtime constructing
the hypergraph of the hypergraph neural network method. Then, the classic
graph-based semi-supervised learning method, the classic hypergraph based
semi-supervised learning method, the graph neural network, the hypergraph
neural network, and our proposed hypergraph neural network are employed to
solve the noisy label learning problem. The accuracies of these five methods
are evaluated and compared. Experimental results show that the hypergraph
neural network methods achieve the best performance when the noise level
increases. Moreover, the hypergraph neural network methods are at least as good
as the graph neural network.","Nguyen Trinh Vu Dang, Loc Tran, Linh Tran",2021,http://arxiv.org/abs/2102.01934v3
"Neural Networks with Complex-Valued Weights Have No Spurious Local
  Minima","We study the benefits of complex-valued weights for neural networks. We prove
that shallow complex neural networks with quadratic activations have no
spurious local minima. In contrast, shallow real neural networks with quadratic
activations have infinitely many spurious local minima under the same
conditions. In addition, we provide specific examples to demonstrate that
complex-valued weights turn poor local minima into saddle points.",Xingtu Liu,2021,http://arxiv.org/abs/2103.07287v2
Introduction to Neural Network Verification,"Deep learning has transformed the way we think of software and what it can
do. But deep neural networks are fragile and their behaviors are often
surprising. In many settings, we need to provide formal guarantees on the
safety, security, correctness, or robustness of neural networks. This book
covers foundational ideas from formal verification and their adaptation to
reasoning about neural networks and deep learning.",Aws Albarghouthi,2021,http://arxiv.org/abs/2109.10317v2
Rethinking Arithmetic for Deep Neural Networks,"We consider efficiency in the implementation of deep neural networks.
Hardware accelerators are gaining interest as machine learning becomes one of
the drivers of high-performance computing. In these accelerators, the directed
graph describing a neural network can be implemented as a directed graph
describing a Boolean circuit. We make this observation precise, leading
naturally to an understanding of practical neural networks as discrete
functions, and show that so-called binarised neural networks are functionally
complete. In general, our results suggest that it is valuable to consider
Boolean circuits as neural networks, leading to the question of which circuit
topologies are promising. We argue that continuity is central to generalisation
in learning, explore the interaction between data coding, network topology, and
node functionality for continuity, and pose some open questions for future
research. As a first step to bridging the gap between continuous and Boolean
views of neural network accelerators, we present some recent results from our
work on LUTNet, a novel Field-Programmable Gate Array inference approach.
Finally, we conclude with additional possible fruitful avenues for research
bridging the continuous and discrete views of neural networks.",George A. Constantinides,2019,http://arxiv.org/abs/1905.02438v2
"Muon Identification Using Deep Neural Networks with the Muon Telescope
  Detector at STAR","The installation of the muon telescope detector opened new possibilities for
studying dimuon production at STAR. However, backgrounds from hadron
punch-through and weak decays of pions and kaons make the identification of
primary muons challenging. In this paper we present a study of shallow and deep
neural networks trained as classifiers for the purpose of muon identification
using information from the muon telescope detector at STAR. The performance of
shallow neural networks is presented as a function of the number of neurons in
their hidden layer. A hyperparameter optimization for determining the optimal
deep neural network classifier architecture is presented. The optimized deep
neural network is compared with shallow neural networks, boosted decision
trees, likelihood ratios, and traditional cut-based PID techniques. The
superiority of the deep neural network based muon identification technique is
demonstrated and compared with traditional PID through the measurement of the
$\phi$ meson and the $\psi(2S)$ in p+p collisions at $\sqrt{s}$ = 200 GeV. The
deep neural network based PID simultaneously provides higher signal efficiency,
signal-to-background ratio, and significance of the $\phi$ peak compared to
traditional PID techniques. Finally, a deep neural network assisted technique
for measuring the muon purity in data is presented and discussed.","J. D. Brandenburg, Frank Geurts",2019,http://arxiv.org/abs/1908.05645v1
A note on the complex and bicomplex valued neural networks,"In this paper we first write a proof of the perceptron convergence algorithm
for the complex multivalued neural networks (CMVNNs). Our primary goal is to
formulate and prove the perceptron convergence algorithm for the bicomplex
multivalued neural networks (BMVNNs) and other important results in the theory
of neural networks based on a bicomplex algebra.","Daniel Alpay, Kamal Diki, Mihaela Vajiac",2022,http://arxiv.org/abs/2202.02354v1
"Deep Neural Networks as the Semi-classical Limit of Topological Quantum
  Neural Networks: The problem of generalisation","Deep Neural Networks miss a principled model of their operation. A novel
framework for supervised learning based on Topological Quantum Field Theory
that looks particularly well suited for implementation on quantum processors
has been recently explored. We propose using this framework to understand the
problem of generalisation in Deep Neural Networks. More specifically, in this
approach, Deep Neural Networks are viewed as the semi-classical limit of
Topological Quantum Neural Networks. A framework of this kind explains the
overfitting behavior of Deep Neural Networks during the training step and the
corresponding generalisation capabilities. We explore the paradigmatic case of
the perceptron, which we implement as the semiclassical limit of Topological
Quantum Neural Networks. We apply a novel algorithm we developed, showing that
it obtains similar results to standard neural networks, but without the need
for training (optimisation).","Antonino Marciano, Emanuele Zappala, Tommaso Torda, Matteo Lulli, Stefano Giagu, Chris Fields, Deen Chen, Filippo Fabrocini",2022,http://arxiv.org/abs/2210.13741v2
"On the Approximation and Complexity of Deep Neural Networks to Invariant
  Functions","Recent years have witnessed a hot wave of deep neural networks in various
domains; however, it is not yet well understood theoretically. A theoretical
characterization of deep neural networks should point out their approximation
ability and complexity, i.e., showing which architecture and size are
sufficient to handle the concerned tasks. This work takes one step on this
direction by theoretically studying the approximation and complexity of deep
neural networks to invariant functions. We first prove that the invariant
functions can be universally approximated by deep neural networks. Then we show
that a broad range of invariant functions can be asymptotically approximated by
various types of neural network models that includes the complex-valued neural
networks, convolutional neural networks, and Bayesian neural networks using a
polynomial number of parameters or optimization iterations. We also provide a
feasible application that connects the parameter estimation and forecasting of
high-resolution signals with our theoretical conclusions. The empirical results
obtained on simulation experiments demonstrate the effectiveness of our method.","Gao Zhang, Jin-Hui Wu, Shao-Qun Zhang",2022,http://arxiv.org/abs/2210.15279v1
VC dimensions of group convolutional neural networks,"We study the generalization capacity of group convolutional neural networks.
We identify precise estimates for the VC dimensions of simple sets of group
convolutional neural networks. In particular, we find that for infinite groups
and appropriately chosen convolutional kernels, already two-parameter families
of convolutional neural networks have an infinite VC dimension, despite being
invariant to the action of an infinite group.","Philipp Christian Petersen, Anna Sepliarskaia",2022,http://arxiv.org/abs/2212.09507v1
Expressivity of Spiking Neural Networks,"The synergy between spiking neural networks and neuromorphic hardware holds
promise for the development of energy-efficient AI applications. Inspired by
this potential, we revisit the foundational aspects to study the capabilities
of spiking neural networks where information is encoded in the firing time of
neurons. Under the Spike Response Model as a mathematical model of a spiking
neuron with a linear response function, we compare the expressive power of
artificial and spiking neural networks, where we initially show that they
realize piecewise linear mappings. In contrast to ReLU networks, we prove that
spiking neural networks can realize both continuous and discontinuous
functions. Moreover, we provide complexity bounds on the size of spiking neural
networks to emulate multi-layer (ReLU) neural networks. Restricting to the
continuous setting, we also establish complexity bounds in the reverse
direction for one-layer spiking neural networks.","Manjot Singh, Adalbert Fono, Gitta Kutyniok",2023,http://arxiv.org/abs/2308.08218v2
"Closed-Form Interpretation of Neural Network Classifiers with Symbolic
  Gradients","I introduce a unified framework for finding a closed-form interpretation of
any single neuron in an artificial neural network. Using this framework I
demonstrate how to interpret neural network classifiers to reveal closed-form
expressions of the concepts encoded in their decision boundaries. In contrast
to neural network-based regression, for classification, it is in general
impossible to express the neural network in the form of a symbolic equation
even if the neural network itself bases its classification on a quantity that
can be written as a closed-form equation. The interpretation framework is based
on embedding trained neural networks into an equivalence class of functions
that encode the same concept. I interpret these neural networks by finding an
intersection between the equivalence class and human-readable equations defined
by a symbolic search space. The approach is not limited to classifiers or full
neural networks and can be applied to arbitrary neurons in hidden layers or
latent spaces.",Sebastian Johann Wetzel,2024,http://arxiv.org/abs/2401.04978v2
"Null Space Properties of Neural Networks with Applications to Image
  Steganography","This paper explores the null space properties of neural networks. We extend
the null space definition from linear to nonlinear maps and discuss the
presence of a null space in neural networks. The null space of a given neural
network can tell us the part of the input data that makes no contribution to
the final prediction so that we can use it to trick the neural network. This
reveals an inherent weakness in neural networks that can be exploited. One
application described here leads to a method of image steganography. Through
experiments on image datasets such as MNIST, we show that we can use null space
components to force the neural network to choose a selected hidden image class,
even though the overall image can be made to look like a completely different
image. We conclude by showing comparisons between what a human viewer would
see, and the part of the image that the neural network is actually using to
make predictions and, hence, show that what the neural network ``sees'' is
completely different than what we would expect.","Xiang Li, Kevin M. Short",2024,http://arxiv.org/abs/2401.10262v1
"A Priori Estimation of the Approximation, Optimization and
  Generalization Errors of Random Neural Networks for Solving Partial
  Differential Equations","In recent years, neural networks have achieved remarkable progress in various
fields and have also drawn much attention in applying them on scientific
problems. A line of methods involving neural networks for solving partial
differential equations (PDEs), such as Physics-Informed Neural Networks (PINNs)
and the Deep Ritz Method (DRM), has emerged. Although these methods outperform
classical numerical methods in certain cases, the optimization problems
involving neural networks are typically non-convex and non-smooth, which can
result in unsatisfactory solutions for PDEs. In contrast to deterministic
neural networks, the hidden weights of random neural networks are sampled from
some prior distribution and only the output weights participate in training.
This makes training much simpler, but it remains unclear how to select the
prior distribution. In this paper, we focus on Barron type functions and
approximate them under Sobolev norms by random neural networks with clear prior
distribution. In addition to the approximation error, we also derive bounds for
the optimization and generalization errors of random neural networks for
solving PDEs when the solutions are Barron type functions.","Xianliang Xu, Zhongyi Huang",2024,http://arxiv.org/abs/2406.03080v2
Approximation by Steklov Neural Network Operators,"The present paper deals with construction of newly family of Neural Network
operators, that is, Steklov Neural Network operators. By using Steklov type
integral, we introduce a new version of Neural Network operators and we obtain
some convergence theorems for the family, such as, pointwise and uniform
convergence, rate of convergence via modulus of continuity.","S. N. Karaman, M. Turgay, T. Acar",2024,http://arxiv.org/abs/2410.01426v2
"Superhypergraph Neural Networks and Plithogenic Graph Neural Networks:
  Theoretical Foundations","Hypergraphs extend traditional graphs by allowing edges to connect multiple
nodes, while superhypergraphs further generalize this concept to represent even
more complex relationships. Neural networks, inspired by biological systems,
are widely used for tasks such as pattern recognition, data classification, and
prediction. Graph Neural Networks (GNNs), a well-established framework, have
recently been extended to Hypergraph Neural Networks (HGNNs), with their
properties and applications being actively studied. The Plithogenic Graph
framework enhances graph representations by integrating multi-valued
attributes, as well as membership and contradiction functions, enabling the
detailed modeling of complex relationships. In the context of handling
uncertainty, concepts such as Fuzzy Graphs and Neutrosophic Graphs have gained
prominence. It is well established that Plithogenic Graphs serve as a
generalization of both Fuzzy Graphs and Neutrosophic Graphs. Furthermore, the
Fuzzy Graph Neural Network has been proposed and is an active area of research.
This paper establishes the theoretical foundation for the development of
SuperHyperGraph Neural Networks (SHGNNs) and Plithogenic Graph Neural Networks,
expanding the applicability of neural networks to these advanced graph
structures. While mathematical generalizations and proofs are presented, future
computational experiments are anticipated.",Takaaki Fujita,2024,http://arxiv.org/abs/2412.01176v1
"LuxNAS: A Coherent Photonic Neural Network Powered by Neural
  Architecture Search","We demonstrate a novel coherent photonic neural network using tunable
phase-change-material-based couplers and neural architecture search. Compared
to the MZI-based Clements network, our results indicate 85% reduction in the
network footprint while maintaining the accuracy.","Amin Shafiee, Febin Sunny, Sudeep Pasricha, Mahdi Nikdast",2025,http://arxiv.org/abs/2503.03731v1
Structure Learning of Deep Neural Networks with Q-Learning,"Recently, with convolutional neural networks gaining significant achievements
in many challenging machine learning fields, hand-crafted neural networks no
longer satisfy our requirements as designing a network will cost a lot, and
automatically generating architectures has attracted increasingly more
attention and focus. Some research on auto-generated networks has achieved
promising results. However, they mainly aim at picking a series of single
layers such as convolution or pooling layers one by one. There are many elegant
and creative designs in the carefully hand-crafted neural networks, such as
Inception-block in GoogLeNet, residual block in residual network and dense
block in dense convolutional network. Based on reinforcement learning and
taking advantages of the superiority of these networks, we propose a novel
automatic process to design a multi-block neural network, whose architecture
contains multiple types of blocks mentioned above, with the purpose to do
structure learning of deep neural networks and explore the possibility whether
different blocks can be composed together to form a well-behaved neural
network. The optimal network is created by the Q-learning agent who is trained
to sequentially pick different types of blocks. To verify the validity of our
proposed method, we use the auto-generated multi-block neural network to
conduct experiments on image benchmark datasets MNIST, SVHN and CIFAR-10 image
classification task with restricted computational resources. The results
demonstrate that our method is very effective, achieving comparable or better
performance than hand-crafted networks and advanced auto-generated neural
networks.","Guoqiang Zhong, Wencong Jiao, Wei Gao",2018,http://arxiv.org/abs/1810.13155v1
Theoretical Analysis of the Advantage of Deepening Neural Networks,"We propose two new criteria to understand the advantage of deepening neural
networks. It is important to know the expressivity of functions computable by
deep neural networks in order to understand the advantage of deepening neural
networks. Unless deep neural networks have enough expressivity, they cannot
have good performance even though learning is successful. In this situation,
the proposed criteria contribute to understanding the advantage of deepening
neural networks since they can evaluate the expressivity independently from the
efficiency of learning. The first criterion shows the approximation accuracy of
deep neural networks to the target function. This criterion has the background
that the goal of deep learning is approximating the target function by deep
neural networks. The second criterion shows the property of linear regions of
functions computable by deep neural networks. This criterion has the background
that deep neural networks whose activation functions are piecewise linear are
also piecewise linear. Furthermore, by the two criteria, we show that to
increase layers is more effective than to increase units at each layer on
improving the expressivity of deep neural networks.","Yasushi Esaki, Yuta Nakahara, Toshiyasu Matsushima",2020,http://arxiv.org/abs/2009.11479v1
Anomaly-resistant Graph Neural Networks via Neural Architecture Search,"In general, Graph Neural Networks(GNN) have been using a message passing
method to aggregate and summarize information about neighbors to express their
information. Nonetheless, previous studies have shown that the performance of
graph neural networks becomes vulnerable when there are abnormal nodes in the
neighborhood due to this message passing method. In this paper, inspired by the
Neural Architecture Search method, we present an algorithm that recognizes
abnormal nodes and automatically excludes them from information aggregation.
Experiments on various real worlds datasets show that our proposed Neural
Architecture Search-based Anomaly Resistance Graph Neural Network (NASAR-GNN)
is actually effective.",M. Park,2021,http://arxiv.org/abs/2111.11406v3
Probabilistic Neural Programs,"We present probabilistic neural programs, a framework for program induction
that permits flexible specification of both a computational model and inference
algorithm while simultaneously enabling the use of deep neural networks.
Probabilistic neural programs combine a computation graph for specifying a
neural network with an operator for weighted nondeterministic choice. Thus, a
program describes both a collection of decisions as well as the neural network
architecture used to make each one. We evaluate our approach on a challenging
diagram question answering task where probabilistic neural programs correctly
execute nearly twice as many programs as a baseline model.","Kenton W. Murray, Jayant Krishnamurthy",2016,http://arxiv.org/abs/1612.00712v1
"Sparsely-Connected Neural Networks: Towards Efficient VLSI
  Implementation of Deep Neural Networks","Recently deep neural networks have received considerable attention due to
their ability to extract and represent high-level abstractions in data sets.
Deep neural networks such as fully-connected and convolutional neural networks
have shown excellent performance on a wide range of recognition and
classification tasks. However, their hardware implementations currently suffer
from large silicon area and high power consumption due to the their high degree
of complexity. The power/energy consumption of neural networks is dominated by
memory accesses, the majority of which occur in fully-connected networks. In
fact, they contain most of the deep neural network parameters. In this paper,
we propose sparsely-connected networks, by showing that the number of
connections in fully-connected networks can be reduced by up to 90% while
improving the accuracy performance on three popular datasets (MNIST, CIFAR10
and SVHN). We then propose an efficient hardware architecture based on
linear-feedback shift registers to reduce the memory requirements of the
proposed sparsely-connected networks. The proposed architecture can save up to
90% of memory compared to the conventional implementations of fully-connected
neural networks. Moreover, implementation results show up to 84% reduction in
the energy consumption of a single neuron of the proposed sparsely-connected
networks compared to a single neuron of fully-connected neural networks.","Arash Ardakani, Carlo Condo, Warren J. Gross",2016,http://arxiv.org/abs/1611.01427v3
Verifying Properties of Binarized Deep Neural Networks,"Understanding properties of deep neural networks is an important challenge in
deep learning. In this paper, we take a step in this direction by proposing a
rigorous way of verifying properties of a popular class of neural networks,
Binarized Neural Networks, using the well-developed means of Boolean
satisfiability. Our main contribution is a construction that creates a
representation of a binarized neural network as a Boolean formula. Our encoding
is the first exact Boolean representation of a deep neural network. Using this
encoding, we leverage the power of modern SAT solvers along with a proposed
counterexample-guided search procedure to verify various properties of these
networks. A particular focus will be on the critical property of robustness to
adversarial perturbations. For this property, our experimental results
demonstrate that our approach scales to medium-size deep neural networks used
in image classification tasks. To the best of our knowledge, this is the first
work on verifying properties of deep neural networks using an exact Boolean
encoding of the network.","Nina Narodytska, Shiva Prasad Kasiviswanathan, Leonid Ryzhyk, Mooly Sagiv, Toby Walsh",2017,http://arxiv.org/abs/1709.06662v2
"Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any
  Architecture are Gaussian Processes","Wide neural networks with random weights and biases are Gaussian processes,
as originally observed by Neal (1995) and more recently by Lee et al. (2018)
and Matthews et al. (2018) for deep fully-connected networks, as well as by
Novak et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional
networks. We show that this Neural Network-Gaussian Process correspondence
surprisingly extends to all modern feedforward or recurrent neural networks
composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph)
convolution, pooling, skip connection, attention, batch normalization, and/or
layer normalization. More generally, we introduce a language for expressing
neural network computations, and our result encompasses all such expressible
neural networks. This work serves as a tutorial on the *tensor programs*
technique formulated in Yang (2019) and elucidates the Gaussian Process results
obtained there. We provide open-source implementations of the Gaussian Process
kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at
github.com/thegregyang/GP4A.",Greg Yang,2019,http://arxiv.org/abs/1910.12478v3
Neural Network Quantization for Efficient Inference: A Survey,"As neural networks have become more powerful, there has been a rising desire
to deploy them in the real world; however, the power and accuracy of neural
networks is largely due to their depth and complexity, making them difficult to
deploy, especially in resource-constrained devices. Neural network quantization
has recently arisen to meet this demand of reducing the size and complexity of
neural networks by reducing the precision of a network. With smaller and
simpler networks, it becomes possible to run neural networks within the
constraints of their target hardware. This paper surveys the many neural
network quantization techniques that have been developed in the last decade.
Based on this survey and comparison of neural network quantization techniques,
we propose future directions of research in the area.",Olivia Weng,2021,http://arxiv.org/abs/2112.06126v2
The semantic landscape paradigm for neural networks,"Deep neural networks exhibit a fascinating spectrum of phenomena ranging from
predictable scaling laws to the unpredictable emergence of new capabilities as
a function of training time, dataset size and network size. Analysis of these
phenomena has revealed the existence of concepts and algorithms encoded within
the learned representations of these networks. While significant strides have
been made in explaining observed phenomena separately, a unified framework for
understanding, dissecting, and predicting the performance of neural networks is
lacking. Here, we introduce the semantic landscape paradigm, a conceptual and
mathematical framework that describes the training dynamics of neural networks
as trajectories on a graph whose nodes correspond to emergent algorithms that
are instrinsic to the learned representations of the networks. This abstraction
enables us to describe a wide range of neural network phenomena in terms of
well studied problems in statistical physics. Specifically, we show that
grokking and emergence with scale are associated with percolation phenomena,
and neural scaling laws are explainable in terms of the statistics of random
walks on graphs. Finally, we discuss how the semantic landscape paradigm
complements existing theoretical and practical approaches aimed at
understanding and interpreting deep neural networks.",Shreyas Gokhale,2023,http://arxiv.org/abs/2307.09550v1
Neural filtering for Neural Network-based Models of Dynamic Systems,"The application of neural networks in modeling dynamic systems has become
prominent due to their ability to estimate complex nonlinear functions. Despite
their effectiveness, neural networks face challenges in long-term predictions,
where the prediction error diverges over time, thus degrading their accuracy.
This paper presents a neural filter to enhance the accuracy of long-term state
predictions of neural network-based models of dynamic systems. Motivated by the
extended Kalman filter, the neural filter combines the neural network state
predictions with the measurements from the physical system to improve the
estimated state's accuracy. The neural filter's improvements in prediction
accuracy are demonstrated through applications to four nonlinear dynamical
systems. Numerical experiments show that the neural filter significantly
improves prediction accuracy and bounds the state estimate covariance,
outperforming the neural network predictions.","Parham Oveissi, Turibius Rozario, Ankit Goel",2024,http://arxiv.org/abs/2409.13654v1
SOCRATES: Towards a Unified Platform for Neural Network Analysis,"Studies show that neural networks, not unlike traditional programs, are
subject to bugs, e.g., adversarial samples that cause classification errors and
discriminatory instances that demonstrate the lack of fairness. Given that
neural networks are increasingly applied in critical applications (e.g.,
self-driving cars, face recognition systems and personal credit rating
systems), it is desirable that systematic methods are developed to analyze
(e.g., test or verify) neural networks against desirable properties. Recently,
a number of approaches have been developed for analyzing neural networks. These
efforts are however scattered (i.e., each approach tackles some restricted
classes of neural networks against certain particular properties), incomparable
(i.e., each approach has its own assumptions and input format) and thus hard to
apply, reuse or extend. In this project, we aim to build a unified framework
for developing techniques to analyze neural networks. Towards this goal, we
develop a platform called SOCRATES which supports a standardized format for a
variety of neural network models, an assertion language for property
specification as well as multiple neural network analysis algorithms including
two novel ones for falsifying and probabilistic verification of neural network
models. SOCRATES is extensible and thus existing approaches can be easily
integrated. Experiment results show that our platform can handle a wide range
of networks models and properties. More importantly, it provides a platform for
synergistic research on neural network analysis.","Long H. Pham, Jiaying Li, Jun Sun",2020,http://arxiv.org/abs/2007.11206v2
Multi-Grade Deep Learning,"The current deep learning model is of a single-grade, that is, it learns a
deep neural network by solving a single nonconvex optimization problem. When
the layer number of the neural network is large, it is computationally
challenging to carry out such a task efficiently. Inspired by the human
education process which arranges learning in grades, we propose a multi-grade
learning model: We successively solve a number of optimization problems of
small sizes, which are organized in grades, to learn a shallow neural network
for each grade. Specifically, the current grade is to learn the leftover from
the previous grade. In each of the grades, we learn a shallow neural network
stacked on the top of the neural network, learned in the previous grades, which
remains unchanged in training of the current and future grades. By dividing the
task of learning a deep neural network into learning several shallow neural
networks, one can alleviate the severity of the nonconvexity of the original
optimization problem of a large size. When all grades of the learning are
completed, the final neural network learned is a stair-shape neural network,
which is the superposition of networks learned from all grades. Such a model
enables us to learn a deep neural network much more effectively and
efficiently. Moreover, multi-grade learning naturally leads to adaptive
learning. We prove that in the context of function approximation if the neural
network generated by a new grade is nontrivial, the optimal error of the grade
is strictly reduced from the optimal error of the previous grade. Furthermore,
we provide several proof-of-concept numerical examples which demonstrate that
the proposed multi-grade model outperforms significantly the traditional
single-grade model and is much more robust than the traditional model.",Yuesheng Xu,2023,http://arxiv.org/abs/2302.00150v1
"Quaternion-Valued Recurrent Projection Neural Networks on Unit
  Quaternions","Hypercomplex-valued neural networks, including quaternion-valued neural
networks, can treat multi-dimensional data as a single entity. In this paper,
we present the quaternion-valued recurrent projection neural networks (QRPNNs).
Briefly, QRPNNs are obtained by combining the non-local projection learning
with the quaternion-valued recurrent correlation neural network (QRCNNs). We
show that QRPNNs overcome the cross-talk problem of QRCNNs. Thus, they are
appropriate to implement associative memories. Furthermore, computational
experiments reveal that QRPNNs exhibit greater storage capacity and noise
tolerance than their corresponding QRCNNs.","Marcos Eduardo Valle, Rodolfo Anibal Lobo",2020,http://arxiv.org/abs/2001.11846v1
Mastering high-dimensional dynamics with Hamiltonian neural networks,"We detail how incorporating physics into neural network design can
significantly improve the learning and forecasting of dynamical systems, even
nonlinear systems of many dimensions. A map building perspective elucidates the
superiority of Hamiltonian neural networks over conventional neural networks.
The results clarify the critical relation between data, dimension, and neural
network learning performance.","Scott T. Miller, John F. Lindner, Anshul Choudhary, Sudeshna Sinha, William L. Ditto",2020,http://arxiv.org/abs/2008.04214v1
Synaptic metaplasticity in binarized neural networks,"Unlike the brain, artificial neural networks, including state-of-the-art deep
neural networks for computer vision, are subject to ""catastrophic forgetting"":
they rapidly forget the previous task when trained on a new one. Neuroscience
suggests that biological synapses avoid this issue through the process of
synaptic consolidation and metaplasticity: the plasticity itself changes upon
repeated synaptic events. In this work, we show that this concept of
metaplasticity can be transferred to a particular type of deep neural networks,
binarized neural networks, to reduce catastrophic forgetting.","Axel Laborieux, Maxence Ernoult, Tifenn Hirtzlin, Damien Querlioz",2021,http://arxiv.org/abs/2101.07592v1
"Convolutional Neural Network(CNN/ConvNet) in Stock Price Movement
  Prediction","With technological advancements and the exponential growth of data, we have
been unfolding different capabilities of neural networks in different sectors.
In this paper, I have tried to use a specific type of Neural Network known as
Convolutional Neural Network(CNN/ConvNet) in the stock market. In other words,
I have tried to construct and train a convolutional neural network on past
stock prices data and then tried to predict the movement of stock price i.e.
whether the stock price would rise or fall, in the coming time.",Kunal Bhardwaj,2021,http://arxiv.org/abs/2106.01920v1
On the space of coefficients of a Feed Forward Neural Network,"We define and establish the conditions for `equivalent neural networks' -
neural networks with different weights, biases, and threshold functions that
result in the same associated function. We prove that given a neural network
$\mathcal{N}$ with piece-wise linear activation, the space of coefficients
describing all equivalent neural networks is given by a semialgebraic set. This
result is obtained by studying different representations of a given piece-wise
linear function using the Tarski-Seidenberg theorem.","Dinesh Valluri, Rory Campbell",2021,http://arxiv.org/abs/2109.03362v1
deepstruct -- linking deep learning and graph theory,"deepstruct connects deep learning models and graph theory such that different
graph structures can be imposed on neural networks or graph structures can be
extracted from trained neural network models. For this, deepstruct provides
deep neural network models with different restrictions which can be created
based on an initial graph. Further, tools to extract graph structures from
trained models are available. This step of extracting graphs can be
computationally expensive even for models of just a few dozen thousand
parameters and poses a challenging problem. deepstruct supports research in
pruning, neural architecture search, automated network design and structure
analysis of neural networks.","Julian Stier, Michael Granitzer",2021,http://arxiv.org/abs/2111.06679v2
HyperNCA: Growing Developmental Networks with Neural Cellular Automata,"In contrast to deep reinforcement learning agents, biological neural networks
are grown through a self-organized developmental process. Here we propose a new
hypernetwork approach to grow artificial neural networks based on neural
cellular automata (NCA). Inspired by self-organising systems and
information-theoretic approaches to developmental biology, we show that our
HyperNCA method can grow neural networks capable of solving common
reinforcement learning tasks. Finally, we explore how the same approach can be
used to build developmental metamorphosis networks capable of transforming
their weights to solve variations of the initial RL task.","Elias Najarro, Shyam Sudhakaran, Claire Glanois, Sebastian Risi",2022,http://arxiv.org/abs/2204.11674v1
Stochastic resonance neurons in artificial neural networks,"Many modern applications of the artificial neural networks ensue large number
of layers making traditional digital implementations increasingly complex.
Optical neural networks offer parallel processing at high bandwidth, but have
the challenge of noise accumulation. We propose here a new type of neural
networks using stochastic resonances as an inherent part of the architecture
and demonstrate a possibility of significant reduction of the required number
of neurons for a given performance accuracy. We also show that such a neural
network is more robust against the impact of noise.","Egor Manuylovich, Diego Argüello Ron, Morteza Kamalian-Kopae, Sergei Turitsyn",2022,http://arxiv.org/abs/2205.10122v2
Issues with Neural Tangent Kernel Approach to Neural Networks,"Neural tangent kernels (NTKs) have been proposed to study the behavior of
trained neural networks from the perspective of Gaussian processes. An
important result in this body of work is the theorem of equivalence between a
trained neural network and kernel regression with the corresponding NTK. This
theorem allows for an interpretation of neural networks as special cases of
kernel regression. However, does this theorem of equivalence hold in practice?
  In this paper, we revisit the derivation of the NTK rigorously and conduct
numerical experiments to evaluate this equivalence theorem. We observe that
adding a layer to a neural network and the corresponding updated NTK do not
yield matching changes in the predictor error. Furthermore, we observe that
kernel regression with a Gaussian process kernel in the literature that does
not account for neural network training produces prediction errors very close
to that of kernel regression with NTKs. These observations suggest the
equivalence theorem does not hold well in practice and puts into question
whether neural tangent kernels adequately address the training process of
neural networks.","Haoran Liu, Anthony Tai, David J. Crandall, Chunfeng Huang",2025,http://arxiv.org/abs/2501.10929v1
"Differentiable Neural Architecture Learning for Efficient Neural Network
  Design","Automated neural network design has received ever-increasing attention with
the evolution of deep convolutional neural networks (CNNs), especially
involving their deployment on embedded and mobile platforms. One of the biggest
problems that neural architecture search (NAS) confronts is that a large number
of candidate neural architectures are required to train, using, for instance,
reinforcement learning and evolutionary optimisation algorithms, at a vast
computation cost. Even recent differentiable neural architecture search (DNAS)
samples a small number of candidate neural architectures based on the
probability distribution of learned architecture parameters to select the final
neural architecture. To address this computational complexity issue, we
introduce a novel \emph{architecture parameterisation} based on scaled sigmoid
function, and propose a general \emph{Differentiable Neural Architecture
Learning} (DNAL) method to optimize the neural architecture without the need to
evaluate candidate neural networks. Specifically, for stochastic supernets as
well as conventional CNNs, we build a new channel-wise module layer with the
architecture components controlled by a scaled sigmoid function. We train these
neural network models from scratch. The network optimization is decoupled into
the weight optimization and the architecture optimization. We address the
non-convex optimization problem of neural architecture by the continuous scaled
sigmoid method with convergence guarantees. Extensive experiments demonstrate
our DNAL method delivers superior performance in terms of neural architecture
search cost. The optimal networks learned by DNAL surpass those produced by the
state-of-the-art methods on the benchmark CIFAR-10 and ImageNet-1K dataset in
accuracy, model size and computational complexity.","Qingbei Guo, Xiao-Jun Wu, Josef Kittler, Zhiquan Feng",2021,http://arxiv.org/abs/2103.02126v1
"Collocation Polynomial Neural Forms and Domain Fragmentation for solving
  Initial Value Problems","Several neural network approaches for solving differential equations employ
trial solutions with a feedforward neural network. There are different means to
incorporate the trial solution in the construction, for instance one may
include them directly in the cost function. Used within the corresponding
neural network, the trial solutions define the so-called neural form. Such
neural forms represent general, flexible tools by which one may solve various
differential equations. In this article we consider time-dependent initial
value problems, which require to set up the neural form framework adequately.
The neural forms presented up to now in the literature for such a setting can
be considered as first order polynomials. In this work we propose to extend the
polynomial order of the neural forms. The novel collocation-type construction
includes several feedforward neural networks, one for each order. Additionally,
we propose the fragmentation of the computational domain into subdomains. The
neural forms are solved on each subdomain, whereas the interfacing grid points
overlap in order to provide initial values over the whole fragmentation. We
illustrate in experiments that the combination of collocation neural forms of
higher order and the domain fragmentation allows to solve initial value
problems over large domains with high accuracy and reliability.","Toni Schneidereit, Michael Breuß",2021,http://arxiv.org/abs/2103.15413v2
"Graph Neural Networks for Learning Equivariant Representations of Neural
  Networks","Neural networks that process the parameters of other neural networks find
applications in domains as diverse as classifying implicit neural
representations, generating neural network weights, and predicting
generalization errors. However, existing approaches either overlook the
inherent permutation symmetry in the neural network or rely on intricate
weight-sharing patterns to achieve equivariance, while ignoring the impact of
the network architecture itself. In this work, we propose to represent neural
networks as computational graphs of parameters, which allows us to harness
powerful graph neural networks and transformers that preserve permutation
symmetry. Consequently, our approach enables a single model to encode neural
computational graphs with diverse architectures. We showcase the effectiveness
of our method on a wide range of tasks, including classification and editing of
implicit neural representations, predicting generalization performance, and
learning to optimize, while consistently outperforming state-of-the-art
methods. The source code is open-sourced at
https://github.com/mkofinas/neural-graphs.","Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J. Burghouts, Efstratios Gavves, Cees G. M. Snoek, David W. Zhang",2024,http://arxiv.org/abs/2403.12143v3
Sparse Neural Networks Topologies,"We propose Sparse Neural Network architectures that are based on random or
structured bipartite graph topologies. Sparse architectures provide compression
of the models learned and speed-ups of computations, they can also surpass
their unstructured or fully connected counterparts. As we show, even more
compact topologies of the so-called SNN (Sparse Neural Network) can be achieved
with the use of structured graphs of connections between consecutive layers of
neurons. In this paper, we investigate how the accuracy and training speed of
the models depend on the topology and sparsity of the neural network. Previous
approaches using sparcity are all based on fully connected neural network
models and create sparcity during training phase, instead we explicitly define
a sparse architectures of connections before the training. Building compact
neural network models is coherent with empirical observations showing that
there is much redundancy in learned neural network models. We show
experimentally that the accuracy of the models learned with neural networks
depends on expander-like properties of the underlying topologies such as the
spectral gap and algebraic connectivity rather than the density of the graphs
of connections.","Alfred Bourely, John Patrick Boueri, Krzysztof Choromonski",2017,http://arxiv.org/abs/1706.05683v1
Optimal modularity and memory capacity of neural reservoirs,"The neural network is a powerful computing framework that has been exploited
by biological evolution and by humans for solving diverse problems. Although
the computational capabilities of neural networks are determined by their
structure, the current understanding of the relationships between a neural
network's architecture and function is still primitive. Here we reveal that
neural network's modular architecture plays a vital role in determining the
neural dynamics and memory performance of the network of threshold neurons. In
particular, we demonstrate that there exists an optimal modularity for memory
performance, where a balance between local cohesion and global connectivity is
established, allowing optimally modular networks to remember longer. Our
results suggest that insights from dynamical analysis of neural networks and
information spreading processes can be leveraged to better design neural
networks and may shed light on the brain's modular organization.","Nathaniel Rodriguez, Eduardo Izquierdo, Yong-Yeol Ahn",2017,http://arxiv.org/abs/1706.06511v3
Probabilistic Verification of Neural Networks Against Group Fairness,"Fairness is crucial for neural networks which are used in applications with
important societal implication. Recently, there have been multiple attempts on
improving fairness of neural networks, with a focus on fairness testing (e.g.,
generating individual discriminatory instances) and fairness training (e.g.,
enhancing fairness through augmented training). In this work, we propose an
approach to formally verify neural networks against fairness, with a focus on
independence-based fairness such as group fairness. Our method is built upon an
approach for learning Markov Chains from a user-provided neural network (i.e.,
a feed-forward neural network or a recurrent neural network) which is
guaranteed to facilitate sound analysis. The learned Markov Chain not only
allows us to verify (with Probably Approximate Correctness guarantee) whether
the neural network is fair or not, but also facilities sensitivity analysis
which helps to understand why fairness is violated. We demonstrate that with
our analysis results, the neural weights can be optimized to improve fairness.
Our approach has been evaluated with multiple models trained on benchmark
datasets and the experiment results show that our approach is effective and
efficient.","Bing Sun, Jun Sun, Ting Dai, Lijun Zhang",2021,http://arxiv.org/abs/2107.08362v1
Nonparametric Regression Quantum Neural Networks,"In two pervious papers \cite{dndiep3}, \cite{dndiep4}, the first author
constructed the least square quantum neural networks (LS-QNN), and ploynomial
interpolation quantum neural networks ( PI-QNN), parametrico-stattistical QNN
like: leanr regrassion quantum neural networks (LR-QNN), polynomial regression
quantum neural networks (PR-QNN), chi-squared quantum neural netowrks
($\chi^2$-QNN). We observed that the method works also in the cases by using
nonparametric statistics. In this paper we analyze and implement the
nonparametric tests on QNN such as: linear nonparametric regression quantum
neural networks (LNR-QNN), polynomial nonparametric regression quantum neural
networks (PNR-QNN). The implementation is constructed through the Gauss-Jordan
Elimination quantum neural networks (GJE-QNN).The training rule is to use the
high probability confidence regions or intervals.","Do Ngoc Diep, Koji Nagata, Tadao Nakamura",2020,http://arxiv.org/abs/2002.02818v1
"Training neural networks using Metropolis Monte Carlo and an adaptive
  variant","We examine the zero-temperature Metropolis Monte Carlo algorithm as a tool
for training a neural network by minimizing a loss function. We find that, as
expected on theoretical grounds and shown empirically by other authors,
Metropolis Monte Carlo can train a neural net with an accuracy comparable to
that of gradient descent, if not necessarily as quickly. The Metropolis
algorithm does not fail automatically when the number of parameters of a neural
network is large. It can fail when a neural network's structure or neuron
activations are strongly heterogenous, and we introduce an adaptive Monte Carlo
algorithm, aMC, to overcome these limitations. The intrinsic stochasticity and
numerical stability of the Monte Carlo method allow aMC to train deep neural
networks and recurrent neural networks in which the gradient is too small or
too large to allow training by gradient descent. Monte Carlo methods offer a
complement to gradient-based methods for training neural networks, allowing
access to a distinct set of network architectures and principles.","Stephen Whitelam, Viktor Selin, Ian Benlolo, Corneel Casert, Isaac Tamblyn",2022,http://arxiv.org/abs/2205.07408v2
"Hardware Accelerator and Neural Network Co-Optimization for
  Ultra-Low-Power Audio Processing Devices","The increasing spread of artificial neural networks does not stop at
ultralow-power edge devices. However, these very often have high computational
demand and require specialized hardware accelerators to ensure the design meets
power and performance constraints. The manual optimization of neural networks
along with the corresponding hardware accelerators can be very challenging.
This paper presents HANNAH (Hardware Accelerator and Neural Network seArcH), a
framework for automated and combined hardware/software co-design of deep neural
networks and hardware accelerators for resource and power-constrained edge
devices. The optimization approach uses an evolution-based search algorithm, a
neural network template technique, and analytical KPI models for the
configurable UltraTrail hardware accelerator template to find an optimized
neural network and accelerator configuration. We demonstrate that HANNAH can
find suitable neural networks with minimized power consumption and high
accuracy for different audio classification tasks such as single-class wake
word detection, multi-class keyword detection, and voice activity detection,
which are superior to the related work.","Christoph Gerum, Adrian Frischknecht, Tobias Hald, Paul Palomero Bernardo, Konstantin Lübeck, Oliver Bringmann",2022,http://arxiv.org/abs/2209.03807v2
"A Hierarchical Fused Quantum Fuzzy Neural Network for Image
  Classification","Neural network is a powerful learning paradigm for data feature learning in
the era of big data. However, most neural network models are deterministic
models that ignore the uncertainty of data. Fuzzy neural networks are proposed
to address this problem. FDNN is a hierarchical deep neural network that
derives information from both fuzzy and neural representations, the
representations are then fused to form representation to be classified. FDNN
perform well on uncertain data classification tasks. In this paper, we proposed
a novel hierarchical fused quantum fuzzy neural network (HQFNN). Different from
classical FDNN, HQFNN uses quantum neural networks to learn fuzzy membership
functions in fuzzy neural network. We conducted simulated experiment on two
types of datasets (Dirty-MNIST and 15-Scene), the results show that the
proposed model can outperform several existing methods. In addition, we
demonstrate the robustness of the proposed quantum circuit.","Sheng-Yao Wu, Run-Ze Li, Yan-Qi Song, Su-Juan Qin, Qiao-Yan Wen, Fei Gao",2024,http://arxiv.org/abs/2403.09318v1
"Graph Neural Networks for Small Graph and Giant Network Representation
  Learning: An Overview","Graph neural networks denote a group of neural network models introduced for
the representation learning tasks on graph data specifically. Graph neural
networks have been demonstrated to be effective for capturing network structure
information, and the learned representations can achieve the state-of-the-art
performance on node and graph classification tasks. Besides the different
application scenarios, the architectures of graph neural network models also
depend on the studied graph types a lot. Graph data studied in research can be
generally categorized into two main types, i.e., small graphs vs. giant
networks, which differ from each other a lot in the size, instance number and
label annotation. Several different types of graph neural network models have
been introduced for learning the representations from such different types of
graphs already. In this paper, for these two different types of graph data, we
will introduce the graph neural networks introduced in recent years. To be more
specific, the graph neural networks introduced in this paper include IsoNN,
SDBN, LF&ER, GCN, GAT, DifNN, GNL, GraphSage and seGEN. Among these graph
neural network models, IsoNN, SDBN and LF&ER are initially proposed for small
graphs and the remaining ones are initially proposed for giant networks
instead. The readers are also suggested to refer to these papers for detailed
information when reading this tutorial paper.",Jiawei Zhang,2019,http://arxiv.org/abs/1908.00187v1
"Efficient and Flexible Method for Reducing Moderate-size Deep Neural
  Networks with Condensation","Neural networks have been extensively applied to a variety of tasks,
achieving astounding results. Applying neural networks in the scientific field
is an important research direction that is gaining increasing attention. In
scientific applications, the scale of neural networks is generally
moderate-size, mainly to ensure the speed of inference during application.
Additionally, comparing neural networks to traditional algorithms in scientific
applications is inevitable. These applications often require rapid
computations, making the reduction of neural network sizes increasingly
important. Existing work has found that the powerful capabilities of neural
networks are primarily due to their non-linearity. Theoretical work has
discovered that under strong non-linearity, neurons in the same layer tend to
behave similarly, a phenomenon known as condensation. Condensation offers an
opportunity to reduce the scale of neural networks to a smaller subnetwork with
similar performance. In this article, we propose a condensation reduction
algorithm to verify the feasibility of this idea in practical problems. Our
reduction method can currently be applied to both fully connected networks and
convolutional networks, achieving positive results. In complex combustion
acceleration tasks, we reduced the size of the neural network to 41.7% of its
original scale while maintaining prediction accuracy. In the CIFAR10 image
classification task, we reduced the network size to 11.5% of the original
scale, still maintaining a satisfactory validation accuracy. Our method can be
applied to most trained neural networks, reducing computational pressure and
improving inference speed.","Tianyi Chen, Zhi-Qin John Xu",2024,http://arxiv.org/abs/2405.01041v2
The empirical size of trained neural networks,"ReLU neural networks define piecewise linear functions of their inputs.
However, initializing and training a neural network is very different from
fitting a linear spline. In this paper, we expand empirically upon previous
theoretical work to demonstrate features of trained neural networks. Standard
network initialization and training produce networks vastly simpler than a
naive parameter count would suggest and can impart odd features to the trained
network. However, we also show the forced simplicity is beneficial and, indeed,
critical for the wide success of these networks.","Kevin K. Chen, Anthony Gamst, Alden Walker",2016,http://arxiv.org/abs/1611.09444v1
An Improved Neural Segmentation Method Based on U-NET,"Neural segmentation has a great impact on the smooth implementation of local
anesthesia surgery. At present, the network for the segmentation includes U-NET
[1] and SegNet [2]. U-NET network has short training time and less training
parameters, but the depth is not deep enough. SegNet network has deeper
structure, but it needs longer training time, and more training samples. In
this paper, we propose an improved U-NET neural network for the segmentation.
This network deepens the original structure through importing residual network.
Compared with U-NET and SegNet, the improved U-NET network has fewer training
parameters, shorter training time and get a great improvement in segmentation
effect. The improved U-NET network structure has a good application scene in
neural segmentation.","Chenyang Xu, Mengxin Li",2017,http://arxiv.org/abs/1708.04747v1
Iterative Neural Networks with Bounded Weights,"A recent analysis of a model of iterative neural network in Hilbert spaces
established fundamental properties of such networks, such as existence of the
fixed points sets, convergence analysis, and Lipschitz continuity. Building on
these results, we show that under a single mild condition on the weights of the
network, one is guaranteed to obtain a neural network converging to its unique
fixed point. We provide a bound on the norm of this fixed point in terms of
norms of weights and biases of the network. We also show why this model of a
feed-forward neural network is not able to accomodate Hopfield networks under
our assumption.","Tomasz Piotrowski, Krzysztof Rykaczewski",2019,http://arxiv.org/abs/1908.05982v2
"A Correspondence Between Random Neural Networks and Statistical Field
  Theory","A number of recent papers have provided evidence that practical design
questions about neural networks may be tackled theoretically by studying the
behavior of random networks. However, until now the tools available for
analyzing random neural networks have been relatively ad-hoc. In this work, we
show that the distribution of pre-activations in random neural networks can be
exactly mapped onto lattice models in statistical physics. We argue that
several previous investigations of stochastic networks actually studied a
particular factorial approximation to the full lattice model. For random linear
networks and random rectified linear networks we show that the corresponding
lattice models in the wide network limit may be systematically approximated by
a Gaussian distribution with covariance between the layers of the network. In
each case, the approximate distribution can be diagonalized by Fourier
transformation. We show that this approximation accurately describes the
results of numerical simulations of wide random neural networks. Finally, we
demonstrate that in each case the large scale behavior of the random networks
can be approximated by an effective field theory.","Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein",2017,http://arxiv.org/abs/1710.06570v1
Expediting Neural Network Verification via Network Reduction,"A wide range of verification methods have been proposed to verify the safety
properties of deep neural networks ensuring that the networks function
correctly in critical applications. However, many well-known verification tools
still struggle with complicated network architectures and large network sizes.
In this work, we propose a network reduction technique as a pre-processing
method prior to verification. The proposed method reduces neural networks via
eliminating stable ReLU neurons, and transforming them into a sequential neural
network consisting of ReLU and Affine layers which can be handled by the most
verification tools. We instantiate the reduction technique on the
state-of-the-art complete and incomplete verification tools, including
alpha-beta-crown, VeriNet and PRIMA. Our experiments on a large set of
benchmarks indicate that the proposed technique can significantly reduce neural
networks and speed up existing verification tools. Furthermore, the experiment
results also show that network reduction can improve the availability of
existing verification tools on many networks by reducing them into sequential
neural networks.","Yuyi Zhong, Ruiwei Wang, Siau-Cheng Khoo",2023,http://arxiv.org/abs/2308.03330v2
On Study of the Binarized Deep Neural Network for Image Classification,"Recently, the deep neural network (derived from the artificial neural
network) has attracted many researchers' attention by its outstanding
performance. However, since this network requires high-performance GPUs and
large storage, it is very hard to use it on individual devices. In order to
improve the deep neural network, many trials have been made by refining the
network structure or training strategy. Unlike those trials, in this paper, we
focused on the basic propagation function of the artificial neural network and
proposed the binarized deep neural network. This network is a pure binary
system, in which all the values and calculations are binarized. As a result,
our network can save a lot of computational resource and storage. Therefore, it
is possible to use it on various devices. Moreover, the experimental results
proved the feasibility of the proposed network.","Song Wang, Dongchun Ren, Li Chen, Wei Fan, Jun Sun, Satoshi Naoi",2016,http://arxiv.org/abs/1602.07373v1
"Graph Neural Networks and Reinforcement Learning for Behavior Generation
  in Semantic Environments","Most reinforcement learning approaches used in behavior generation utilize
vectorial information as input. However, this requires the network to have a
pre-defined input-size -- in semantic environments this means assuming the
maximum number of vehicles. Additionally, this vectorial representation is not
invariant to the order and number of vehicles. To mitigate the above-stated
disadvantages, we propose combining graph neural networks with actor-critic
reinforcement learning. As graph neural networks apply the same network to
every vehicle and aggregate incoming edge information, they are invariant to
the number and order of vehicles. This makes them ideal candidates to be used
as networks in semantic environments -- environments consisting of objects
lists. Graph neural networks exhibit some other advantages that make them
favorable to be used in semantic environments. The relational information is
explicitly given and does not have to be inferred. Moreover, graph neural
networks propagate information through the network and can gather higher-degree
information. We demonstrate our approach using a highway lane-change scenario
and compare the performance of graph neural networks to conventional ones. We
show that graph neural networks are capable of handling scenarios with a
varying number and order of vehicles during training and application.","Patrick Hart, Alois Knoll",2020,http://arxiv.org/abs/2006.12576v1
"The cross-sectional stock return predictions via quantum neural network
  and tensor network","In this paper, we investigate the application of quantum and quantum-inspired
machine learning algorithms to stock return predictions. Specifically, we
evaluate the performance of quantum neural network, an algorithm suited for
noisy intermediate-scale quantum computers, and tensor network, a
quantum-inspired machine learning algorithm, against classical models such as
linear regression and neural networks. To evaluate their abilities, we
construct portfolios based on their predictions and measure investment
performances. The empirical study on the Japanese stock market shows the tensor
network model achieves superior performance compared to classical benchmark
models, including linear and neural network models. Though the quantum neural
network model attains a lowered risk-adjusted excess return than the classical
neural network models over the whole period, both the quantum neural network
and tensor network models have superior performances in the latest market
environment, which suggests the capability of the model's capturing
non-linearity between input features.","Nozomu Kobayashi, Yoshiyuki Suimon, Koichi Miyamoto, Kosuke Mitarai",2023,http://arxiv.org/abs/2304.12501v2
Convexified Convolutional Neural Networks,"We describe the class of convexified convolutional neural networks (CCNNs),
which capture the parameter sharing of convolutional neural networks in a
convex manner. By representing the nonlinear convolutional filters as vectors
in a reproducing kernel Hilbert space, the CNN parameters can be represented as
a low-rank matrix, which can be relaxed to obtain a convex optimization
problem. For learning two-layer convolutional neural networks, we prove that
the generalization error obtained by a convexified CNN converges to that of the
best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise
manner. Empirically, CCNNs achieve performance competitive with CNNs trained by
backpropagation, SVMs, fully-connected neural networks, stacked denoising
auto-encoders, and other baseline methods.","Yuchen Zhang, Percy Liang, Martin J. Wainwright",2016,http://arxiv.org/abs/1609.01000v1
Rational neural networks,"We consider neural networks with rational activation functions. The choice of
the nonlinear activation function in deep learning architectures is crucial and
heavily impacts the performance of a neural network. We establish optimal
bounds in terms of network complexity and prove that rational neural networks
approximate smooth functions more efficiently than ReLU networks with
exponentially smaller depth. The flexibility and smoothness of rational
activation functions make them an attractive alternative to ReLU, as we
demonstrate with numerical experiments.","Nicolas Boullé, Yuji Nakatsukasa, Alex Townsend",2020,http://arxiv.org/abs/2004.01902v2
"Coarse and fine-grained automatic cropping deep convolutional neural
  network","The existing convolutional neural network pruning algorithms can be divided
into two categories: coarse-grained clipping and fine-grained clipping. This
paper proposes a coarse and fine-grained automatic pruning algorithm, which can
achieve more efficient and accurate compression acceleration for convolutional
neural networks. First, cluster the intermediate feature maps of the
convolutional neural network to obtain the network structure after
coarse-grained clipping, and then use the particle swarm optimization algorithm
to iteratively search and optimize the structure. Finally, the optimal network
tailoring substructure is obtained.",Jingfei Chang,2020,http://arxiv.org/abs/2010.06379v2
An Energy-Based View of Graph Neural Networks,"Graph neural networks are a popular variant of neural networks that work with
graph-structured data. In this work, we consider combining graph neural
networks with the energy-based view of Grathwohl et al. (2019) with the aim of
obtaining a more robust classifier. We successfully implement this framework by
proposing a novel method to ensure generation over features as well as the
adjacency matrix and evaluate our method against the standard graph
convolutional network (GCN) architecture (Kipf & Welling (2016)). Our approach
obtains comparable discriminative performance while improving robustness,
opening promising new directions for future research for energy-based graph
neural networks.","John Y. Shin, Prathamesh Dharangutte",2021,http://arxiv.org/abs/2104.13492v2
Binary Multi Channel Morphological Neural Network,"Neural networks and particularly Deep learning have been comparatively little
studied from the theoretical point of view. Conversely, Mathematical Morphology
is a discipline with solid theoretical foundations. We combine these domains to
propose a new type of neural architecture that is theoretically more
explainable. We introduce a Binary Morphological Neural Network (BiMoNN) built
upon the convolutional neural network. We design it for learning morphological
networks with binary inputs and outputs. We demonstrate an equivalence between
BiMoNNs and morphological operators that we can use to binarize entire
networks. These can learn classical morphological operators and show promising
results on a medical imaging application.","Theodore Aouad, Hugues Talbot",2022,http://arxiv.org/abs/2204.08768v1
"A Derivation of Feedforward Neural Network Gradients Using Fréchet
  Calculus","We present a derivation of the gradients of feedforward neural networks using
Fr\'echet calculus which is arguably more compact than the ones usually
presented in the literature. We first derive the gradients for ordinary neural
networks working on vectorial data and show how these derived formulas can be
used to derive a simple and efficient algorithm for calculating a neural
networks gradients. Subsequently we show how our analysis generalizes to more
general neural network architectures including, but not limited to,
convolutional networks.",Thomas Hamm,2022,http://arxiv.org/abs/2209.13234v1
Sparsity-depth Tradeoff in Infinitely Wide Deep Neural Networks,"We investigate how sparse neural activity affects the generalization
performance of a deep Bayesian neural network at the large width limit. To this
end, we derive a neural network Gaussian Process (NNGP) kernel with rectified
linear unit (ReLU) activation and a predetermined fraction of active neurons.
Using the NNGP kernel, we observe that the sparser networks outperform the
non-sparse networks at shallow depths on a variety of datasets. We validate
this observation by extending the existing theory on the generalization error
of kernel-ridge regression.","Chanwoo Chun, Daniel D. Lee",2023,http://arxiv.org/abs/2305.10550v1
TASI Lectures on Physics for Machine Learning,"These notes are based on lectures I gave at TASI 2024 on Physics for Machine
Learning. The focus is on neural network theory, organized according to network
expressivity, statistics, and dynamics. I present classic results such as the
universal approximation theorem and neural network / Gaussian process
correspondence, and also more recent results such as the neural tangent kernel,
feature learning with the maximal update parameterization, and
Kolmogorov-Arnold networks. The exposition on neural network theory emphasizes
a field theoretic perspective familiar to theoretical physicists. I elaborate
on connections between the two, including a neural network approach to field
theory.",Jim Halverson,2024,http://arxiv.org/abs/2408.00082v1
"Integrating Deep Neural Networks with Full-waveform Inversion:
  Reparametrization, Regularization, and Uncertainty Quantification","Full-waveform inversion (FWI) is an accurate imaging approach for modeling
velocity structure by minimizing the misfit between recorded and predicted
seismic waveforms. However, the strong non-linearity of FWI resulting from
fitting oscillatory waveforms can trap the optimization in local minima. We
propose a neural-network-based full waveform inversion method (NNFWI) that
integrates deep neural networks with FWI by representing the velocity model
with a generative neural network. Neural networks can naturally introduce
spatial correlations as regularization to the generated velocity model, which
suppresses noise in the gradients and mitigates local minima. The velocity
model generated by neural networks is input to the same partial differential
equation (PDE) solvers used in conventional FWI. The gradients of both the
neural networks and PDEs are calculated using automatic differentiation, which
back-propagates gradients through the acoustic PDEs and neural network layers
to update the weights of the generative neural network. Experiments on 1D
velocity models, the Marmousi model, and the 2004 BP model demonstrate that
NNFWI can mitigate local minima, especially for imaging high-contrast features
like salt bodies, and significantly improves the inversion in the presence of
noise. Adding dropout layers to the neural network model also allows analyzing
the uncertainty of the inversion results through Monte Carlo dropout. NNFWI
opens a new pathway to combine deep learning and FWI for exploiting both the
characteristics of deep neural networks and the high accuracy of PDE solvers.
Because NNFWI does not require extra training data and optimization loops, it
provides an attractive and straightforward alternative to conventional FWI.","Weiqiang Zhu, Kailai Xu, Eric Darve, Biondo Biondi, Gregory C. Beroza",2020,http://arxiv.org/abs/2012.11149v3
Understanding and mitigating noise in trained deep neural networks,"Deep neural networks unlocked a vast range of new applications by solving
tasks of which many were previously deemed as reserved to higher human
intelligence. One of the developments enabling this success was a boost in
computing power provided by special purpose hardware, such as graphic or tensor
processing units. However, these do not leverage fundamental features of neural
networks like parallelism and analog state variables. Instead, they emulate
neural networks relying on binary computing, which results in unsustainable
energy consumption and comparatively low speed. Fully parallel and analogue
hardware promises to overcome these challenges, yet the impact of analogue
neuron noise and its propagation, i.e. accumulation, threatens rendering such
approaches inept. Here, we determine for the first time the propagation of
noise in deep neural networks comprising noisy nonlinear neurons in trained
fully connected layers. We study additive and multiplicative as well as
correlated and uncorrelated noise, and develop analytical methods that predict
the noise level in any layer of symmetric deep neural networks or deep neural
networks trained with back propagation. We find that noise accumulation is
generally bound, and adding additional network layers does not worsen the
signal to noise ratio beyond a limit. Most importantly, noise accumulation can
be suppressed entirely when neuron activation functions have a slope smaller
than unity. We therefore developed the framework for noise in fully connected
deep neural networks implemented in analog systems, and identify criteria
allowing engineers to design noise-resilient novel neural network hardware.","Nadezhda Semenova, Laurent Larger, Daniel Brunner",2021,http://arxiv.org/abs/2103.07413v3
Neural Networks on Groups,"Although neural networks traditionally are typically used to approximate
functions defined over $\mathbb{R}^n$, the successes of graph neural networks,
point-cloud neural networks, and manifold deep learning among other methods
have demonstrated the clear value of leveraging neural networks to approximate
functions defined over more general spaces. The theory of neural networks has
not kept up however,and the relevant theoretical results (when they exist at
all) have been proven on a case-by-case basis without a general theory or
connection to classical work. The process of deriving new theoretical backing
for each new type of network has become a bottleneck to understanding and
validating new approaches.
  In this paper we extend the definition of neural networks to general
topological groups and prove that neural networks with a single hidden layer
and a bounded non-constant activation function can approximate any
$\mathcal{L}^p$ function defined over any locally compact Abelian group. This
framework and universal approximation theorem encompass all of the
aforementioned contexts. We also derive important corollaries and extensions
with minor modification, including the case for approximating continuous
functions on a compact subset, neural networks with ReLU activation functions
on a linearly bi-ordered group, and neural networks with affine transformations
on a vector space. Our work obtains as special cases the recent theorems of Qi
et al. [2017], Sennai et al. [2019], Keriven and Peyre [2019], and Maron et al.
[2019]",Stella Rose Biderman,2019,http://arxiv.org/abs/1907.03742v2
"Guaranteeing Safety for Neural Network-Based Aircraft Collision
  Avoidance Systems","The decision logic for the ACAS X family of aircraft collision avoidance
systems is represented as a large numeric table. Due to storage constraints of
certified avionics hardware, neural networks have been suggested as a way to
significantly compress the data while still preserving performance in terms of
safety. However, neural networks are complex continuous functions with outputs
that are difficult to predict. Because simulations evaluate only a finite
number of encounters, simulations are not sufficient to guarantee that the
neural network will perform correctly in all possible situations. We propose a
method to provide safety guarantees when using a neural network collision
avoidance system. The neural network outputs are bounded using neural network
verification tools like Reluplex and Reluval, and a reachability method
determines all possible ways aircraft encounters will resolve using neural
network advisories and assuming bounded aircraft dynamics. Experiments with
systems inspired by ACAS X show that neural networks giving either horizontal
or vertical maneuvers can be proven safe. We explore how relaxing the bounds on
aircraft dynamics can lead to potentially unsafe encounters and demonstrate how
neural network controllers can be modified to guarantee safety through online
costs or lowering alerting cost. The reachability method is flexible and can
incorporate uncertainties such as pilot delay and sensor error. These results
suggest a method for certifying neural network collision avoidance systems for
use in real aircraft.","Kyle D. Julian, Mykel J. Kochenderfer",2019,http://arxiv.org/abs/1912.07084v2
"A$^3$: Accelerating Attention Mechanisms in Neural Networks with
  Approximation","With the increasing computational demands of neural networks, many hardware
accelerators for the neural networks have been proposed. Such existing neural
network accelerators often focus on popular neural network types such as
convolutional neural networks (CNNs) and recurrent neural networks (RNNs);
however, not much attention has been paid to attention mechanisms, an emerging
neural network primitive that enables neural networks to retrieve most relevant
information from a knowledge-base, external memory, or past states. The
attention mechanism is widely adopted by many state-of-the-art neural networks
for computer vision, natural language processing, and machine translation, and
accounts for a large portion of total execution time. We observe today's
practice of implementing this mechanism using matrix-vector multiplication is
suboptimal as the attention mechanism is semantically a content-based search
where a large portion of computations ends up not being used. Based on this
observation, we design and architect A3, which accelerates attention mechanisms
in neural networks with algorithmic approximation and hardware specialization.
Our proposed accelerator achieves multiple orders of magnitude improvement in
energy efficiency (performance/watt) as well as substantial speedup over the
state-of-the-art conventional hardware.","Tae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H. Oh, Yeonhong Park, Yoonho Song, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae W. Lee, Deog-Kyoon Jeong",2020,http://arxiv.org/abs/2002.10941v1
HyBNN and FedHyBNN: (Federated) Hybrid Binary Neural Networks,"Binary Neural Networks (BNNs), neural networks with weights and activations
constrained to -1(0) and +1, are an alternative to deep neural networks which
offer faster training, lower memory consumption and lightweight models, ideal
for use in resource constrained devices while being able to utilize the
architecture of their deep neural network counterpart. However, the input
binarization step used in BNNs causes a severe accuracy loss. In this paper, we
introduce a novel hybrid neural network architecture, Hybrid Binary Neural
Network (HyBNN), consisting of a task-independent, general, full-precision
variational autoencoder with a binary latent space and a task specific binary
neural network that is able to greatly limit the accuracy loss due to input
binarization by using the full precision variational autoencoder as a feature
extractor. We use it to combine the state-of-the-art accuracy of deep neural
networks with the much faster training time, quicker test-time inference and
power efficiency of binary neural networks. We show that our proposed system is
able to very significantly outperform a vanilla binary neural network with
input binarization. We also introduce FedHyBNN, a highly communication
efficient federated counterpart to HyBNN and demonstrate that it is able to
reach the same accuracy as its non-federated equivalent. We make our source
code, experimental parameters and models available at:
https://anonymous.4open.science/r/HyBNN.",Kinshuk Dua,2022,http://arxiv.org/abs/2205.09839v1
Rule Extraction Algorithm for Deep Neural Networks: A Review,"Despite the highest classification accuracy in wide varieties of application
areas, artificial neural network has one disadvantage. The way this Network
comes to a decision is not easily comprehensible. The lack of explanation
ability reduces the acceptability of neural network in data mining and decision
system. This drawback is the reason why researchers have proposed many rule
extraction algorithms to solve the problem. Recently, Deep Neural Network (DNN)
is achieving a profound result over the standard neural network for
classification and recognition problems. It is a hot machine learning area
proven both useful and innovative. This paper has thoroughly reviewed various
rule extraction algorithms, considering the classification scheme:
decompositional, pedagogical, and eclectics. It also presents the evaluation of
these algorithms based on the neural network structure with which the algorithm
is intended to work. The main contribution of this review is to show that there
is a limited study of rule extraction algorithm from DNN.",Tameru Hailesilassie,2016,http://arxiv.org/abs/1610.05267v1
Environmental Sound Recognition using Masked Conditional Neural Networks,"Neural network based architectures used for sound recognition are usually
adapted from other application domains, which may not harness sound related
properties. The ConditionaL Neural Network (CLNN) is designed to consider the
relational properties across frames in a temporal signal, and its extension the
Masked ConditionaL Neural Network (MCLNN) embeds a filterbank behavior within
the network, which enforces the network to learn in frequency bands rather than
bins. Additionally, it automates the exploration of different feature
combinations analogous to handcrafting the optimum combination of features for
a recognition task. We applied the MCLNN to the environmental sounds of the
ESC-10 dataset. The MCLNN achieved competitive accuracies compared to
state-of-the-art convolutional neural networks and hand-crafted attempts.","Fady Medhat, David Chesmore, John Robinson",2018,http://arxiv.org/abs/1804.02665v2
A Taxonomy for Neural Memory Networks,"In this paper, a taxonomy for memory networks is proposed based on their
memory organization. The taxonomy includes all the popular memory networks:
vanilla recurrent neural network (RNN), long short term memory (LSTM ), neural
stack and neural Turing machine and their variants. The taxonomy puts all these
networks under a single umbrella and shows their relative expressive power ,
i.e. vanilla RNN <=LSTM<=neural stack<=neural RAM. The differences and
commonality between these networks are analyzed. These differences are also
connected to the requirements of different tasks which can give the user
instructions of how to choose or design an appropriate memory network for a
specific task. As a conceptual simplified class of problems, four tasks of
synthetic symbol sequences: counting, counting with interference, reversing and
repeat counting are developed and tested to verify our arguments. And we use
two natural language processing problems to discuss how this taxonomy helps
choosing the appropriate neural memory networks for real world problem.","Ying Ma, Jose Principe",2018,http://arxiv.org/abs/1805.00327v1
Theoretical Investigation of Composite Neural Network,"This work theoretically investigates the performance of a composite neural
network. A composite neural network is a rooted directed acyclic graph
combining a set of pre-trained and non-instantiated neural network models,
where a pre-trained neural network model is well-crafted for a specific task
and targeted to approximate a specific function with instantiated weights. The
advantages of adopting such a pre-trained model in a composite neural network
are two folds. One is to benefit from other's intelligence and diligence, and
the other is saving the efforts in data preparation and resources and time in
training. However, the overall performance of composite neural network is still
not clear. In this work, we prove that a composite neural network, with high
probability, performs better than any of its pre-trained components under
certain assumptions. In addition, if an extra pre-trained component is added to
a composite network, with high probability the overall performance will be
improved. In the empirical evaluations, distinctively different applications
support the above findings.","Ming-Chuan Yang, Meng Chang Chen",2019,http://arxiv.org/abs/1910.09351v2
"Measurement error models: from nonparametric methods to deep neural
  networks","The success of deep learning has inspired recent interests in applying neural
networks in statistical inference. In this paper, we investigate the use of
deep neural networks for nonparametric regression with measurement errors. We
propose an efficient neural network design for estimating measurement error
models, in which we use a fully connected feed-forward neural network (FNN) to
approximate the regression function $f(x)$, a normalizing flow to approximate
the prior distribution of $X$, and an inference network to approximate the
posterior distribution of $X$. Our method utilizes recent advances in
variational inference for deep neural networks, such as the importance weight
autoencoder, doubly reparametrized gradient estimator, and non-linear
independent components estimation. We conduct an extensive numerical study to
compare the neural network approach with classical nonparametric methods and
observe that the neural network approach is more flexible in accommodating
different classes of regression functions and performs superior or comparable
to the best available method in nearly all settings.","Zhirui Hu, Zheng Tracy Ke, Jun S Liu",2020,http://arxiv.org/abs/2007.07498v1
"Neural tensor contractions and the expressive power of deep neural
  quantum states","We establish a direct connection between general tensor networks and deep
feed-forward artificial neural networks. The core of our results is the
construction of neural-network layers that efficiently perform tensor
contractions, and that use commonly adopted non-linear activation functions.
The resulting deep networks feature a number of edges that closely matches the
contraction complexity of the tensor networks to be approximated. In the
context of many-body quantum states, this result establishes that
neural-network states have strictly the same or higher expressive power than
practically usable variational tensor networks. As an example, we show that all
matrix product states can be efficiently written as neural-network states with
a number of edges polynomial in the bond dimension and depth logarithmic in the
system size. The opposite instead does not hold true, and our results imply
that there exist quantum states that are not efficiently expressible in terms
of matrix product states or PEPS, but that are instead efficiently expressible
with neural network states.","Or Sharir, Amnon Shashua, Giuseppe Carleo",2021,http://arxiv.org/abs/2103.10293v3
Detecting Gas Vapor Leaks Using Uncalibrated Sensors,"Chemical and infra-red sensors generate distinct responses under similar
conditions because of sensor drift, noise or resolution errors. In this work,
we use different time-series data sets obtained by infra-red and E-nose sensors
in order to detect Volatile Organic Compounds (VOCs) and Ammonia vapor leaks.
We process time-series sensor signals using deep neural networks (DNN). Three
neural network algorithms are utilized for this purpose. Additive neural
networks (termed AddNet) are based on a multiplication-devoid operator and
consequently exhibit energy-efficiency compared to regular neural networks. The
second algorithm uses generative adversarial neural networks so as to expose
the classifying neural network to more realistic data points in order to help
the classifier network to deliver improved generalization. Finally, we use
conventional convolutional neural networks as a baseline method and compare
their performance with the two aforementioned deep neural network algorithms in
order to evaluate their effectiveness empirically.","Diaa Badawi, Tuba Ayhan, Sule Ozev, Chengmo Yang, Alex Orailoglu, A. Enis Çetin",2019,http://arxiv.org/abs/1908.07619v1
Neural Networks as Geometric Chaotic Maps,"The use of artificial neural networks as models of chaotic dynamics has been
rapidly expanding. Still, a theoretical understanding of how neural networks
learn chaos is lacking. Here, we employ a geometric perspective to show that
neural networks can efficiently model chaotic dynamics by becoming structurally
chaotic themselves. We first confirm neural network's efficiency in emulating
chaos by showing that a parsimonious neural network trained only on few data
points can reconstruct strange attractors, extrapolate outside training data
boundaries, and accurately predict local divergence rates. We then posit that
the trained network's map comprises sequential geometric stretching, rotation,
and compression operations. These geometric operations indicate topological
mixing and chaos, explaining why neural networks are naturally suitable to
emulate chaotic dynamics.","Ziwei Li, Sai Ravela",2019,http://arxiv.org/abs/1912.05081v4
"Deep Spiking Convolutional Neural Network for Single Object Localization
  Based On Deep Continuous Local Learning","With the advent of neuromorphic hardware, spiking neural networks can be a
good energy-efficient alternative to artificial neural networks. However, the
use of spiking neural networks to perform computer vision tasks remains
limited, mainly focusing on simple tasks such as digit recognition. It remains
hard to deal with more complex tasks (e.g. segmentation, object detection) due
to the small number of works on deep spiking neural networks for these tasks.
The objective of this paper is to make the first step towards modern computer
vision with supervised spiking neural networks. We propose a deep convolutional
spiking neural network for the localization of a single object in a grayscale
image. We propose a network based on DECOLLE, a spiking model that enables
local surrogate gradient-based learning. The encouraging results reported on
Oxford-IIIT-Pet validates the exploitation of spiking neural networks with a
supervised learning approach for more elaborate vision tasks in the future.","Sami Barchid, José Mennesson, Chaabane Djéraba",2021,http://arxiv.org/abs/2105.05609v1
Experimentally Realizable Continuous-variable Quantum Neural Networks,"Continuous-variable (CV) quantum computing has shown great potential for
building neural network models. These neural networks can have different levels
of quantum-classical hybridization depending on the complexity of the problem.
Previous work on CV neural network protocols required the implementation of
non-Gaussian operators in the network. These operators were used to introduce
non-linearity, an essential feature of neural networks. However, these
protocols are hard to execute experimentally. We built a CV hybrid
quantum-classical neural network protocol that can be realized experimentally
with current photonic quantum hardware. Our protocol uses Gaussian gates only
with the addition of ancillary qumodes. We implemented non-linearity through
repeat-until-success measurements on ancillary qumodes. To test our neural
network, we studied canonical machine learning and quantum computer problems in
a supervised learning setting -- state preparation, curve fitting, and
classification problems. We achieved high fidelity in state preparation of
single-photon (99.9%), cat (99.8%), and Gottesman-Kitaev-Preskill (93.9%)
states, a well-fitted curve in the presence of noise at a cost of less than 1%,
and more than 95% accuracy in classification problems. These results bode well
for real-world applications of CV quantum neural networks.","Shikha Bangar, Leanto Sunny, Kubra Yeter-Aydeniz, George Siopsis",2023,http://arxiv.org/abs/2306.02525v2
A Comparison of Neural Networks for Wireless Channel Prediction,"The performance of modern wireless communications systems depends critically
on the quality of the available channel state information (CSI) at the
transmitter and receiver. Several previous works have proposed concepts and
algorithms that help maintain high quality CSI even in the presence of high
mobility and channel aging, such as temporal prediction schemes that employ
neural networks. However, it is still unclear which neural network-based scheme
provides the best performance in terms of prediction quality, training
complexity and practical feasibility. To investigate such a question, this
paper first provides an overview of state-of-the-art neural networks applicable
to channel prediction and compares their performance in terms of prediction
quality. Next, a new comparative analysis is proposed for four promising neural
networks with different prediction horizons. The well-known tapped delay
channel model recommended by the Third Generation Partnership Program is used
for a standardized comparison among the neural networks. Based on this
comparative evaluation, the advantages and disadvantages of each neural network
are discussed and guidelines for selecting the best-suited neural network in
channel prediction applications are given.","Oscar Stenhammar, Gabor Fodor, Carlo Fischione",2023,http://arxiv.org/abs/2308.14020v1
Hybrid deep additive neural networks,"Traditional neural networks (multi-layer perceptrons) have become an
important tool in data science due to their success across a wide range of
tasks. However, their performance is sometimes unsatisfactory, and they often
require a large number of parameters, primarily due to their reliance on the
linear combination structure. Meanwhile, additive regression has been a popular
alternative to linear regression in statistics. In this work, we introduce
novel deep neural networks that incorporate the idea of additive regression.
Our neural networks share architectural similarities with Kolmogorov-Arnold
networks but are based on simpler yet flexible activation and basis functions.
Additionally, we introduce several hybrid neural networks that combine this
architecture with that of traditional neural networks. We derive their
universal approximation properties and demonstrate their effectiveness through
simulation studies and a real-data application. The numerical results indicate
that our neural networks generally achieve better performance than traditional
neural networks while using fewer parameters.","Gyu Min Kim, Jeong Min Jeon",2024,http://arxiv.org/abs/2411.09175v2
"Predicting Hidden Links and Missing Nodes in Scale-Free Networks with
  Artificial Neural Networks","There are many networks in real life which exist as form of Scale-free
networks such as World Wide Web, protein-protein inter action network, semantic
networks, airline networks, interbank payment networks, etc. If we want to
analyze these networks, it is really necessary to understand the properties of
scale-free networks. By using the properties of scale free networks, we can
identify any type of anomalies in those networks. In this research, we proposed
a methodology in a form of an algorithm to predict hidden links and missing
nodes in scale-free networks where we combined a generator of random networks
as a source of train data, on one hand, with artificial neural networks for
supervised classification, on the other, we aimed at training the neural
networks to discriminate between different subtypes of scale-free networks and
predicted the missing nodes and hidden links among (present and missing) nodes
in a given scale-free network. We chose Bela Bollobas's directed scale-free
random graph generation algorithm as a generator of random networks to generate
a large set of scale-free network's data.","Rakib Hassan Pran, Ljupco Todorovski",2021,http://arxiv.org/abs/2109.12331v1
A Probabilistic Approach to Neural Network Pruning,"Neural network pruning techniques reduce the number of parameters without
compromising predicting ability of a network. Many algorithms have been
developed for pruning both over-parameterized fully-connected networks (FCNs)
and convolutional neural networks (CNNs), but analytical studies of
capabilities and compression ratios of such pruned sub-networks are lacking. We
theoretically study the performance of two pruning techniques (random and
magnitude-based) on FCNs and CNNs. Given a target network {whose weights are
independently sampled from appropriate distributions}, we provide a universal
approach to bound the gap between a pruned and the target network in a
probabilistic sense. The results establish that there exist pruned networks
with expressive power within any specified bound from the target network.","Xin Qian, Diego Klabjan",2021,http://arxiv.org/abs/2105.10065v1
Effect of dilution in asymmetric recurrent neural networks,"We study with numerical simulation the possible limit behaviors of
synchronous discrete-time deterministic recurrent neural networks composed of N
binary neurons as a function of a network's level of dilution and asymmetry.
The network dilution measures the fraction of neuron couples that are
connected, and the network asymmetry measures to what extent the underlying
connectivity matrix is asymmetric. For each given neural network, we study the
dynamical evolution of all the different initial conditions, thus
characterizing the full dynamical landscape without imposing any learning rule.
Because of the deterministic dynamics, each trajectory converges to an
attractor, that can be either a fixed point or a limit cycle. These attractors
form the set of all the possible limit behaviors of the neural network. For
each network, we then determine the convergence times, the limit cycles'
length, the number of attractors, and the sizes of the attractors' basin. We
show that there are two network structures that maximize the number of possible
limit behaviors. The first optimal network structure is fully-connected and
symmetric. On the contrary, the second optimal network structure is highly
sparse and asymmetric. The latter optimal is similar to what observed in
different biological neuronal circuits. These observations lead us to
hypothesize that independently from any given learning model, an efficient and
effective biologic network that stores a number of limit behaviors close to its
maximum capacity tends to develop a connectivity structure similar to one of
the optimal networks we found.","Viola Folli, Giorgio Gosti, Marco Leonetti, Giancarlo Ruocco",2018,http://arxiv.org/abs/1805.03886v1
k-Winners-Take-All Ensemble Neural Network,"Ensembling is one approach that improves the performance of a neural network
by combining a number of independent neural networks, usually by either
averaging or summing up their individual outputs. We modify this ensembling
approach by training the sub-networks concurrently instead of independently.
This concurrent training of sub-networks leads them to cooperate with each
other, and we refer to them as ""cooperative ensemble"". Meanwhile, the
mixture-of-experts approach improves a neural network performance by dividing
up a given dataset to its sub-networks. It then uses a gating network that
assigns a specialization to each of its sub-networks called ""experts"". We
improve on these aforementioned ways for combining a group of neural networks
by using a k-Winners-Take-All (kWTA) activation function, that acts as the
combination method for the outputs of each sub-network in the ensemble. We
refer to this proposed model as ""kWTA ensemble neural networks"" (kWTA-ENN).
With the kWTA activation function, the losing neurons of the sub-networks are
inhibited while the winning neurons are retained. This results in sub-networks
having some form of specialization but also sharing knowledge with one another.
We compare our approach with the cooperative ensemble and mixture-of-experts,
where we used a feed-forward neural network with one hidden layer having 100
neurons as the sub-network architecture. Our approach yields a better
performance compared to the baseline models, reaching the following test
accuracies on benchmark datasets: 98.34% on MNIST, 88.06% on Fashion-MNIST,
91.56% on KMNIST, and 95.97% on WDBC.","Abien Fred Agarap, Arnulfo P. Azcarraga",2024,http://arxiv.org/abs/2401.02092v1
Hierarchical Graph Neural Networks,"Over the recent years, Graph Neural Networks have become increasingly popular
in network analytic and beyond. With that, their architecture noticeable
diverges from the classical multi-layered hierarchical organization of the
traditional neural networks. At the same time, many conventional approaches in
network science efficiently utilize the hierarchical approaches to account for
the hierarchical organization of the networks, and recent works emphasize their
critical importance. This paper aims to connect the dots between the
traditional Neural Network and the Graph Neural Network architectures as well
as the network science approaches, harnessing the power of the hierarchical
network organization. A Hierarchical Graph Neural Network architecture is
proposed, supplementing the original input network layer with the hierarchy of
auxiliary network layers and organizing the computational scheme updating the
node features through both - horizontal network connections within each layer
as well as the vertical connection between the layers. It enables simultaneous
learning of the individual node features along with the aggregated network
features at variable resolution and uses them to improve the convergence and
stability of the individual node feature learning. The proposed Hierarchical
Graph Neural network architecture is successfully evaluated on the network
embedding and modeling as well as network classification, node labeling, and
community tasks and demonstrates increased efficiency in those.",Stanislav Sobolevsky,2021,http://arxiv.org/abs/2105.03388v2
"Automatic Organization of Neural Modules for Enhanced Collaboration in
  Neural Networks","This work proposes a new perspective on the structure of Neural Networks
(NNs). Traditional Neural Networks are typically tree-like structures for
convenience, which can be predefined or learned by NAS methods. However, such a
structure can not facilitate communications between nodes at the same level or
signal transmissions to previous levels. These defects prevent effective
collaboration, restricting the capabilities of neural networks. It is
well-acknowledged that the biological neural system contains billions of neural
units. Their connections are far more complicated than the current NN
structure. To enhance the representational ability of neural networks, existing
works try to increase the depth of the neural network and introduce more
parameters. However, they all have limitations with constrained parameters. In
this work, we introduce a synchronous graph-based structure to establish a
novel way of organizing the neural units: the Neural Modules. This framework
allows any nodes to communicate with each other and encourages neural units to
work collectively, demonstrating a departure from the conventional constrained
paradigm. Such a structure also provides more candidates for the NAS methods.
Furthermore, we also propose an elegant regularization method to organize
neural units into multiple independent, balanced neural modules systematically.
This would be convenient for handling these neural modules in parallel.
Compared to traditional NNs, our method unlocks the potential of NNs from
tree-like structures to general graphs and makes NNs be optimized in an almost
complete set. Our approach proves adaptable to diverse tasks, offering
compatibility across various scenarios. Quantitative experimental results
substantiate the potential of our structure, indicating the improvement of NNs.","Xinshun Liu, Yizhi Fang, Yichao Jiang",2020,http://arxiv.org/abs/2005.04088v4
"Decomposing spiking neural networks with Graphical Neural Activity
  Threads","A satisfactory understanding of information processing in spiking neural
networks requires appropriate computational abstractions of neural activity.
Traditionally, the neural population state vector has been the most common
abstraction applied to spiking neural networks, but this requires artificially
partitioning time into bins that are not obviously relevant to the network
itself. We introduce a distinct set of techniques for analyzing spiking neural
networks that decomposes neural activity into multiple, disjoint, parallel
threads of activity. We construct these threads by estimating the degree of
causal relatedness between pairs of spikes, then use these estimates to
construct a directed acyclic graph that traces how the network activity evolves
through individual spikes. We find that this graph of spiking activity
naturally decomposes into disjoint connected components that overlap in space
and time, which we call Graphical Neural Activity Threads (GNATs). We provide
an efficient algorithm for finding analogous threads that reoccur in large
spiking datasets, revealing that seemingly distinct spike trains are composed
of similar underlying threads of activity, a hallmark of compositionality. The
picture of spiking neural networks provided by our GNAT analysis points to new
abstractions for spiking neural computation that are naturally adapted to the
spatiotemporally distributed dynamics of spiking neural networks.","Bradley H. Theilman, Felix Wang, Fred Rothganger, James B. Aimone",2023,http://arxiv.org/abs/2306.16684v1
Random-coupled Neural Network,"Improving the efficiency of current neural networks and modeling them in
biological neural systems have become popular research directions in recent
years. Pulse-coupled neural network (PCNN) is a well applicated model for
imitating the computation characteristics of the human brain in computer vision
and neural network fields. However, differences between the PCNN and biological
neural systems remain: limited neural connection, high computational cost, and
lack of stochastic property. In this study, random-coupled neural network
(RCNN) is proposed. It overcomes these difficulties in PCNN's neuromorphic
computing via a random inactivation process. This process randomly closes some
neural connections in the RCNN model, realized by the random inactivation
weight matrix of link input. This releases the computational burden of PCNN,
making it affordable to achieve vast neural connections. Furthermore, the image
and video processing mechanisms of RCNN are researched. It encodes constant
stimuli as periodic spike trains and periodic stimuli as chaotic spike trains,
the same as biological neural information encoding characteristics. Finally,
the RCNN is applicated to image segmentation, fusion, and pulse shape
discrimination subtasks. It is demonstrated to be robust, efficient, and highly
anti-noised, with outstanding performance in all applications mentioned above.","Haoran Liu, Mingzhe Liu, Peng Li, Jiahui Wu, Xin Jiang, Zhuo Zuo, Bingqi Liu",2024,http://arxiv.org/abs/2403.17512v1
Spatially Varying Nanophotonic Neural Networks,"The explosive growth of computation and energy cost of artificial
intelligence has spurred strong interests in new computing modalities as
potential alternatives to conventional electronic processors. Photonic
processors that execute operations using photons instead of electrons, have
promised to enable optical neural networks with ultra-low latency and power
consumption. However, existing optical neural networks, limited by the
underlying network designs, have achieved image recognition accuracy far below
that of state-of-the-art electronic neural networks. In this work, we close
this gap by embedding massively parallelized optical computation into flat
camera optics that perform neural network computation during the capture,
before recording an image on the sensor. Specifically, we harness large kernels
and propose a large-kernel spatially-varying convolutional neural network
learned via low-dimensional reparameterization techniques. We experimentally
instantiate the network with a flat meta-optical system that encompasses an
array of nanophotonic structures designed to induce angle-dependent responses.
Combined with an extremely lightweight electronic backend with approximately 2K
parameters we demonstrate a reconfigurable nanophotonic neural network reaches
72.76\% blind test classification accuracy on CIFAR-10 dataset, and, as such,
the first time, an optical neural network outperforms the first modern digital
neural network -- AlexNet (72.64\%) with 57M parameters, bringing optical
neural network into modern deep learning era.","Kaixuan Wei, Xiao Li, Johannes Froech, Praneeth Chakravarthula, James Whitehead, Ethan Tseng, Arka Majumdar, Felix Heide",2023,http://arxiv.org/abs/2308.03407v3
Towards Natural Robustness Against Adversarial Examples,"Recent studies have shown that deep neural networks are vulnerable to
adversarial examples, but most of the methods proposed to defense adversarial
examples cannot solve this problem fundamentally. In this paper, we
theoretically prove that there is an upper bound for neural networks with
identity mappings to constrain the error caused by adversarial noises. However,
in actual computations, this kind of neural network no longer holds any upper
bound and is therefore susceptible to adversarial examples. Following similar
procedures, we explain why adversarial examples can fool other deep neural
networks with skip connections. Furthermore, we demonstrate that a new family
of deep neural networks called Neural ODEs (Chen et al., 2018) holds a weaker
upper bound. This weaker upper bound prevents the amount of change in the
result from being too large. Thus, Neural ODEs have natural robustness against
adversarial examples. We evaluate the performance of Neural ODEs compared with
ResNet under three white-box adversarial attacks (FGSM, PGD, DI2-FGSM) and one
black-box adversarial attack (Boundary Attack). Finally, we show that the
natural robustness of Neural ODEs is even better than the robustness of neural
networks that are trained with adversarial training methods, such as TRADES and
YOPO.","Haoyu Chu, Shikui Wei, Yao Zhao",2020,http://arxiv.org/abs/2012.02452v1
Dive into the Power of Neuronal Heterogeneity,"The biological neural network is a vast and diverse structure with high
neural heterogeneity. Conventional Artificial Neural Networks (ANNs) primarily
focus on modifying the weights of connections through training while modeling
neurons as highly homogenized entities and lacking exploration of neural
heterogeneity. Only a few studies have addressed neural heterogeneity by
optimizing neuronal properties and connection weights to ensure network
performance. However, this strategy impact the specific contribution of
neuronal heterogeneity. In this paper, we first demonstrate the challenges
faced by backpropagation-based methods in optimizing Spiking Neural Networks
(SNNs) and achieve more robust optimization of heterogeneous neurons in random
networks using an Evolutionary Strategy (ES). Experiments on tasks such as
working memory, continuous control, and image recognition show that neuronal
heterogeneity can improve performance, particularly in long sequence tasks.
Moreover, we find that membrane time constants play a crucial role in neural
heterogeneity, and their distribution is similar to that observed in biological
experiments. Therefore, we believe that the neglected neuronal heterogeneity
plays an essential role, providing new approaches for exploring neural
heterogeneity in biology and new ways for designing more biologically plausible
neural networks.","Guobin Shen, Dongcheng Zhao, Yiting Dong, Yang Li, Yi Zeng",2023,http://arxiv.org/abs/2305.11484v2
Addressing caveats of neural persistence with deep graph persistence,"Neural Persistence is a prominent measure for quantifying neural network
complexity, proposed in the emerging field of topological data analysis in deep
learning. In this work, however, we find both theoretically and empirically
that the variance of network weights and spatial concentration of large weights
are the main factors that impact neural persistence. Whilst this captures
useful information for linear classifiers, we find that no relevant spatial
structure is present in later layers of deep neural networks, making neural
persistence roughly equivalent to the variance of weights. Additionally, the
proposed averaging procedure across layers for deep neural networks does not
consider interaction between layers. Based on our analysis, we propose an
extension of the filtration underlying neural persistence to the whole neural
network instead of single layers, which is equivalent to calculating neural
persistence on one particular matrix. This yields our deep graph persistence
measure, which implicitly incorporates persistent paths through the network and
alleviates variance-related issues through standardisation. Code is available
at https://github.com/ExplainableML/Deep-Graph-Persistence .","Leander Girrbach, Anders Christensen, Ole Winther, Zeynep Akata, A. Sophia Koepke",2023,http://arxiv.org/abs/2307.10865v3
Neural Piecewise-Constant Delay Differential Equations,"Continuous-depth neural networks, such as the Neural Ordinary Differential
Equations (ODEs), have aroused a great deal of interest from the communities of
machine learning and data science in recent years, which bridge the connection
between deep neural networks and dynamical systems. In this article, we
introduce a new sort of continuous-depth neural network, called the Neural
Piecewise-Constant Delay Differential Equations (PCDDEs). Here, unlike the
recently proposed framework of the Neural Delay Differential Equations (DDEs),
we transform the single delay into the piecewise-constant delay(s). The Neural
PCDDEs with such a transformation, on one hand, inherit the strength of
universal approximating capability in Neural DDEs. On the other hand, the
Neural PCDDEs, leveraging the contributions of the information from the
multiple previous time steps, further promote the modeling capability without
augmenting the network dimension. With such a promotion, we show that the
Neural PCDDEs do outperform the several existing continuous-depth neural
frameworks on the one-dimensional piecewise-constant delay population dynamics
and real-world datasets, including MNIST, CIFAR10, and SVHN.","Qunxi Zhu, Yifei Shen, Dongsheng Li, Wei Lin",2022,http://arxiv.org/abs/2201.00960v1
Watermarking for Neural Radiation Fields by Invertible Neural Network,"To protect the copyright of the 3D scene represented by the neural radiation
field, the embedding and extraction of the neural radiation field watermark are
considered as a pair of inverse problems of image transformations. A scheme for
protecting the copyright of the neural radiation field is proposed using
invertible neural network watermarking, which utilizes watermarking techniques
for 2D images to achieve the protection of the 3D scene. The scheme embeds the
watermark in the training image of the neural radiation field through the
forward process in the invertible network and extracts the watermark from the
image rendered by the neural radiation field using the inverse process to
realize the copyright protection of both the neural radiation field and the 3D
scene. Since the rendering process of the neural radiation field can cause the
loss of watermark information, the scheme incorporates an image quality
enhancement module, which utilizes a neural network to recover the rendered
image and then extracts the watermark. The scheme embeds a watermark in each
training image to train the neural radiation field and enables the extraction
of watermark information from multiple viewpoints. Simulation experimental
results demonstrate the effectiveness of the method.","Wenquan Sun, Jia Liu, Weina Dong, Lifeng Chen, Ke Niu",2023,http://arxiv.org/abs/2312.02456v1
"Emergence of functional and structural properties of the head direction
  system by optimization of recurrent neural networks","Recent work suggests goal-driven training of neural networks can be used to
model neural activity in the brain. While response properties of neurons in
artificial neural networks bear similarities to those in the brain, the network
architectures are often constrained to be different. Here we ask if a neural
network can recover both neural representations and, if the architecture is
unconstrained and optimized, the anatomical properties of neural circuits. We
demonstrate this in a system where the connectivity and the functional
organization have been characterized, namely, the head direction circuits of
the rodent and fruit fly. We trained recurrent neural networks (RNNs) to
estimate head direction through integration of angular velocity. We found that
the two distinct classes of neurons observed in the head direction system, the
Compass neurons and the Shifter neurons, emerged naturally in artificial neural
networks as a result of training. Furthermore, connectivity analysis and
in-silico neurophysiology revealed structural and mechanistic similarities
between artificial networks and the head direction system. Overall, our results
show that optimization of RNNs in a goal-driven task can recapitulate the
structure and function of biological circuits, suggesting that artificial
neural networks can be used to study the brain at the level of both neural
activity and anatomical organization.","Christopher J. Cueva, Peter Y. Wang, Matthew Chin, Xue-Xin Wei",2019,http://arxiv.org/abs/1912.10189v2
Sampling and Recovery of Graph Signals based on Graph Neural Networks,"We propose interpretable graph neural networks for sampling and recovery of
graph signals, respectively. To take informative measurements, we propose a new
graph neural sampling module, which aims to select those vertices that
maximally express their corresponding neighborhoods. Such expressiveness can be
quantified by the mutual information between vertices' features and
neighborhoods' features, which are estimated via a graph neural network. To
reconstruct an original graph signal from the sampled measurements, we propose
a graph neural recovery module based on the algorithm-unrolling technique.
Compared to previous analytical sampling and recovery, the proposed methods are
able to flexibly learn a variety of graph signal models from data by leveraging
the learning ability of neural networks; compared to previous
neural-network-based sampling and recovery, the proposed methods are designed
through exploiting specific graph properties and provide interpretability. We
further design a new multiscale graph neural network, which is a trainable
multiscale graph filter bank and can handle various graph-related learning
tasks. The multiscale network leverages the proposed graph neural sampling and
recovery modules to achieve multiscale representations of a graph. In the
experiments, we illustrate the effects of the proposed graph neural sampling
and recovery modules and find that the modules can flexibly adapt to various
graph structures and graph signals. In the task of active-sampling-based
semi-supervised learning, the graph neural sampling module improves the
classification accuracy over 10% in Cora dataset. We further validate the
proposed multiscale graph neural network on several standard datasets for both
vertex and graph classification. The results show that our method consistently
improves the classification accuracies.","Siheng Chen, Maosen Li, Ya Zhang",2020,http://arxiv.org/abs/2011.01412v1
"Towards Neural Network Patching: Evaluating Engagement-Layers and
  Patch-Architectures","In this report we investigate fundamental requirements for the application of
classifier patching on neural networks. Neural network patching is an approach
for adapting neural network models to handle concept drift in nonstationary
environments. Instead of creating or updating the existing network to
accommodate concept drift, neural network patching leverages the inner layers
of the network as well as its output to learn a patch that enhances the
classification and corrects errors caused by the drift. It learns (i) a
predictor that estimates whether the original network will misclassify an
instance, and (ii) a patching network that fixes the misclassification. Neural
network patching is based on the idea that the original network can still
classify a majority of instances well, and that the inner feature
representations encoded in the deep network aid the classifier to cope with
unseen or changed inputs. In order to apply this kind of patching, we evaluate
different engagement layers and patch architectures in this report, and find a
set of generally applicable heuristics, which aid in parametrizing the patching
procedure.","Sebastian Kauschke, David Hermann Lehmann",2018,http://arxiv.org/abs/1812.03468v2
"Atom: Neural Traffic Compression with Spatio-Temporal Graph Neural
  Networks","Storing network traffic data is key to efficient network management; however,
it is becoming more challenging and costly due to the ever-increasing data
transmission rates, traffic volumes, and connected devices. In this paper, we
explore the use of neural architectures for network traffic compression.
Specifically, we consider a network scenario with multiple measurement points
in a network topology. Such measurements can be interpreted as multiple time
series that exhibit spatial and temporal correlations induced by network
topology, routing, or user behavior. We present \textit{Atom}, a neural traffic
compression method that leverages spatial and temporal correlations present in
network traffic. \textit{Atom} implements a customized spatio-temporal graph
neural network design that effectively exploits both types of correlations
simultaneously. The experimental results show that \textit{Atom} can outperform
GZIP's compression ratios by 50\%-65\% on three real-world networks.","Paul Almasan, Krzysztof Rusek, Shihan Xiao, Xiang Shi, Xiangle Cheng, Albert Cabellos-Aparicio, Pere Barlet-Ros",2023,http://arxiv.org/abs/2311.05337v1
Modular Representation of Layered Neural Networks,"Layered neural networks have greatly improved the performance of various
applications including image processing, speech recognition, natural language
processing, and bioinformatics. However, it is still difficult to discover or
interpret knowledge from the inference provided by a layered neural network,
since its internal representation has many nonlinear and complex parameters
embedded in hierarchical layers. Therefore, it becomes important to establish a
new methodology by which layered neural networks can be understood.
  In this paper, we propose a new method for extracting a global and simplified
structure from a layered neural network. Based on network analysis, the
proposed method detects communities or clusters of units with similar
connection patterns. We show its effectiveness by applying it to three use
cases. (1) Network decomposition: it can decompose a trained neural network
into multiple small independent networks thus dividing the problem and reducing
the computation time. (2) Training assessment: the appropriateness of a trained
result with a given hyperparameter or randomly chosen initial parameters can be
evaluated by using a modularity index. And (3) data analysis: in practical data
it reveals the community structure in the input, hidden, and output layers,
which serves as a clue for discovering knowledge from a trained neural network.","Chihiro Watanabe, Kaoru Hiramatsu, Kunio Kashino",2017,http://arxiv.org/abs/1703.00168v2
CompNet: Neural networks growing via the compact network morphism,"It is often the case that the performance of a neural network can be improved
by adding layers. In real-world practices, we always train dozens of neural
network architectures in parallel which is a wasteful process. We explored
$CompNet$, in which case we morph a well-trained neural network to a deeper one
where network function can be preserved and the added layer is compact. The
work of the paper makes two contributions: a). The modified network can
converge fast and keep the same functionality so that we do not need to train
from scratch again; b). The layer size of the added layer in the neural network
is controlled by removing the redundant parameters with sparse optimization.
This differs from previous network morphism approaches which tend to add more
neurons or channels beyond the actual requirements and result in redundance of
the model. The method is illustrated using several neural network structures on
different data sets including MNIST and CIFAR10.","Jun Lu, Wei Ma, Boi Faltings",2018,http://arxiv.org/abs/1804.10316v1
Composite Neural Network: Theory and Application to PM2.5 Prediction,"This work investigates the framework and performance issues of the composite
neural network, which is composed of a collection of pre-trained and
non-instantiated neural network models connected as a rooted directed acyclic
graph for solving complicated applications. A pre-trained neural network model
is generally well trained, targeted to approximate a specific function. Despite
a general belief that a composite neural network may perform better than a
single component, the overall performance characteristics are not clear. In
this work, we construct the framework of a composite network, and prove that a
composite neural network performs better than any of its pre-trained components
with a high probability bound. In addition, if an extra pre-trained component
is added to a composite network, with high probability, the overall performance
will not be degraded. In the study, we explore a complicated application --
PM2.5 prediction -- to illustrate the correctness of the proposed composite
network theory. In the empirical evaluations of PM2.5 prediction, the
constructed composite neural network models support the proposed theory and
perform better than other machine learning models, demonstrate the advantages
of the proposed framework.","Ming-Chuan Yang, Meng Chang Chen",2019,http://arxiv.org/abs/1910.09739v2
Parameter Convex Neural Networks,"Deep learning utilizing deep neural networks (DNNs) has achieved a lot of
success recently in many important areas such as computer vision, natural
language processing, and recommendation systems. The lack of convexity for DNNs
has been seen as a major disadvantage of many optimization methods, such as
stochastic gradient descent, which greatly reduces the genelization of neural
network applications. We realize that the convexity make sense in the neural
network and propose the exponential multilayer neural network (EMLP), a class
of parameter convex neural network (PCNN) which is convex with regard to the
parameters of the neural network under some conditions that can be realized.
Besides, we propose the convexity metric for the two-layer EGCN and test the
accuracy when the convexity metric changes. For late experiments, we use the
same architecture to make the exponential graph convolutional network (EGCN)
and do the experiment on the graph classificaion dataset in which our model
EGCN performs better than the graph convolutional network (GCN) and the graph
attention network (GAT).","Jingcheng Zhou, Wei Wei, Xing Li, Bowen Pang, Zhiming Zheng",2022,http://arxiv.org/abs/2206.05562v1
"Collaboration between parallel connected neural networks -- A possible
  criterion for distinguishing artificial neural networks from natural organs","We find experimentally that when artificial neural networks are connected in
parallel and trained together, they display the following properties. (i) When
the parallel-connected neural network (PNN) is optimized, each sub-network in
the connection is not optimized. (ii) The contribution of an inferior
sub-network to the whole PNN can be on par with that of the superior
sub-network. (iii) The PNN can output the correct result even when all
sub-networks give incorrect results. These properties are unlikely for natural
biological sense organs. Therefore, they could serve as a simple yet effective
criterion for measuring the bionic level of neural networks. With this
criterion, we further show that when serving as the activation function, the
ReLU function can make an artificial neural network more bionic than the
sigmoid and Tanh functions do.",Guang Ping He,2022,http://arxiv.org/abs/2208.09983v1
Spiking Generative Adversarial Network with Attention Scoring Decoding,"Generative models based on neural networks present a substantial challenge
within deep learning. As it stands, such models are primarily limited to the
domain of artificial neural networks. Spiking neural networks, as the third
generation of neural networks, offer a closer approximation to brain-like
processing due to their rich spatiotemporal dynamics. However, generative
models based on spiking neural networks are not well studied. In this work, we
pioneer constructing a spiking generative adversarial network capable of
handling complex images. Our first task was to identify the problems of
out-of-domain inconsistency and temporal inconsistency inherent in spiking
generative adversarial networks. We addressed these issues by incorporating the
Earth-Mover distance and an attention-based weighted decoding method,
significantly enhancing the performance of our algorithm across several
datasets. Experimental results reveal that our approach outperforms existing
methods on the MNIST, FashionMNIST, CIFAR10, and CelebA datasets. Moreover,
compared with hybrid spiking generative adversarial networks, where the
discriminator is an artificial analog neural network, our methodology
demonstrates closer alignment with the information processing patterns observed
in the mouse.","Linghao Feng, Dongcheng Zhao, Yi Zeng",2023,http://arxiv.org/abs/2305.10246v3
Multi-stage Neural Networks: Function Approximator of Machine Precision,"Deep learning techniques are increasingly applied to scientific problems,
where the precision of networks is crucial. Despite being deemed as universal
function approximators, neural networks, in practice, struggle to reduce the
prediction errors below $O(10^{-5})$ even with large network size and extended
training iterations. To address this issue, we developed the multi-stage neural
networks that divides the training process into different stages, with each
stage using a new network that is optimized to fit the residue from the
previous stage. Across successive stages, the residue magnitudes decreases
substantially and follows an inverse power-law relationship with the residue
frequencies. The multi-stage neural networks effectively mitigate the spectral
biases associated with regular neural networks, enabling them to capture the
high frequency feature of target functions. We demonstrate that the prediction
error from the multi-stage training for both regression problems and
physics-informed neural networks can nearly reach the machine-precision
$O(10^{-16})$ of double-floating point within a finite number of iterations.
Such levels of accuracy are rarely attainable using single neural networks
alone.","Yongji Wang, Ching-Yao Lai",2023,http://arxiv.org/abs/2307.08934v1
Learning to Generate Genotypes with Neural Networks,"Neural networks and evolutionary computation have a rich intertwined history.
They most commonly appear together when an evolutionary algorithm optimises the
parameters and topology of a neural network for reinforcement learning
problems, or when a neural network is applied as a surrogate fitness function
to aid the evolutionary optimisation of expensive fitness functions. In this
paper we take a different approach, asking the question of whether a neural
network can be used to provide a mutation distribution for an evolutionary
algorithm, and what advantages this approach may offer? Two modern neural
network models are investigated, a Denoising Autoencoder modified to produce
stochastic outputs and the Neural Autoregressive Distribution Estimator.
Results show that the neural network approach to learning genotypes is able to
solve many difficult discrete problems, such as MaxSat and HIFF, and regularly
outperforms other evolutionary techniques.","Alexander W. Churchill, Siddharth Sigtia, Chrisantha Fernando",2016,http://arxiv.org/abs/1604.04153v1
"Effective and Efficient Computation with Multiple-timescale Spiking
  Recurrent Neural Networks","The emergence of brain-inspired neuromorphic computing as a paradigm for edge
AI is motivating the search for high-performance and efficient spiking neural
networks to run on this hardware. However, compared to classical neural
networks in deep learning, current spiking neural networks lack competitive
performance in compelling areas. Here, for sequential and streaming tasks, we
demonstrate how a novel type of adaptive spiking recurrent neural network
(SRNN) is able to achieve state-of-the-art performance compared to other
spiking neural networks and almost reach or exceed the performance of classical
recurrent neural networks (RNNs) while exhibiting sparse activity. From this,
we calculate a $>$100x energy improvement for our SRNNs over classical RNNs on
the harder tasks. To achieve this, we model standard and adaptive
multiple-timescale spiking neurons as self-recurrent neural units, and leverage
surrogate gradients and auto-differentiation in the PyTorch Deep Learning
framework to efficiently implement backpropagation-through-time, including
learning of the important spiking neuron parameters to adapt our spiking
neurons to the tasks.","Bojian Yin, Federico Corradi, Sander M. Bohté",2020,http://arxiv.org/abs/2005.11633v2
Coevolutionary Neural Population Models,"We present a method for using neural networks to model evolutionary
population dynamics, and draw parallels to recent deep learning advancements in
which adversarially-trained neural networks engage in coevolutionary
interactions. We conduct experiments which demonstrate that models from
evolutionary game theory are capable of describing the behavior of these neural
population systems.","Nick Moran, Jordan Pollack",2018,http://arxiv.org/abs/1804.04187v1
"Energy-efficient Spiking Neural Network Equalization for IM/DD Systems
  with Optimized Neural Encoding","We propose an energy-efficient equalizer for IM/DD systems based on spiking
neural networks. We optimize a neural spike encoding that boosts the
equalizer's performance while decreasing energy consumption.","Alexander von Bank, Eike-Manuel Edelmann, Laurent Schmalen",2023,http://arxiv.org/abs/2312.12909v1
Convolutional Conditional Neural Processes,"Neural processes are a family of models which use neural networks to directly
parametrise a map from data sets to predictions. Directly parametrising this
map enables the use of expressive neural networks in small-data problems where
neural networks would traditionally overfit. Neural processes can produce
well-calibrated uncertainties, effectively deal with missing data, and are
simple to train. These properties make this family of models appealing for a
breadth of applications areas, such as healthcare or environmental sciences.
  This thesis advances neural processes in three ways.
  First, we propose convolutional neural processes (ConvNPs). ConvNPs improve
data efficiency of neural processes by building in a symmetry called
translation equivariance. ConvNPs rely on convolutional neural networks rather
than multi-layer perceptrons.
  Second, we propose Gaussian neural processes (GNPs). GNPs directly
parametrise dependencies in the predictions of a neural process. Current
approaches to modelling dependencies in the predictions depend on a latent
variable, which consequently requires approximate inference, undermining the
simplicity of the approach.
  Third, we propose autoregressive conditional neural processes (AR CNPs). AR
CNPs train a neural process without any modifications to the model or training
procedure and, at test time, roll out the model in an autoregressive fashion.
AR CNPs equip the neural process framework with a new knob where modelling
complexity and computational expense at training time can be traded for
computational expense at test time.
  In addition to methodological advancements, this thesis also proposes a
software abstraction that enables a compositional approach to implementing
neural processes. This approach allows the user to rapidly explore the space of
neural process models by putting together elementary building blocks in
different ways.",Wessel P. Bruinsma,2024,http://arxiv.org/abs/2408.09583v1
"A Survey: Time Travel in Deep Learning Space: An Introduction to Deep
  Learning Models and How Deep Learning Models Evolved from the Initial Ideas","This report will show the history of deep learning evolves. It will trace
back as far as the initial belief of connectionism modelling of brain, and come
back to look at its early stage realization: neural networks. With the
background of neural network, we will gradually introduce how convolutional
neural network, as a representative of deep discriminative models, is developed
from neural networks, together with many practical techniques that can help in
optimization of neural networks. On the other hand, we will also trace back to
see the evolution history of deep generative models, to see how researchers
balance the representation power and computation complexity to reach Restricted
Boltzmann Machine and eventually reach Deep Belief Nets. Further, we will also
look into the development history of modelling time series data with neural
networks. We start with Time Delay Neural Networks and move further to
currently famous model named Recurrent Neural Network and its extension Long
Short Term Memory. We will also briefly look into how to construct deep
recurrent neural networks. Finally, we will conclude this report with some
interesting open-ended questions of deep neural networks.","Haohan Wang, Bhiksha Raj",2015,http://arxiv.org/abs/1510.04781v2
"Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural
  Networks","Taking inspiration from biological evolution, we explore the idea of ""Can
deep neural networks evolve naturally over successive generations into highly
efficient deep neural networks?"" by introducing the notion of synthesizing new
highly efficient, yet powerful deep neural networks over successive generations
via an evolutionary process from ancestor deep neural networks. The
architectural traits of ancestor deep neural networks are encoded using
synaptic probability models, which can be viewed as the `DNA' of these
networks. New descendant networks with differing network architectures are
synthesized based on these synaptic probability models from the ancestor
networks and computational environmental factor models, in a random manner to
mimic heredity, natural selection, and random mutation. These offspring
networks are then trained into fully functional networks, like one would train
a newborn, and have more efficient, more diverse network architectures than
their ancestor networks, while achieving powerful modeling capabilities.
Experimental results for the task of visual saliency demonstrated that the
synthesized `evolved' offspring networks can achieve state-of-the-art
performance while having network architectures that are significantly more
efficient (with a staggering $\sim$48-fold decrease in synapses by the fourth
generation) compared to the original ancestor network.","Mohammad Javad Shafiee, Akshaya Mishra, Alexander Wong",2016,http://arxiv.org/abs/1606.04393v3
Extreme Compression of Adaptive Neural Images,"Implicit Neural Representations (INRs) and Neural Fields are a novel paradigm
for signal representation, from images and audio to 3D scenes and videos. The
fundamental idea is to represent a signal as a continuous and differentiable
neural network. This idea offers unprecedented benefits such as continuous
resolution and memory efficiency, enabling new compression techniques. However,
representing data as neural networks poses new challenges. For instance, given
a 2D image as a neural network, how can we further compress such a neural
image?. In this work, we present a novel analysis on compressing neural fields,
with the focus on images. We also introduce Adaptive Neural Images (ANI), an
efficient neural representation that enables adaptation to different inference
or transmission requirements. Our proposed method allows to reduce the
bits-per-pixel (bpp) of the neural image by 4x, without losing sensitive
details or harming fidelity. We achieve this thanks to our successful
implementation of 4-bit neural representations. Our work offers a new framework
for developing compressed neural fields.","Leo Hoshikawa, Marcos V. Conde, Takeshi Ohashi, Atsushi Irie",2024,http://arxiv.org/abs/2405.16807v2
"Implicit Gradient Neural Networks with a Positive-Definite Mass Matrix
  for Online Linear Equations Solving","Motivated by the advantages achieved by implicit analogue net for solving
online linear equations, a novel implicit neural model is designed based on
conventional explicit gradient neural networks in this letter by introducing a
positive-definite mass matrix. In addition to taking the advantages of the
implicit neural dynamics, the proposed implicit gradient neural networks can
still achieve globally exponential convergence to the unique theoretical
solution of linear equations and also global stability even under no-solution
and multi-solution situations. Simulative results verify theoretical
convergence analysis on the proposed neural dynamics.",Ke Chen,2017,http://arxiv.org/abs/1703.05955v1
Pixel Normalization from Numeric Data as Input to Neural Networks,"Text to image transformation for input to neural networks requires
intermediate steps. This paper attempts to present a new approach to pixel
normalization so as to convert textual data into image, suitable as input for
neural networks. This method can be further improved by its Graphics Processing
Unit (GPU) implementation to provide significant speedup in computational time.","Parth Sane, Ravindra Agrawal",2017,http://arxiv.org/abs/1705.01809v1
What are Neural Networks made of?,"The success of Deep Learning methods is not well understood, though various
attempts at explaining it have been made, typically centered on properties of
stochastic gradient descent. Even less clear is why certain neural network
architectures perform better than others. We provide a potential opening with
the hypothesis that neural network training is a form of Genetic Programming.",Rene Schaub,2019,http://arxiv.org/abs/1909.09588v1
diagNNose: A Library for Neural Activation Analysis,"In this paper we introduce diagNNose, an open source library for analysing
the activations of deep neural networks. diagNNose contains a wide array of
interpretability techniques that provide fundamental insights into the inner
workings of neural networks. We demonstrate the functionality of diagNNose with
a case study on subject-verb agreement within language models. diagNNose is
available at https://github.com/i-machine-think/diagnnose.",Jaap Jumelet,2020,http://arxiv.org/abs/2011.06819v1
Spiking Neural Network Equalization for IM/DD Optical Communication,"A spiking neural network (SNN) equalizer model suitable for electronic
neuromorphic hardware is designed for an IM/DD link. The SNN achieves the same
bit-error-rate as an artificial neural network, outperforming linear
equalization.","Elias Arnold, Georg Böcherer, Eric Müller, Philipp Spilger, Johannes Schemmel, Stefano Calabrò, Maxim Kuschnerov",2022,http://arxiv.org/abs/2205.04263v2
Fast Neural Network based Solving of Partial Differential Equations,"We present a novel method for using Neural Networks (NNs) for finding
solutions to a class of Partial Differential Equations (PDEs). Our method
builds on recent advances in Neural Radiance Field research (NeRFs) and allows
for a NN to converge to a PDE solution much faster than classic Physically
Informed Neural Network (PINNs) approaches.","Jaroslaw Rzepecki, Daniel Bates, Chris Doran",2022,http://arxiv.org/abs/2205.08978v2
Spiking Neural Network Decision Feedback Equalization for IM/DD Systems,"A spiking neural network (SNN) equalizer with a decision feedback structure
is applied to an IM/DD link with various parameters. The SNN outperforms linear
and artificial neural network (ANN) based equalizers.","Alexander von Bank, Eike-Manuel Edelmann, Laurent Schmalen",2023,http://arxiv.org/abs/2304.14152v1
"KHNNs: hypercomplex neural networks computations via Keras using
  TensorFlow and PyTorch","Neural networks used in computations with more advanced algebras than real
numbers perform better in some applications. However, there is no general
framework for constructing hypercomplex neural networks. We propose a library
integrated with Keras that can do computations within TensorFlow and PyTorch.
It provides Dense and Convolutional 1D, 2D, and 3D layers architectures.","Agnieszka Niemczynowicz, Radosław Antoni Kycia",2024,http://arxiv.org/abs/2407.00452v1
"Universal approximation theorem for neural networks with inputs from a
  topological vector space","We study feedforward neural networks with inputs from a topological vector
space (TVS-FNNs). Unlike traditional feedforward neural networks, TVS-FNNs can
process a broader range of inputs, including sequences, matrices, functions and
more. We prove a universal approximation theorem for TVS-FNNs, which
demonstrates their capacity to approximate any continuous function defined on
this expanded input space.",Vugar Ismailov,2024,http://arxiv.org/abs/2409.12913v1
Attentive Neural Network for Named Entity Recognition in Vietnamese,"We propose an attentive neural network for the task of named entity
recognition in Vietnamese. The proposed attentive neural model makes use of
character-based language models and word embeddings to encode words as vector
representations. A neural network architecture of encoder, attention, and
decoder layers is then utilized to encode knowledge of input sentences and to
label entity tags. The experimental results show that the proposed attentive
neural network achieves the state-of-the-art results on the benchmark named
entity recognition datasets in Vietnamese in comparison to both hand-crafted
features based models and neural models.","Kim Anh Nguyen, Ngan Dong, Cam-Tu Nguyen",2018,http://arxiv.org/abs/1810.13097v2
"Study of a committee of neural networks for biometric hand-geometry
  recognition","This Paper studies different committees of neural networks for biometric
pattern recognition. We use the neural nets as classifiers for identification
and verification purposes. We show that a committee of nets can improve the
recognition rates when compared with a multi-start initialization algo-rithm
that just picks up the neural net which offers the best performance. On the
other hand, we found that there is no strong correlation between
identifi-cation and verification applications using the same classifier.",Marcos Faundez-Zanuy,2022,http://arxiv.org/abs/2204.03935v1
"Generalization bounds for neural ordinary differential equations and
  deep residual networks","Neural ordinary differential equations (neural ODEs) are a popular family of
continuous-depth deep learning models. In this work, we consider a large family
of parameterized ODEs with continuous-in-time parameters, which include
time-dependent neural ODEs. We derive a generalization bound for this class by
a Lipschitz-based argument. By leveraging the analogy between neural ODEs and
deep residual networks, our approach yields in particular a generalization
bound for a class of deep residual networks. The bound involves the magnitude
of the difference between successive weight matrices. We illustrate numerically
how this quantity affects the generalization capability of neural networks.",Pierre Marion,2023,http://arxiv.org/abs/2305.06648v2
On the Performance of Temporal Difference Learning With Neural Networks,"Neural Temporal Difference (TD) Learning is an approximate temporal
difference method for policy evaluation that uses a neural network for function
approximation. Analysis of Neural TD Learning has proven to be challenging. In
this paper we provide a convergence analysis of Neural TD Learning with a
projection onto $B(\theta_0, \omega)$, a ball of fixed radius $\omega$ around
the initial point $\theta_0$. We show an approximation bound of $O(\epsilon) +
\tilde{O} (1/\sqrt{m})$ where $\epsilon$ is the approximation quality of the
best neural network in $B(\theta_0, \omega)$ and $m$ is the width of all hidden
layers in the network.","Haoxing Tian, Ioannis Ch. Paschalidis, Alex Olshevsky",2023,http://arxiv.org/abs/2312.05397v1
CS591 Report: Application of siamesa network in 2D transformation,"Deep learning has been extensively used various aspects of computer vision
area. Deep learning separate itself from traditional neural network by having a
much deeper and complicated network layers in its network structures.
Traditionally, deep neural network is abundantly used in computer vision tasks
including classification and detection and has achieve remarkable success and
set up a new state of the art results in these fields. Instead of using neural
network for vision recognition and detection. I will show the ability of neural
network to do image registration, synthesis of images and image retrieval in
this report.",Dorothy Chang,2017,http://arxiv.org/abs/1706.09598v1
"Cramnet: Layer-wise Deep Neural Network Compression with Knowledge
  Transfer from a Teacher Network","Neural Networks accomplish amazing things, but they suffer from computational
and memory bottlenecks that restrict their usage. Nowhere can this be better
seen than in the mobile space, where specialized hardware is being created just
to satisfy the demand for neural networks. Previous studies have shown that
neural networks have vastly more connections than they actually need to do
their work. This thesis develops a method that can compress networks to less
than 10% of memory and less than 25% of computational power, without loss of
accuracy, and without creating sparse networks that require special code to
run.",Jon Hoffman,2019,http://arxiv.org/abs/1904.05982v1
Sheaf Neural Networks,"We present a generalization of graph convolutional networks by generalizing
the diffusion operation underlying this class of graph neural networks. These
sheaf neural networks are based on the sheaf Laplacian, a generalization of the
graph Laplacian that encodes additional relational structure parameterized by
the underlying graph. The sheaf Laplacian and associated matrices provide an
extended version of the diffusion operation in graph convolutional networks,
providing a proper generalization for domains where relations between nodes are
non-constant, asymmetric, and varying in dimension. We show that the resulting
sheaf neural networks can outperform graph convolutional networks in domains
where relations between nodes are asymmetric and signed.","Jakob Hansen, Thomas Gebhart",2020,http://arxiv.org/abs/2012.06333v1
"Learning Compact Neural Networks Using Ordinary Differential Equations
  as Activation Functions","Most deep neural networks use simple, fixed activation functions, such as
sigmoids or rectified linear units, regardless of domain or network structure.
We introduce differential equation units (DEUs), an improvement to modern
neural networks, which enables each neuron to learn a particular nonlinear
activation function from a family of solutions to an ordinary differential
equation. Specifically, each neuron may change its functional form during
training based on the behavior of the other parts of the network. We show that
using neurons with DEU activation functions results in a more compact network
capable of achieving comparable, if not superior, performance when is compared
to much larger networks.","MohamadAli Torkamani, Phillip Wallis, Shiv Shankar, Amirmohammad Rooshenas",2019,http://arxiv.org/abs/1905.07685v1
"Quantile and moment neural networks for learning functionals of
  distributions","We study news neural networks to approximate function of distributions in a
probability space. Two classes of neural networks based on quantile and moment
approximation are proposed to learn these functions and are theoretically
supported by universal approximation theorems. By mixing the quantile and
moment features in other new networks, we develop schemes that outperform
existing networks on numerical test cases involving univariate distributions.
For bivariate distributions, the moment neural network outperforms all other
networks.",Xavier Warin,2023,http://arxiv.org/abs/2303.11060v1
"Exactly conservative physics-informed neural networks and deep operator
  networks for dynamical systems","We introduce a method for training exactly conservative physics-informed
neural networks and physics-informed deep operator networks for dynamical
systems. The method employs a projection-based technique that maps a candidate
solution learned by the neural network solver for any given dynamical system
possessing at least one first integral onto an invariant manifold. We
illustrate that exactly conservative physics-informed neural network solvers
and physics-informed deep operator networks for dynamical systems vastly
outperform their non-conservative counterparts for several real-world problems
from the mathematical sciences.","Elsa Cardoso-Bihlo, Alex Bihlo",2023,http://arxiv.org/abs/2311.14131v1
"A Neural Network Model to Classify Liver Cancer Patients Using Data
  Expansion and Compression","We develop a neural network model to classify liver cancer patients into
high-risk and low-risk groups using genomic data. Our approach provides a novel
technique to classify big data sets using neural network models. We preprocess
the data before training the neural network models. We first expand the data
using wavelet analysis. We then compress the wavelet coefficients by mapping
them onto a new scaled orthonormal coordinate system. Then the data is used to
train a neural network model that enables us to classify cancer patients into
two different classes of high-risk and low-risk patients. We use the
leave-one-out approach to build a neural network model. This neural network
model enables us to classify a patient using genomic data as a high-risk or
low-risk patient without any information about the survival time of the
patient. The results from genomic data analysis are compared with survival time
analysis. It is shown that the expansion and compression of data using wavelet
analysis and singular value decomposition (SVD) is essential to train the
neural network model.","Ashkan Zeinalzadeh, Tom Wenska, Gordon Okimoto",2016,http://arxiv.org/abs/1611.07588v2
"Theoretical Properties for Neural Networks with Weight Matrices of Low
  Displacement Rank","Recently low displacement rank (LDR) matrices, or so-called structured
matrices, have been proposed to compress large-scale neural networks. Empirical
results have shown that neural networks with weight matrices of LDR matrices,
referred as LDR neural networks, can achieve significant reduction in space and
computational complexity while retaining high accuracy. We formally study LDR
matrices in deep learning. First, we prove the universal approximation property
of LDR neural networks with a mild condition on the displacement operators. We
then show that the error bounds of LDR neural networks are as efficient as
general neural networks with both single-layer and multiple-layer structure.
Finally, we propose back-propagation based training algorithm for general LDR
neural networks.","Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Victor Pan, Bo Yuan",2017,http://arxiv.org/abs/1703.00144v4
"Sparse-Input Neural Networks for High-dimensional Nonparametric
  Regression and Classification","Neural networks are usually not the tool of choice for nonparametric
high-dimensional problems where the number of input features is much larger
than the number of observations. Though neural networks can approximate complex
multivariate functions, they generally require a large number of training
observations to obtain reasonable fits, unless one can learn the appropriate
network structure. In this manuscript, we show that neural networks can be
applied successfully to high-dimensional settings if the true function falls in
a low dimensional subspace, and proper regularization is used. We propose
fitting a neural network with a sparse group lasso penalty on the first-layer
input weights. This results in a neural net that only uses a small subset of
the original features. In addition, we characterize the statistical convergence
of the penalized empirical risk minimizer to the optimal neural network: we
show that the excess risk of this penalized estimator only grows with the
logarithm of the number of input features; and we show that the weights of
irrelevant features converge to zero. Via simulation studies and data analyses,
we show that these sparse-input neural networks outperform existing
nonparametric high-dimensional estimation methods when the data has complex
higher-order interactions.","Jean Feng, Noah Simon",2017,http://arxiv.org/abs/1711.07592v2
Masked Conditional Neural Networks for Audio Classification,"We present the ConditionaL Neural Network (CLNN) and the Masked ConditionaL
Neural Network (MCLNN) designed for temporal signal recognition. The CLNN takes
into consideration the temporal nature of the sound signal and the MCLNN
extends upon the CLNN through a binary mask to preserve the spatial locality of
the features and allows an automated exploration of the features combination
analogous to hand-crafting the most relevant features for the recognition task.
MCLNN has achieved competitive recognition accuracies on the GTZAN and the
ISMIR2004 music datasets that surpass several state-of-the-art neural network
based architectures and hand-crafted methods applied on both datasets.","Fady Medhat, David Chesmore, John Robinson",2018,http://arxiv.org/abs/1803.02421v2
Training of photonic neural networks through in situ backpropagation,"Recently, integrated optics has gained interest as a hardware platform for
implementing machine learning algorithms. Of particular interest are artificial
neural networks, since matrix-vector multi- plications, which are used heavily
in artificial neural networks, can be done efficiently in photonic circuits.
The training of an artificial neural network is a crucial step in its
application. However, currently on the integrated photonics platform there is
no efficient protocol for the training of these networks. In this work, we
introduce a method that enables highly efficient, in situ training of a
photonic neural network. We use adjoint variable methods to derive the photonic
analogue of the backpropagation algorithm, which is the standard method for
computing gradients of conventional neural networks. We further show how these
gradients may be obtained exactly by performing intensity measurements within
the device. As an application, we demonstrate the training of a numerically
simulated photonic artificial neural network. Beyond the training of photonic
machine learning implementations, our method may also be of broad interest to
experimental sensitivity analysis of photonic systems and the optimization of
reconfigurable optics platforms.","Tyler W. Hughes, Momchil Minkov, Yu Shi, Shanhui Fan",2018,http://arxiv.org/abs/1805.09943v1
"Exponential Convergence of the Deep Neural Network Approximation for
  Analytic Functions","We prove that for analytic functions in low dimension, the convergence rate
of the deep neural network approximation is exponential.","Weinan E, Qingcan Wang",2018,http://arxiv.org/abs/1807.00297v1
"Selective Deep Convolutional Neural Network for Low Cost Distorted Image
  Classification","Deep convolutional neural networks have proven to be well suited for image
classification applications. However, if there is distortion in the image, the
classification accuracy can be significantly degraded, even with
state-of-the-art neural networks. The accuracy cannot be significantly improved
by simply training with distorted images. Instead, this paper proposes a
multiple neural network topology referred to as a selective deep convolutional
neural network. By modifying existing state-of-the-art neural networks in the
proposed manner, it is shown that a similar level of classification accuracy
can be achieved, but at a significantly lower cost. The cost reduction is
obtained primarily through the use of fewer weight parameters. Using fewer
weights reduces the number of multiply-accumulate operations and also reduces
the energy required for data accesses. Finally, it is shown that the
effectiveness of the proposed selective deep convolutional neural network can
be further improved by combining it with previously proposed network cost
reduction methods.","Minho Ha, Younghoon Byeon, Youngjoo Lee, Sunggu Lee",2018,http://arxiv.org/abs/1807.01418v2
"Evaluation of Complex-Valued Neural Networks on Real-Valued
  Classification Tasks","Complex-valued neural networks are not a new concept, however, the use of
real-valued models has often been favoured over complex-valued models due to
difficulties in training and performance. When comparing real-valued versus
complex-valued neural networks, existing literature often ignores the number of
parameters, resulting in comparisons of neural networks with vastly different
sizes. We find that when real and complex neural networks of similar capacity
are compared, complex models perform equal to or slightly worse than
real-valued models for a range of real-valued classification tasks. The use of
complex numbers allows neural networks to handle noise on the complex plane.
When classifying real-valued data with a complex-valued neural network, the
imaginary parts of the weights follow their real parts. This behaviour is
indicative for a task that does not require a complex-valued model. We further
investigated this in a synthetic classification task. We can transfer many
activation functions from the real to the complex domain using different
strategies. The weight initialisation of complex neural networks, however,
remains a significant problem.","Nils Mönning, Suresh Manandhar",2018,http://arxiv.org/abs/1811.12351v1
Deep Neural Network Ensembles,"Current deep neural networks suffer from two problems; first, they are hard
to interpret, and second, they suffer from overfitting. There have been many
attempts to define interpretability in neural networks, but they typically lack
causality or generality. A myriad of regularization techniques have been
developed to prevent overfitting, and this has driven deep learning to become
the hot topic it is today; however, while most regularization techniques are
justified empirically and even intuitively, there is not much underlying
theory. This paper argues that to extract the features used in neural networks
to make decisions, it's important to look at the paths between clusters
existing in the hidden spaces of neural networks. These features are of
particular interest because they reflect the true decision making process of
the neural network. This analysis is then furthered to present an ensemble
algorithm for arbitrary neural networks which has guarantees for test accuracy.
Finally, a discussion detailing the aforementioned guarantees is introduced and
the implications to neural networks, including an intuitive explanation for all
current regularization methods, are presented. The ensemble algorithm has
generated state-of-the-art results for Wide-ResNets on CIFAR-10 (top 5 for all
models) and has improved test accuracy for all models it has been applied to.",Sean Tao,2019,http://arxiv.org/abs/1904.05488v2
Iterative training of neural networks for intra prediction,"This paper presents an iterative training of neural networks for intra
prediction in a block-based image and video codec. First, the neural networks
are trained on blocks arising from the codec partitioning of images, each
paired with its context. Then, iteratively, blocks are collected from the
partitioning of images via the codec including the neural networks trained at
the previous iteration, each paired with its context, and the neural networks
are retrained on the new pairs. Thanks to this training, the neural networks
can learn intra prediction functions that both stand out from those already in
the initial codec and boost the codec in terms of rate-distortion. Moreover,
the iterative process allows the design of training data cleansings essential
for the neural network training. When the iteratively trained neural networks
are put into H.265 (HM-16.15), -4.2% of mean dB-rate reduction is obtained. By
moving them into H.266 (VTM-5.0), the mean dB-rate reduction reaches -1.9%.","Thierry Dumas, Franck Galpin, Philippe Bordes",2020,http://arxiv.org/abs/2003.06812v2
On Tractable Representations of Binary Neural Networks,"We consider the compilation of a binary neural network's decision function
into tractable representations such as Ordered Binary Decision Diagrams (OBDDs)
and Sentential Decision Diagrams (SDDs). Obtaining this function as an OBDD/SDD
facilitates the explanation and formal verification of a neural network's
behavior. First, we consider the task of verifying the robustness of a neural
network, and show how we can compute the expected robustness of a neural
network, given an OBDD/SDD representation of it. Next, we consider a more
efficient approach for compiling neural networks, based on a pseudo-polynomial
time algorithm for compiling a neuron. We then provide a case study in a
handwritten digits dataset, highlighting how two neural networks trained from
the same dataset can have very high accuracies, yet have very different levels
of robustness. Finally, in experiments, we show that it is feasible to obtain
compact representations of neural networks as SDDs.","Weijia Shi, Andy Shih, Adnan Darwiche, Arthur Choi",2020,http://arxiv.org/abs/2004.02082v2
"Adaptive Neural Network-Based Approximation to Accelerate Eulerian Fluid
  Simulation","The Eulerian fluid simulation is an important HPC application. The neural
network has been applied to accelerate it. The current methods that accelerate
the fluid simulation with neural networks lack flexibility and generalization.
In this paper, we tackle the above limitation and aim to enhance the
applicability of neural networks in the Eulerian fluid simulation. We introduce
Smartfluidnet, a framework that automates model generation and application.
Given an existing neural network as input, Smartfluidnet generates multiple
neural networks before the simulation to meet the execution time and simulation
quality requirement. During the simulation, Smartfluidnet dynamically switches
the neural networks to make the best efforts to reach the user requirement on
simulation quality. Evaluating with 20,480 input problems, we show that
Smartfluidnet achieves 1.46x and 590x speedup comparing with a state-of-the-art
neural network model and the original fluid simulation respectively on an
NVIDIA Titan X Pascal GPU, while providing better simulation quality than the
state-of-the-art model.","Wenqian Dong, Jie Liu, Zhen Xie, Dong Li",2020,http://arxiv.org/abs/2008.11832v1
Improving accuracy of turbulence models by neural network,"Neural networks of simple structures are used to construct a turbulence model
for large-eddy simulation (LES). Data obtained by direct numerical simulation
(DNS) of homogeneous isotropic turbulence are used to train neural networks. It
is shown that two methods are effective for improvement of accuracy of the
model: weighting data for training and addition of the second-order derivatives
of velocity to the input variables. As a result, high correlation between the
exact subgrid scale stress and the prediction by the neural network is obtained
for large filter width; the correlation coefficient is about 0:9 and 0:8 for
filter widths 48:8\eta and 97:4\eta, respectively, where \eta is the Kolmogorov
scale. The models established by neural networks are close to but not identical
with the gradient models. LES with the neural network model is performed for
the homogeneous isotropic turbulence and the initial-value problem of the
Taylor-Green vortices. The results obtained with the neural network model are
in reasonable agreement with those of the filtered DNS. However, symmetry in
the latter problem is broken since the neural network model does not possess
rigorous symmetry under orthogonal transformations.","Satoshi Miyazaki, Yuji Hattori",2020,http://arxiv.org/abs/2012.01723v1
"Graph Neural Networks: Taxonomy, Advances and Trends","Graph neural networks provide a powerful toolkit for embedding real-world
graphs into low-dimensional spaces according to specific tasks. Up to now,
there have been several surveys on this topic. However, they usually lay
emphasis on different angles so that the readers can not see a panorama of the
graph neural networks. This survey aims to overcome this limitation, and
provide a comprehensive review on the graph neural networks. First of all, we
provide a novel taxonomy for the graph neural networks, and then refer to up to
400 relevant literatures to show the panorama of the graph neural networks. All
of them are classified into the corresponding categories. In order to drive the
graph neural networks into a new stage, we summarize four future research
directions so as to overcome the facing challenges. It is expected that more
and more scholars can understand and exploit the graph neural networks, and use
them in their research community.","Yu Zhou, Haixia Zheng, Xin Huang, Shufeng Hao, Dengao Li, Jumin Zhao",2020,http://arxiv.org/abs/2012.08752v4
Towards Robust Neural Networks via Close-loop Control,"Despite their success in massive engineering applications, deep neural
networks are vulnerable to various perturbations due to their black-box nature.
Recent study has shown that a deep neural network can misclassify the data even
if the input data is perturbed by an imperceptible amount. In this paper, we
address the robustness issue of neural networks by a novel close-loop control
method from the perspective of dynamic systems. Instead of modifying the
parameters in a fixed neural network architecture, a close-loop control process
is added to generate control signals adaptively for the perturbed or corrupted
data. We connect the robustness of neural networks with optimal control using
the geometrical information of underlying data to design the control objective.
The detailed analysis shows how the embedding manifolds of state trajectory
affect error estimation of the proposed method. Our approach can simultaneously
maintain the performance on clean data and improve the robustness against many
types of data perturbations. It can also further improve the performance of
robustly trained neural networks against different perturbations. To the best
of our knowledge, this is the first work that improves the robustness of neural
networks with close-loop control.","Zhuotong Chen, Qianxiao Li, Zheng Zhang",2021,http://arxiv.org/abs/2102.01862v2
"A Weight Initialization Based on the Linear Product Structure for Neural
  Networks","Weight initialization plays an important role in training neural networks and
also affects tremendous deep learning applications. Various weight
initialization strategies have already been developed for different activation
functions with different neural networks. These initialization algorithms are
based on minimizing the variance of the parameters between layers and might
still fail when neural networks are deep, e.g., dying ReLU. To address this
challenge, we study neural networks from a nonlinear computation point of view
and propose a novel weight initialization strategy that is based on the linear
product structure (LPS) of neural networks. The proposed strategy is derived
from the polynomial approximation of activation functions by using theories of
numerical algebraic geometry to guarantee to find all the local minima. We also
provide a theoretical analysis that the LPS initialization has a lower
probability of dying ReLU comparing to other existing initialization
strategies. Finally, we test the LPS initialization algorithm on both fully
connected neural networks and convolutional neural networks to show its
feasibility, efficiency, and robustness on public datasets.","Qipin Chen, Wenrui Hao, Juncai He",2021,http://arxiv.org/abs/2109.00125v2
Naming Schema for a Human Brain-Scale Neural Network,"Deep neural networks have become increasingly large and sparse, allowing for
the storage of large-scale neural networks with decreased costs of storage and
computation. Storage of a neural network with as many connections as the human
brain is possible with current versions of the high-performance Apache Accumulo
database and the Distributed Dimensional Data Model (D4M) software. Neural
networks of such large scale may be of particular interest to scientists within
the human brain Connectome community. To aid in research and understanding of
artificial neural networks that parallel existing neural networks like the
brain, a naming schema can be developed to label groups of neurons in the
artificial network that parallel those in the brain. Groups of artificial
neurons are able to be specifically labeled in small regions for future study.","Morgan Schaefer, Lauren Michelin, Jeremy Kepner",2021,http://arxiv.org/abs/2109.10951v1
"Neural Network based Explicit Mixture Models and
  Expectation-maximization based Learning","We propose two neural network based mixture models in this article. The
proposed mixture models are explicit in nature. The explicit models have
analytical forms with the advantages of computing likelihood and efficiency of
generating samples. Computation of likelihood is an important aspect of our
models. Expectation-maximization based algorithms are developed for learning
parameters of the proposed models. We provide sufficient conditions to realize
the expectation-maximization based learning. The main requirements are
invertibility of neural networks that are used as generators and Jacobian
computation of functional form of the neural networks. The requirements are
practically realized using a flow-based neural network. In our first mixture
model, we use multiple flow-based neural networks as generators. Naturally the
model is complex. A single latent variable is used as the common input to all
the neural networks. The second mixture model uses a single flow-based neural
network as a generator to reduce complexity. The single generator has a latent
variable input that follows a Gaussian mixture distribution. We demonstrate
efficiency of proposed mixture models through extensive experiments for
generating samples and maximum likelihood based classification.","Dong Liu, Minh Thành Vu, Saikat Chatterjee, Lars K. Rasmussen",2019,http://arxiv.org/abs/1907.13432v2
Ensemble Neural Networks (ENN): A gradient-free stochastic method,"In this study, an efficient stochastic gradient-free method, the ensemble
neural networks (ENN), is developed. In the ENN, the optimization process
relies on covariance matrices rather than derivatives. The covariance matrices
are calculated by the ensemble randomized maximum likelihood algorithm (EnRML),
which is an inverse modeling method. The ENN is able to simultaneously provide
estimations and perform uncertainty quantification since it is built under the
Bayesian framework. The ENN is also robust to small training data size because
the ensemble of stochastic realizations essentially enlarges the training
dataset. This constitutes a desirable characteristic, especially for real-world
engineering applications. In addition, the ENN does not require the calculation
of gradients, which enables the use of complicated neuron models and loss
functions in neural networks. We experimentally demonstrate benefits of the
proposed model, in particular showing that the ENN performs much better than
the traditional Bayesian neural networks (BNN). The EnRML in ENN is a
substitution of gradient-based optimization algorithms, which means that it can
be directly combined with the feed-forward process in other existing (deep)
neural networks, such as convolutional neural networks (CNN) and recurrent
neural networks (RNN), broadening future applications of the ENN.","Yuntian Chen, Haibin Chang, Meng Jin, Dongxiao Zhang",2019,http://arxiv.org/abs/1908.01113v1
"Verification of Neural Network Control Policy Under Persistent
  Adversarial Perturbation","Deep neural networks are known to be fragile to small adversarial
perturbations. This issue becomes more critical when a neural network is
interconnected with a physical system in a closed loop. In this paper, we show
how to combine recent works on neural network certification tools (which are
mainly used in static settings such as image classification) with robust
control theory to certify a neural network policy in a control loop.
Specifically, we give a sufficient condition and an algorithm to ensure that
the closed loop state and control constraints are satisfied when the persistent
adversarial perturbation is l-infinity norm bounded. Our method is based on
finding a positively invariant set of the closed loop dynamical system, and
thus we do not require the differentiability or the continuity of the neural
network policy. Along with the verification result, we also develop an
effective attack strategy for neural network control systems that outperforms
exhaustive Monte-Carlo search significantly. We show that our certification
algorithm works well on learned models and achieves 5 times better result than
the traditional Lipschitz-based method to certify the robustness of a neural
network policy on a cart pole control problem.","Yuh-Shyang Wang, Tsui-Wei Weng, Luca Daniel",2019,http://arxiv.org/abs/1908.06353v1
On the Validity of Bayesian Neural Networks for Uncertainty Estimation,"Deep neural networks (DNN) are versatile parametric models utilised
successfully in a diverse number of tasks and domains. However, they have
limitations---particularly from their lack of robustness and over-sensitivity
to out of distribution samples. Bayesian Neural Networks, due to their
formulation under the Bayesian framework, provide a principled approach to
building neural networks that address these limitations. This paper describes a
study that empirically evaluates and compares Bayesian Neural Networks to their
equivalent point estimate Deep Neural Networks to quantify the predictive
uncertainty induced by their parameters, as well as their performance in view
of this uncertainty. In this study, we evaluated and compared three point
estimate deep neural networks against comparable Bayesian neural network
alternatives using two well-known benchmark image classification datasets
(CIFAR-10 and SVHN).","John Mitros, Brian Mac Namee",2019,http://arxiv.org/abs/1912.01530v2
Adversarial VC-dimension and Sample Complexity of Neural Networks,"Adversarial attacks during the testing phase of neural networks pose a
challenge for the deployment of neural networks in security critical settings.
These attacks can be performed by adding noise that is imperceptible to humans
on top of the original data. By doing so, an attacker can create an adversarial
sample, which will cause neural networks to misclassify. In this paper, we seek
to understand the theoretical limits of what can be learned by neural networks
in the presence of an adversary. We first defined the hypothesis space of a
neural network, and showed the relationship between the growth number of the
entire neural network and the growth number of each neuron. Combine that with
the adversarial Vapnik-Chervonenkis(VC)-dimension of halfspace classifiers, we
concluded the adversarial VC-dimension of the neural networks with sign
activation functions.","Zetong Qi, T. J. Wilder",2019,http://arxiv.org/abs/1912.08865v1
"Geometry Perspective Of Estimating Learning Capability Of Neural
  Networks","The paper uses statistical and differential geometric motivation to acquire
prior information about the learning capability of an artificial neural network
on a given dataset. The paper considers a broad class of neural networks with
generalized architecture performing simple least square regression with
stochastic gradient descent (SGD). The system characteristics at two critical
epochs in the learning trajectory are analyzed. During some epochs of the
training phase, the system reaches equilibrium with the generalization
capability attaining a maximum. The system can also be coherent with localized,
non-equilibrium states, which is characterized by the stabilization of the
Hessian matrix. The paper proves that neural networks with higher
generalization capability will have a slower convergence rate. The relationship
between the generalization capability with the stability of the neural network
has also been discussed. By correlating the principles of high-energy physics
with the learning theory of neural networks, the paper establishes a variant of
the Complexity-Action conjecture from an artificial neural network perspective.","Ankan Dutta, Arnab Rakshit",2020,http://arxiv.org/abs/2011.04588v2
Optimal Approximation with Sparse Neural Networks and Applications,"We use deep sparsely connected neural networks to measure the complexity of a
function class in $L^2(\mathbb R^d)$ by restricting connectivity and memory
requirement for storing the neural networks. We also introduce representation
system - a countable collection of functions to guide neural networks, since
approximation theory with representation system has been well developed in
Mathematics. We then prove the fundamental bound theorem, implying a quantity
intrinsic to the function class itself can give information about the
approximation ability of neural networks and representation system. We also
provides a method for transferring existing theories about approximation by
representation systems to that of neural networks, greatly amplifying the
practical values of neural networks. Finally, we use neural networks to
approximate B-spline functions, which are used to generate the B-spline curves.
Then, we analyse the complexity of a class called $\beta$ cartoon-like
functions using rate-distortion theory and wedgelets construction.",Khay Boon Hong,2021,http://arxiv.org/abs/2108.06467v1
"Quaternion-Valued Convolutional Neural Network Applied for Acute
  Lymphoblastic Leukemia Diagnosis","The field of neural networks has seen significant advances in recent years
with the development of deep and convolutional neural networks. Although many
of the current works address real-valued models, recent studies reveal that
neural networks with hypercomplex-valued parameters can better capture,
generalize, and represent the complexity of multidimensional data. This paper
explores the quaternion-valued convolutional neural network application for a
pattern recognition task from medicine, namely, the diagnosis of acute
lymphoblastic leukemia. Precisely, we compare the performance of real-valued
and quaternion-valued convolutional neural networks to classify lymphoblasts
from the peripheral blood smear microscopic images. The quaternion-valued
convolutional neural network achieved better or similar performance than its
corresponding real-valued network but using only 34% of its parameters. This
result confirms that quaternion algebra allows capturing and extracting
information from a color image with fewer parameters.","Marco Aurélio Granero, Cristhian Xavier Hernández, Marcos Eduardo Valle",2021,http://arxiv.org/abs/2112.06685v1
On the Use of Neural Networks for Full Waveform Inversion,"Neural networks have recently gained attention in solving inverse problems.
One prominent methodology are Physics-Informed Neural Networks (PINNs) which
can solve both forward and inverse problems. In the paper at hand, full
waveform inversion is the considered inverse problem. The performance of PINNs
is compared against classical adjoint optimization, focusing on three key
aspects: the forward-solver, the neural network Ansatz for the inverse field,
and the sensitivity computation for the gradient-based minimization. Starting
from PINNs, each of these key aspects is adapted individually until the
classical adjoint optimization emerges. It is shown that it is beneficial to
use the neural network only for the discretization of the unknown material
field, where the neural network produces reconstructions without oscillatory
artifacts as typically encountered in classical full waveform inversion
approaches. Due to this finding, a hybrid approach is proposed. It exploits
both the efficient gradient computation with the continuous adjoint method as
well as the neural network Ansatz for the unknown material field. This new
hybrid approach outperforms Physics-Informed Neural Networks and the classical
adjoint optimization in settings of two and three-dimensional examples.","Leon Herrmann, Tim Bürchner, Felix Dietrich, Stefan Kollmannsberger",2023,http://arxiv.org/abs/2303.03260v1
"Provable Identifiability of Two-Layer ReLU Neural Networks via LASSO
  Regularization","LASSO regularization is a popular regression tool to enhance the prediction
accuracy of statistical models by performing variable selection through the
$\ell_1$ penalty, initially formulated for the linear model and its variants.
In this paper, the territory of LASSO is extended to two-layer ReLU neural
networks, a fashionable and powerful nonlinear regression model. Specifically,
given a neural network whose output $y$ depends only on a small subset of input
$\boldsymbol{x}$, denoted by $\mathcal{S}^{\star}$, we prove that the LASSO
estimator can stably reconstruct the neural network and identify
$\mathcal{S}^{\star}$ when the number of samples scales logarithmically with
the input dimension. This challenging regime has been well understood for
linear models while barely studied for neural networks. Our theory lies in an
extended Restricted Isometry Property (RIP)-based analysis framework for
two-layer ReLU neural networks, which may be of independent interest to other
LASSO or neural network settings. Based on the result, we advocate a neural
network-based variable selection method. Experiments on simulated and
real-world datasets show promising performance of the variable selection
approach compared with existing techniques.","Gen Li, Ganghua Wang, Jie Ding",2023,http://arxiv.org/abs/2305.04267v1
$e$PCA: High Dimensional Exponential Family PCA,"Many applications, such as photon-limited imaging and genomics, involve large
datasets with noisy entries from exponential family distributions. It is of
interest to estimate the covariance structure and principal components of the
noiseless distribution. Principal Component Analysis (PCA), the standard method
for this setting, can be inefficient when the noise is non-Gaussian.
  We develop $e$PCA (exponential family PCA), a new methodology for PCA on
exponential family distributions. $e$PCA can be used for dimensionality
reduction and denoising of large data matrices. $e$PCA involves the
eigendecomposition of a new covariance matrix estimator, constructed in a
simple and deterministic way using moment calculations, shrinkage, and random
matrix theory.
  We provide several theoretical justifications for our estimator, including
the finite-sample convergence rate, and the Marchenko-Pastur law in high
dimensions. $e$PCA compares favorably to PCA and various PCA alternatives for
exponential families, in simulations as well as in XFEL and SNP data analysis.
An open-source implementation is available.","Lydia T. Liu, Edgar Dobriban, Amit Singer",2016,http://arxiv.org/abs/1611.05550v2
Bucketed PCA Neural Networks with Neurons Mirroring Signals,"The bucketed PCA neural network (PCA-NN) with transforms is developed here in
an effort to benchmark deep neural networks (DNN's), for problems on supervised
classification. Most classical PCA models apply PCA to the entire training data
set to establish a reductive representation and then employ non-network tools
such as high-order polynomial classifiers. In contrast, the bucketed PCA-NN
applies PCA to individual buckets which are constructed in two consecutive
phases, as well as retains a genuine architecture of a neural network. This
facilitates a fair apple-to-apple comparison to DNN's, esp. to reveal that a
major chunk of accuracy achieved by many impressive DNN's could possibly be
explained by the bucketed PCA-NN (e.g., 96% out of 98% for the MNIST data set
as an example). Compared with most DNN's, the three building blocks of the
bucketed PCA-NN are easier to comprehend conceptually - PCA, transforms, and
bucketing for error correction. Furthermore, unlike the somewhat quasi-random
neurons ubiquitously observed in DNN's, the PCA neurons resemble or mirror the
input signals and are more straightforward to decipher as a result.",Jackie Shen,2021,http://arxiv.org/abs/2108.00605v1
Principal Component Analysis: A Generalized Gini Approach,"A principal component analysis based on the generalized Gini correlation
index is proposed (Gini PCA). The Gini PCA generalizes the standard PCA based
on the variance. It is shown, in the Gaussian case, that the standard PCA is
equivalent to the Gini PCA. It is also proven that the dimensionality reduction
based on the generalized Gini correlation matrix, that relies on city-block
distances, is robust to outliers. Monte Carlo simulations and an application on
cars data (with outliers) show the robustness of the Gini PCA and provide
different interpretations of the results compared with the variance PCA.","Charpentier, Arthur, Mussard, Stephane, Tea Ouraga",2019,http://arxiv.org/abs/1910.10133v1
Sparse PCA on fixed-rank matrices,"Sparse PCA is the optimization problem obtained from PCA by adding a sparsity
constraint on the principal components. Sparse PCA is NP-hard and hard to
approximate even in the single-component case. In this paper we settle the
computational complexity of sparse PCA with respect to the rank of the
covariance matrix. We show that, if the rank of the covariance matrix is a
fixed value, then there is an algorithm that solves sparse PCA to global
optimality, whose running time is polynomial in the number of features. We also
prove a similar result for the version of sparse PCA which requires the
principal components to have disjoint supports.",Alberto Del Pia,2022,http://arxiv.org/abs/2201.02487v1
"When Collaborative Filtering is not Collaborative: Unfairness of PCA for
  Recommendations","We study the fairness of dimensionality reduction methods for
recommendations. We focus on the established method of principal component
analysis (PCA), which identifies latent components and produces a low-rank
approximation via the leading components while discarding the trailing
components. Prior works have defined notions of ""fair PCA""; however, these
definitions do not answer the following question: what makes PCA unfair? We
identify two underlying mechanisms of PCA that induce unfairness at the item
level. The first negatively impacts less popular items, due to the fact that
less popular items rely on trailing latent components to recover their values.
The second negatively impacts the highly popular items, since the leading PCA
components specialize in individual popular items instead of capturing
similarities between items. To address these issues, we develop a
polynomial-time algorithm, Item-Weighted PCA, a modification of PCA that uses
item-specific weights in the objective. On a stylized class of matrices, we
prove that Item-Weighted PCA using a specific set of weights minimizes a
popularity-normalized error metric. Our evaluations on real-world datasets show
that Item-Weighted PCA not only improves overall recommendation quality by up
to $0.1$ item-level AUC-ROC but also improves on both popular and less popular
items.","David Liu, Jackie Baek, Tina Eliassi-Rad",2023,http://arxiv.org/abs/2310.09687v1
"Cancer-Net PCa-Data: An Open-Source Benchmark Dataset for Prostate
  Cancer Clinical Decision Support using Synthetic Correlated Diffusion Imaging
  Data","The recent introduction of synthetic correlated diffusion (CDI$^s$) imaging
has demonstrated significant potential in the realm of clinical decision
support for prostate cancer (PCa). CDI$^s$ is a new form of magnetic resonance
imaging (MRI) designed to characterize tissue characteristics through the joint
correlation of diffusion signal attenuation across different Brownian motion
sensitivities. Despite the performance improvement, the CDI$^s$ data for PCa
has not been previously made publicly available. In our commitment to advance
research efforts for PCa, we introduce Cancer-Net PCa-Data, an open-source
benchmark dataset of volumetric CDI$^s$ imaging data of PCa patients.
Cancer-Net PCa-Data consists of CDI$^s$ volumetric images from a patient cohort
of 200 patient cases, along with full annotations (gland masks, tumor masks,
and PCa diagnosis for each tumor). We also analyze the demographic and label
region diversity of Cancer-Net PCa-Data for potential biases. Cancer-Net
PCa-Data is the first-ever public dataset of CDI$^s$ imaging data for PCa, and
is a part of the global open-source initiative dedicated to advancement in
machine learning and imaging research to aid clinicians in the global fight
against cancer.","Hayden Gunraj, Chi-en Amy Tai, Alexander Wong",2023,http://arxiv.org/abs/2311.11647v1
On the efficiency-loss free ordering-robustness of product-PCA,"This article studies the robustness of the eigenvalue ordering, an important
issue when estimating the leading eigen-subspace by principal component
analysis (PCA). In Yata and Aoshima (2010), cross-data-matrix PCA (CDM-PCA) was
proposed and shown to have smaller bias than PCA in estimating eigenvalues.
While CDM-PCA has the potential to achieve better estimation of the leading
eigen-subspace than the usual PCA, its robustness is not well recognized. In
this article, we first develop a more stable variant of CDM-PCA, which we call
product-PCA (PPCA), that provides a more convenient formulation for theoretical
investigation. Secondly, we prove that, in the presence of outliers, PPCA is
more robust than PCA in maintaining the correct ordering of leading
eigenvalues. The robustness gain in PPCA comes from the random data partition,
and it does not rely on a data down-weighting scheme as most robust statistical
methods do. This enables us to establish the surprising finding that, when
there are no outliers, PPCA and PCA share the same asymptotic distribution.
That is, the robustness gain of PPCA in estimating the leading eigen-subspace
has no efficiency loss in comparison with PCA. Simulation studies and a face
data example are presented to show the merits of PPCA. In conclusion, PPCA has
a good potential to replace the role of the usual PCA in real applications
whether outliers are present or not.","Hung Hung, Su-Yun Huang",2023,http://arxiv.org/abs/2302.11124v3
A Compared Study Between Some Subspace Based Algorithms,"The technology of face recognition has made some progress in recent years.
After studying the PCA, 2DPCA, R1-PCA, L1-PCA, KPCA and KECA algorithms, in
this paper ECA (2DECA) is proposed by extracting features in PCA (2DPCA) based
on Renyi entropy contribution. And then we conduct a study on the 2DL1-PCA and
2DR1-PCA algorithms. On the basis of the experiments, this paper compares the
difference of the recognition accuracy and operational efficiency between the
above algorithms.","Xing Liu, Xiao-Jun Wu, Zhen Liu, He-Feng Yin",2019,http://arxiv.org/abs/1912.10657v1
Efficient fair PCA for fair representation learning,"We revisit the problem of fair principal component analysis (PCA), where the
goal is to learn the best low-rank linear approximation of the data that
obfuscates demographic information. We propose a conceptually simple approach
that allows for an analytic solution similar to standard PCA and can be
kernelized. Our methods have the same complexity as standard PCA, or kernel
PCA, and run much faster than existing methods for fair PCA based on
semidefinite programming or manifold optimization, while achieving similar
results.","Matthäus Kleindessner, Michele Donini, Chris Russell, Muhammad Bilal Zafar",2023,http://arxiv.org/abs/2302.13319v1
Torus Principal Component Analysis with an Application to RNA Structures,"There are several cutting edge applications needing PCA methods for data on
tori and we propose a novel torus-PCA method with important properties that can
be generally applied. There are two existing general methods: tangent space PCA
and geodesic PCA. However, unlike tangent space PCA, our torus-PCA honors the
cyclic topology of the data space whereas, unlike geodesic PCA, our torus-PCA
produces a variety of non-winding, non-dense descriptors. This is achieved by
deforming tori into spheres and then using a variant of the recently developed
principle nested spheres analysis. This PCA analysis involves a step of small
sphere fitting and we provide an improved test to avoid overfitting. However,
deforming tori into spheres creates singularities. We introduce a data-adaptive
pre-clustering technique to keep the singularities away from the data. For the
frequently encountered case that the residual variance around the PCA main
component is small, we use a post-mode hunting technique for more fine-grained
clustering. Thus in general, there are three successive interrelated key steps
of torus-PCA in practice: pre-clustering, deformation, and post-mode hunting.
We illustrate our method with two recently studied RNA structure (tori) data
sets: one is a small RNA data set which is established as the benchmark for PCA
and we validate our method through this data. Another is a large RNA data set
(containing the small RNA data set) for which we show that our method provides
interpretable principal components as well as giving further insight into its
structure.","Benjamin Eltzner, Stephan Huckemann, Kanti V. Mardia",2015,http://arxiv.org/abs/1511.04993v1
"Accelerating Wireless Federated Learning via Nesterov's Momentum and
  Distributed Principle Component Analysis","A wireless federated learning system is investigated by allowing a server and
workers to exchange uncoded information via orthogonal wireless channels. Since
the workers frequently upload local gradients to the server via
bandwidth-limited channels, the uplink transmission from the workers to the
server becomes a communication bottleneck. Therefore, a one-shot distributed
principle component analysis (PCA) is leveraged to reduce the dimension of
uploaded gradients such that the communication bottleneck is relieved. A
PCA-based wireless federated learning (PCA-WFL) algorithm and its accelerated
version (i.e., PCA-AWFL) are proposed based on the low-dimensional gradients
and the Nesterov's momentum. For the non-convex loss functions, a finite-time
analysis is performed to quantify the impacts of system hyper-parameters on the
convergence of the PCA-WFL and PCA-AWFL algorithms. The PCA-AWFL algorithm is
theoretically certified to converge faster than the PCA-WFL algorithm. Besides,
the convergence rates of PCA-WFL and PCA-AWFL algorithms quantitatively reveal
the linear speedup with respect to the number of workers over the vanilla
gradient descent algorithm. Numerical results are used to demonstrate the
improved convergence rates of the proposed PCA-WFL and PCA-AWFL algorithms over
the benchmarks.","Yanjie Dong, Luya Wang, Yuanfang Chi, Jia Wang, Haijun Zhang, Fei Richard Yu, Victor C. M. Leung, Xiping Hu",2023,http://arxiv.org/abs/2303.17885v1
Comparison of PCA with ICA from data distribution perspective,"We performed an empirical comparison of ICA and PCA algorithms by applying
them on two simulated noisy time series with varying distribution parameters
and level of noise. In general, ICA shows better results than PCA because it
takes into account higher moments of data distribution. On the other hand, PCA
remains quite sensitive to the level of correlations among signals.",Miron Ivanov,2017,http://arxiv.org/abs/1709.10222v1
"PCA of high dimensional random walks with comparison to neural network
  training","One technique to visualize the training of neural networks is to perform PCA
on the parameters over the course of training and to project to the subspace
spanned by the first few PCA components. In this paper we compare this
technique to the PCA of a high dimensional random walk. We compute the
eigenvalues and eigenvectors of the covariance of the trajectory and prove that
in the long trajectory and high dimensional limit most of the variance is in
the first few PCA components, and that the projection of the trajectory onto
any subspace spanned by PCA components is a Lissajous curve. We generalize
these results to a random walk with momentum and to an Ornstein-Uhlenbeck
processes (i.e., a random walk in a quadratic potential) and show that in high
dimensions the walk is not mean reverting, but will instead be trapped at a
fixed distance from the minimum. We finally compare the distribution of PCA
variances and the PCA projected training trajectories of a linear model trained
on CIFAR-10 and ResNet-50-v2 trained on Imagenet and find that the distribution
of PCA variances resembles a random walk with drift.","Joseph M. Antognini, Jascha Sohl-Dickstein",2018,http://arxiv.org/abs/1806.08805v1
Internal Partial Combinatory Algebras and their Slices,"A partial combinatory algebra (PCA) is a set equipped with a partial binary
operation that models a notion of computability. This paper studies a
generalization of PCAs, introduced by W. Stekelenburg, where a PCA is not a set
but an object in a given regular category. The corresponding class of
categories of assemblies is closed both under taking small products and under
slicing, which is to be contrasted with the situation for ordinary PCAs. We
describe these two constructions explicitly at the level of PCAs, allowing us
to compute a number of examples of products and slices of PCAs. Moreover, we
show how PCAs can be transported along regular functors, enabling us to compare
PCAs constructed over different base categories. Via a Grothendieck
construction, this leads to a (2-)category whose objects are PCAs and whose
arrows are generalized applicative morphisms. This category has small products,
which correspond to the small products of categories of assemblies, and it has
finite coproducts in a weak sense. Finally, we give a criterion when a functor
between categories of assemblies that is induced by an applicative morphism has
a right adjoint, by generalizing the notion of computational density introduced
by P. Hofstra and J. van Oosten.",Jetze Zoethout,2019,http://arxiv.org/abs/1910.09816v1
Toroidal PCA via density ridges,"Principal Component Analysis (PCA) is a well-known linear dimension-reduction
technique designed for Euclidean data. In a wide spectrum of applied fields,
however, it is common to observe multivariate circular data (also known as
toroidal data), rendering spurious the use of PCA on it due to the periodicity
of its support. This paper introduces Toroidal Ridge PCA (TR-PCA), a novel
construction of PCA for bivariate circular data that leverages the concept of
density ridges as a flexible first principal component analog. Two reference
bivariate circular distributions, the bivariate sine von Mises and the
bivariate wrapped Cauchy, are employed as the parametric distributional basis
of TR-PCA. Efficient algorithms are presented to compute density ridges for
these two distribution models. A complete PCA methodology adapted to toroidal
data (including scores, variance decomposition, and resolution of edge cases)
is introduced and implemented in the companion R package ridgetorus. The
usefulness of TR-PCA is showcased with a novel case study involving the
analysis of ocean currents on the coast of Santa Barbara.","Eduardo García-Portugués, Arturo Prieto-Tirado",2022,http://arxiv.org/abs/2212.10856v2
Streaming Probabilistic PCA for Missing Data with Heteroscedastic Noise,"Streaming principal component analysis (PCA) is an integral tool in
large-scale machine learning for rapidly estimating low-dimensional subspaces
of very high dimensional and high arrival-rate data with missing entries and
corrupting noise. However, modern trends increasingly combine data from a
variety of sources, meaning they may exhibit heterogeneous quality across
samples. Since standard streaming PCA algorithms do not account for non-uniform
noise, their subspace estimates can quickly degrade. On the other hand, the
recently proposed Heteroscedastic Probabilistic PCA Technique (HePPCAT)
addresses this heterogeneity, but it was not designed to handle missing entries
and streaming data, nor does it adapt to non-stationary behavior in time series
data. This paper proposes the Streaming HeteroscedASTic Algorithm for PCA
(SHASTA-PCA) to bridge this divide. SHASTA-PCA employs a stochastic alternating
expectation maximization approach that jointly learns the low-rank latent
factors and the unknown noise variances from streaming data that may have
missing entries and heteroscedastic noise, all while maintaining a low memory
and computational footprint. Numerical experiments validate the superior
subspace estimation of our method compared to state-of-the-art streaming PCA
algorithms in the heteroscedastic setting. Finally, we illustrate SHASTA-PCA
applied to highly-heterogeneous real data from astronomy.","Kyle Gilman, David Hong, Jeffrey A. Fessler, Laura Balzano",2023,http://arxiv.org/abs/2310.06277v1
TL-PCA: Transfer Learning of Principal Component Analysis,"Principal component analysis (PCA) can be significantly limited when there is
too few examples of the target data of interest. We propose a transfer learning
approach to PCA (TL-PCA) where knowledge from a related source task is used in
addition to the scarce data of a target task. Our TL-PCA has two versions, one
that uses a pretrained PCA solution of the source task, and another that uses
the source data. Our proposed approach extends the PCA optimization objective
with a penalty on the proximity of the target subspace and the source subspace
as given by the pretrained source model or the source data. This optimization
is solved by eigendecomposition for which the number of data-dependent
eigenvectors (i.e., principal directions of TL-PCA) is not limited to the
number of target data examples, which is a root cause that limits the standard
PCA performance. Accordingly, our results for image datasets show that the
representation of test data is improved by TL-PCA for dimensionality reduction
where the learned subspace dimension is lower or higher than the number of
target data examples.","Sharon Hendy, Yehuda Dar",2024,http://arxiv.org/abs/2410.10805v1
"Intelligent Reflecting Surface Aided Pilot Contamination Attack and Its
  Countermeasure","Pilot contamination attack (PCA) in a time division duplex wireless
communication system is considered, where an eavesdropper (Eve) attacks the
reverse pilot transmission phase in order to wiretap the data transmitted from
a transmitter, Alice, to a receiver, Bob. We propose a new PCA scheme for Eve,
wherein Eve does not emit any signal by itself but uses an intelligent
reflecting surface (IRS) to reflect the pilot sent by Bob to Alice. The
proposed new PCA scheme, referred to as IRS-PCA, increases the signal leakage
from Alice to the IRS during the data transmission phase, which is then
reflected by the IRS to Eve in order to improve the wiretapping capability of
Eve. The proposed IRS-PCA scheme disables many existing countermeasures on PCA
due to the fact that with IRS-PCA, Eve no longer needs to know the pilot
sequence of Bob, and therefore, poses severe threat to the security of the
legitimate wireless communication system. In view of this, the problems of 1)
IRS-PCA detection and 2) secure transmission under IRSPCA are considered in
this paper. For IRS-PCA detection, a generalized cumulative sum (GCUSUM)
detection procedure is proposed based on the framework of quickest detection,
aiming at detecting the occurrence of IRS-PCA as soon as possible once it
occurs. For secure transmission under IRS-PCA, a cooperative channel estimation
scheme is proposed to estimate the channel of the IRS, based on which
zero-forcing beamforming is designed to reduce signal leakage.","Ke-Wen Huang, Hui-Ming Wang",2020,http://arxiv.org/abs/2009.08512v1
"A co-kurtosis PCA based dimensionality reduction with nonlinear
  reconstruction using neural networks","For turbulent reacting flows, identification of low-dimensional
representations of the thermo-chemical state space is vitally important,
primarily to significantly reduce the computational cost of device-scale
simulations. Principal component analysis (PCA), and its variants, is a widely
employed class of methods. Recently, an alternative technique that focuses on
higher-order statistical interactions, co-kurtosis PCA (CoK-PCA), has been
shown to effectively provide a low-dimensional representation by capturing the
stiff chemical dynamics associated with spatiotemporally localized reaction
zones. While its effectiveness has only been demonstrated based on a priori
analysis with linear reconstruction, in this work, we employ nonlinear
techniques to reconstruct the full thermo-chemical state and evaluate the
efficacy of CoK-PCA compared to PCA. Specifically, we combine a CoK-PCA/PCA
based dimensionality reduction (encoding) with an artificial neural network
(ANN) based reconstruction (decoding) and examine a priori the reconstruction
errors of the thermo-chemical state. In addition, we evaluate the errors in
species production rates and heat release rates that are nonlinear functions of
the reconstructed state as a measure of the overall accuracy of the
dimensionality reduction technique. We employ four datasets to assess
CoK-PCA/PCA coupled with ANN-based reconstruction: a homogeneous reactor for
autoignition of an ethylene/air mixture that has conventional single-stage
ignition kinetics, a dimethyl ether (DME)/air mixture which has two-stage
ignition kinetics, a one-dimensional freely propagating premixed ethylene/air
laminar flame, and a two-dimensional homogeneous charge compression ignition of
ethanol. The analyses demonstrate the robustness of the CoK-PCA based
low-dimensional manifold with ANN reconstruction in accurately capturing the
data, specifically from the reaction zones.","Dibyajyoti Nayak, Anirudh Jonnalagadda, Uma Balakrishnan, Hemanth Kolla, Konduri Aditya",2023,http://arxiv.org/abs/2307.03289v1
Log-PCA versus Geodesic PCA of histograms in the Wasserstein space,"This paper is concerned by the statistical analysis of data sets whose
elements are random histograms. For the purpose of learning principal modes of
variation from such data, we consider the issue of computing the PCA of
histograms with respect to the 2-Wasserstein distance between probability
measures. To this end, we propose to compare the methods of log-PCA and
geodesic PCA in the Wasserstein space as introduced by Bigot et al. (2015) and
Seguy and Cuturi (2015). Geodesic PCA involves solving a non-convex
optimization problem. To solve it approximately, we propose a novel
forward-backward algorithm. This allows a detailed comparison between log-PCA
and geodesic PCA of one-dimensional histograms, which we carry out using
various data sets, and stress the benefits and drawbacks of each method. We
extend these results for two-dimensional data and compare both methods in that
setting.","Elsa Cazelles, Vivien Seguy, Jérémie Bigot, Marco Cuturi, Nicolas Papadakis",2017,http://arxiv.org/abs/1708.08143v1
Unsupervised and Supervised Principal Component Analysis: Tutorial,"This is a detailed tutorial paper which explains the Principal Component
Analysis (PCA), Supervised PCA (SPCA), kernel PCA, and kernel SPCA. We start
with projection, PCA with eigen-decomposition, PCA with one and multiple
projection directions, properties of the projection matrix, reconstruction
error minimization, and we connect to autoencoder. Then, PCA with singular
value decomposition, dual PCA, and kernel PCA are covered. SPCA using both
scoring and Hilbert-Schmidt independence criterion are explained. Kernel SPCA
using both direct and dual approaches are then introduced. We cover all cases
of projection and reconstruction of training and out-of-sample data. Finally,
some simulations are provided on Frey and AT&T face datasets for verifying the
theory in practice.","Benyamin Ghojogh, Mark Crowley",2019,http://arxiv.org/abs/1906.03148v2
"Empirical Bayes Covariance Decomposition, and a solution to the Multiple
  Tuning Problem in Sparse PCA","Sparse Principal Components Analysis (PCA) has been proposed as a way to
improve both interpretability and reliability of PCA. However, use of sparse
PCA in practice is hindered by the difficulty of tuning the multiple
hyperparameters that control the sparsity of different PCs (the ""multiple
tuning problem"", MTP). Here we present a solution to the MTP using Empirical
Bayes methods. We first introduce a general formulation for penalized PCA of a
data matrix $\mathbf{X}$, which includes some existing sparse PCA methods as
special cases. We show that this formulation also leads to a penalized
decomposition of the covariance (or Gram) matrix, $\mathbf{X}^T\mathbf{X}$. We
introduce empirical Bayes versions of these penalized problems, in which the
penalties are determined by prior distributions that are estimated from the
data by maximum likelihood rather than cross-validation. The resulting
""Empirical Bayes Covariance Decomposition"" provides a principled and efficient
solution to the MTP in sparse PCA, and one that can be immediately extended to
incorporate other structural assumptions (e.g. non-negative PCA). We illustrate
the effectiveness of this approach on both simulated and real data examples.","Joonsuk Kang, Matthew Stephens",2023,http://arxiv.org/abs/2312.03274v1
"On the asymptotic properties of product-PCA under the high-dimensional
  setting","Principal component analysis (PCA) is a widely used dimension reduction
method, but its performance is known to be non-robust to outliers. Recently,
product-PCA (PPCA) has been shown to possess the efficiency-loss free
ordering-robustness property: (i) in the absence of outliers, PPCA and PCA
share the same asymptotic distributions; (ii), in the presence of outliers,
PPCA is more ordering-robust than PCA in estimating the leading eigenspace.
PPCA is thus different from the conventional robust PCA methods, and may
deserve further investigations. In this article, we study the high-dimensional
statistical properties of the PPCA eigenvalues via the techniques of random
matrix theory. In particular, we derive the critical value for being distant
spiked eigenvalues, the limiting values of the sample spiked eigenvalues, and
the limiting spectral distribution of PPCA. Similar to the case of PCA, the
explicit forms of the asymptotic properties of PPCA become available under the
special case of the simple spiked model. These results enable us to more
clearly understand the superiorities of PPCA in comparison with PCA. Numerical
studies are conducted to verify our results.","Hung Hung, Chi-Chun Yeh, Su-Yun Huang",2024,http://arxiv.org/abs/2407.19725v2
Extensions of Scott's Graph Model and Kleene's Second Algebra,"We use a way to extend partial combinatory algebras (pcas) by forcing them to
represent certain functions. In the case of Scott's Graph model, equality is
computable relative to the complement function. However, the converse is not
true. This creates a hierarchy of pcas which relates to similar structures of
extensions on other pcas. We study one such structure on Kleene's second model
and one on a pca equivalent but not isomorphic to it. For the recursively
enumerable sub pca of the Graph model, results differ as we can compute the
(partial) complement function using the equality.","Jaap van Oosten, Niels Voorneveld",2016,http://arxiv.org/abs/1610.04050v1
"Shapley Values of Reconstruction Errors of PCA for Explaining Anomaly
  Detection","We present a method to compute the Shapley values of reconstruction errors of
principal component analysis (PCA), which is particularly useful in explaining
the results of anomaly detection based on PCA. Because features are usually
correlated when PCA-based anomaly detection is applied, care must be taken in
computing a value function for the Shapley values. We utilize the probabilistic
view of PCA, particularly its conditional distribution, to exactly compute a
value function for the Shapely values. We also present numerical examples,
which imply that the Shapley values are advantageous for explaining detected
anomalies than raw reconstruction errors of each feature.",Naoya Takeishi,2019,http://arxiv.org/abs/1909.03495v2
"2DR1-PCA and 2DL1-PCA: two variant 2DPCA algorithms based on none L2
  norm","In this paper, two novel methods: 2DR1-PCA and 2DL1-PCA are proposed for face
recognition. Compared to the traditional 2DPCA algorithm, 2DR1-PCA and 2DL1-PCA
are based on the R1 norm and L1 norm, respectively. The advantage of these
proposed methods is they are less sensitive to outliers. These proposed methods
are tested on the ORL, YALE and XM2VTS databases and the performance of the
related methods is compared experimentally.","Xing Liu, Xiao-Jun Wu, Zi-Qi Li",2019,http://arxiv.org/abs/1912.10768v1
"Fourier spectra for nonuniform phase-shifting algorithms based on
  principal component analysis","We develop an error-free, nonuniform phase-stepping algorithm (nPSA) based on
principal component analysis (PCA). PCA-based algorithms typically give
phase-demodulation errors when applied to nonuniform phase-shifted
interferograms. We present a straightforward way to correct those PCA
phase-demodulation errors. We give mathematical formulas to fully analyze
PCA-based nPSA (PCA-nPSA). These formulas give a) the PCA-nPSA frequency
transfer function (FTF), b) its corrected Lissajous figure, c) the corrected
PCA-nPSA formula, d) its harmonic robustness, and e) its signal-to-noise-ratio
(SNR). We show that the PCA-nPSA can be seen as a linear quadrature filter, and
as consequence, one can find its FTF. Using the FTF, we show why plain PCA
often fails to demodulate nonuniform phase-shifted fringes. Previous works on
PCA-nPSA (without FTF), give specific numerical/experimental fringe data to
""visually demonstrate"" that their new nPSA works better than competitors. This
often leads to biased/favorable fringe pattern selections which ""visually
demonstrate"" the superior performance of their new nPSA. This biasing is herein
totally avoided because we provide figures-of-merit formulas based on linear
systems and stochastic process theories. However, and for illustrative purposes
only, we provide specific fringe data phase-demodulation, including
comprehensive analysis and comparisons.","Manuel Servin, Moises Padilla, Guillermo Garnica, Gonzalo Paez",2019,http://arxiv.org/abs/1904.01071v1
"Supervised Discriminative Sparse PCA for Com-Characteristic Gene
  Selection and Tumor Classification on Multiview Biological Data","Principal Component Analysis (PCA) has been used to study the pathogenesis of
diseases. To enhance the interpretability of classical PCA, various improved
PCA methods have been proposed to date. Among these, a typical method is the
so-called sparse PCA, which focuses on seeking sparse loadings. However, the
performance of these methods is still far from satisfactory due to their
limitation of using unsupervised learning methods; moreover, the class
ambiguity within the sample is high. To overcome this problem, this study
developed a new PCA method, which is named the Supervised Discriminative Sparse
PCA (SDSPCA). The main innovation of this method is the incorporation of
discriminative information and sparsity into the PCA model. Specifically, in
contrast to the traditional sparse PCA, which imposes sparsity on the loadings,
here, sparse components are obtained to represent the data. Furthermore, via
linear transformation, the sparse components approximate the given label
information. On the one hand, sparse components improve interpretability over
traditional PCA, while on the other hand, they are have discriminative
abilities suitable for classification purposes. A simple algorithm is developed
and its convergence proof is provided. The SDSPCA has been applied to common
characteristic gene selection (com-characteristic gene) and tumor
classification on multi-view biological data. The sparsity and classification
performance of the SDSPCA are empirically verified via abundant, reasonable,
and effective experiments, and the obtained results demonstrate that SDSPCA
outperforms other state-of-the-art methods.","Chun-Mei Feng, Yong Xu, Jin-Xing Liu, Ying-Lian Gao, Chun-Hou Zheng",2019,http://arxiv.org/abs/1905.11837v1
"FAST-PCA: A Fast and Exact Algorithm for Distributed Principal Component
  Analysis","Principal Component Analysis (PCA) is a fundamental data preprocessing tool
in the world of machine learning. While PCA is often thought of as a
dimensionality reduction method, the purpose of PCA is actually two-fold:
dimension reduction and uncorrelated feature learning. Furthermore, the
enormity of the dimensions and sample size in the modern day datasets have
rendered the centralized PCA solutions unusable. In that vein, this paper
reconsiders the problem of PCA when data samples are distributed across nodes
in an arbitrarily connected network. While a few solutions for distributed PCA
exist, those either overlook the uncorrelated feature learning aspect of the
PCA, tend to have high communication overhead that makes them inefficient
and/or lack `exact' or `global' convergence guarantees. To overcome these
aforementioned issues, this paper proposes a distributed PCA algorithm termed
FAST-PCA (Fast and exAct diSTributed PCA). The proposed algorithm is efficient
in terms of communication and is proven to converge linearly and exactly to the
principal components, leading to dimension reduction as well as uncorrelated
features. The claims are further supported by experimental results.","Arpita Gang, Waheed U. Bajwa",2021,http://arxiv.org/abs/2108.12373v2
"Fair principal component analysis (PCA): minorization-maximization
  algorithms for Fair PCA, Fair Robust PCA and Fair Sparse PCA","In this paper we propose a new iterative algorithm to solve the fair PCA
(FPCA) problem. We start with the max-min fair PCA formulation originally
proposed in [1] and derive a simple and efficient iterative algorithm which is
based on the minorization-maximization (MM) approach. The proposed algorithm
relies on the relaxation of a semi-orthogonality constraint which is proved to
be tight at every iteration of the algorithm. The vanilla version of the
proposed algorithm requires solving a semi-definite program (SDP) at every
iteration, which can be further simplified to a quadratic program by
formulating the dual of the surrogate maximization problem. We also propose two
important reformulations of the fair PCA problem: a) fair robust PCA -- which
can handle outliers in the data, and b) fair sparse PCA -- which can enforce
sparsity on the estimated fair principal components. The proposed algorithms
are computationally efficient and monotonically increase their respective
design objectives at every iteration. An added feature of the proposed
algorithms is that they do not require the selection of any hyperparameter
(except for the fair sparse PCA case where a penalty parameter that controls
the sparsity has to be chosen by the user). We numerically compare the
performance of the proposed methods with two of the state-of-the-art approaches
on synthetic data sets and a real-life data set.","Prabhu Babu, Petre Stoica",2023,http://arxiv.org/abs/2305.05963v1
"A Deep-Learning-Based Geological Parameterization for History Matching
  Complex Models","A new low-dimensional parameterization based on principal component analysis
(PCA) and convolutional neural networks (CNN) is developed to represent complex
geological models. The CNN-PCA method is inspired by recent developments in
computer vision using deep learning. CNN-PCA can be viewed as a generalization
of an existing optimization-based PCA (O-PCA) method. Both CNN-PCA and O-PCA
entail post-processing a PCA model to better honor complex geological features.
In CNN-PCA, rather than use a histogram-based regularization as in O-PCA, a new
regularization involving a set of metrics for multipoint statistics is
introduced. The metrics are based on summary statistics of the nonlinear filter
responses of geological models to a pre-trained deep CNN. In addition, in the
CNN-PCA formulation presented here, a convolutional neural network is trained
as an explicit transform function that can post-process PCA models quickly.
CNN-PCA is shown to provide both unconditional and conditional realizations
that honor the geological features present in reference SGeMS geostatistical
realizations for a binary channelized system. Flow statistics obtained through
simulation of random CNN-PCA models closely match results for random SGeMS
models for a demanding case in which O-PCA models lead to significant
discrepancies. Results for history matching are also presented. In this
assessment CNN-PCA is applied with derivative-free optimization, and a subspace
randomized maximum likelihood method is used to provide multiple posterior
models. Data assimilation and significant uncertainty reduction are achieved
for existing wells, and physically reasonable predictions are also obtained for
new wells. Finally, the CNN-PCA method is extended to a more complex
non-stationary bimodal deltaic fan system, and is shown to provide high-quality
realizations for this challenging example.","Yimin Liu, Wenyue Sun, Louis J. Durlofsky",2018,http://arxiv.org/abs/1807.02716v1
Semi-Orthogonal Multilinear PCA with Relaxed Start,"Principal component analysis (PCA) is an unsupervised method for learning
low-dimensional features with orthogonal projections. Multilinear PCA methods
extend PCA to deal with multidimensional data (tensors) directly via
tensor-to-tensor projection or tensor-to-vector projection (TVP). However,
under the TVP setting, it is difficult to develop an effective multilinear PCA
method with the orthogonality constraint. This paper tackles this problem by
proposing a novel Semi-Orthogonal Multilinear PCA (SO-MPCA) approach. SO-MPCA
learns low-dimensional features directly from tensors via TVP by imposing the
orthogonality constraint in only one mode. This formulation results in more
captured variance and more learned features than full orthogonality. For better
generalization, we further introduce a relaxed start (RS) strategy to get
SO-MPCA-RS by fixing the starting projection vectors, which increases the bias
and reduces the variance of the learning model. Experiments on both face (2D)
and gait (3D) data demonstrate that SO-MPCA-RS outperforms other competing
algorithms on the whole, and the relaxed start strategy is also effective for
other TVP-based PCA methods.","Qiquan Shi, Haiping Lu",2015,http://arxiv.org/abs/1504.08142v2
"Sparse Generalized Principal Component Analysis for Large-scale
  Applications beyond Gaussianity","Principal Component Analysis (PCA) is a dimension reduction technique. It
produces inconsistent estimators when the dimensionality is moderate to high,
which is often the problem in modern large-scale applications where algorithm
scalability and model interpretability are difficult to achieve, not to mention
the prevalence of missing values. While existing sparse PCA methods alleviate
inconsistency, they are constrained to the Gaussian assumption of classical PCA
and fail to address algorithm scalability issues. We generalize sparse PCA to
the broad exponential family distributions under high-dimensional setup, with
built-in treatment for missing values. Meanwhile we propose a family of
iterative sparse generalized PCA (SG-PCA) algorithms such that despite the
non-convexity and non-smoothness of the optimization task, the loss function
decreases in every iteration. In terms of ease and intuitive parameter tuning,
our sparsity-inducing regularization is far superior to the popular Lasso.
Furthermore, to promote overall scalability, accelerated gradient is integrated
for fast convergence, while a progressive screening technique gradually
squeezes out nuisance dimensions of a large-scale problem for feasible
optimization. High-dimensional simulation and real data experiments demonstrate
the efficiency and efficacy of SG-PCA.","Qiaoya Zhang, Yiyuan She",2015,http://arxiv.org/abs/1512.03883v2
"On the performance overhead tradeoff of distributed principal component
  analysis via data partitioning","Principal component analysis (PCA) is not only a fundamental dimension
reduction method, but is also a widely used network anomaly detection
technique. Traditionally, PCA is performed in a centralized manner, which has
poor scalability for large distributed systems, on account of the large network
bandwidth cost required to gather the distributed state at a fusion center.
Consequently, several recent works have proposed various distributed PCA
algorithms aiming to reduce the communication overhead incurred by PCA without
losing its inferential power. This paper evaluates the tradeoff between
communication cost and solution quality of two distributed PCA algorithms on a
real domain name system (DNS) query dataset from a large network. We also apply
the distributed PCA algorithm in the area of network anomaly detection and
demonstrate that the detection accuracy of both distributed PCA-based methods
has little degradation in quality, yet achieves significant savings in
communication bandwidth.","Ni An, Steven Weber",2015,http://arxiv.org/abs/1512.05172v3
Towards a Theoretical Analysis of PCA for Heteroscedastic Data,"Principal Component Analysis (PCA) is a method for estimating a subspace
given noisy samples. It is useful in a variety of problems ranging from
dimensionality reduction to anomaly detection and the visualization of high
dimensional data. PCA performs well in the presence of moderate noise and even
with missing data, but is also sensitive to outliers. PCA is also known to have
a phase transition when noise is independent and identically distributed;
recovery of the subspace sharply declines at a threshold noise variance.
Effective use of PCA requires a rigorous understanding of these behaviors. This
paper provides a step towards an analysis of PCA for samples with
heteroscedastic noise, that is, samples that have non-uniform noise variances
and so are no longer identically distributed. In particular, we provide a
simple asymptotic prediction of the recovery of a one-dimensional subspace from
noisy heteroscedastic samples. The prediction enables: a) easy and efficient
calculation of the asymptotic performance, and b) qualitative reasoning to
understand how PCA is impacted by heteroscedasticity (such as outliers).","David Hong, Laura Balzano, Jeffrey A. Fessler",2016,http://arxiv.org/abs/1610.03595v1
Principal Component Analysis: A Natural Approach to Data Exploration,"Principal component analysis (PCA) is often used for analyzing data in the
most diverse areas. In this work, we report an integrated approach to several
theoretical and practical aspects of PCA. We start by providing, in an
intuitive and accessible manner, the basic principles underlying PCA and its
applications. Next, we present a systematic, though no exclusive, survey of
some representative works illustrating the potential of PCA applications to a
wide range of areas. An experimental investigation of the ability of PCA for
variance explanation and dimensionality reduction is also developed, which
confirms the efficacy of PCA and also shows that standardizing or not the
original data can have important effects on the obtained results. Overall, we
believe the several covered issues can assist researchers from the most diverse
areas in using and interpreting PCA.","Felipe L. Gewers, Gustavo R. Ferreira, Henrique F. de Arruda, Filipi N. Silva, Cesar H. Comin, Diego R. Amancio, Luciano da F. Costa",2018,http://arxiv.org/abs/1804.02502v2
PCA by Optimisation of Symmetric Functions has no Spurious Local Optima,"Principal Component Analysis (PCA) finds the best linear representation of
data, and is an indispensable tool in many learning and inference tasks.
Classically, principal components of a dataset are interpreted as the
directions that preserve most of its ""energy"", an interpretation that is
theoretically underpinned by the celebrated Eckart-Young-Mirsky Theorem.
  This paper introduces many other ways of performing PCA, with various
geometric interpretations, and proves that the corresponding family of
non-convex programs have no spurious local optima, while possessing only strict
saddle points. These programs therefore loosely behave like convex problems and
can be efficiently solved to global optimality, for example, with certain
variants of the stochastic gradient descent.
  Beyond providing new geometric interpretations and enhancing our theoretical
understanding of PCA, our findings might pave the way for entirely new
approaches to structured dimensionality reduction, such as sparse PCA and
nonnegative matrix factorisation. More specifically, we study an unconstrained
formulation of PCA using determinant optimisation that might provide an elegant
alternative to the deflating scheme commonly used in sparse PCA.","Raphael A. Hauser, Armin Eftekhari",2018,http://arxiv.org/abs/1805.07459v3
Discriminative Principal Component Analysis: A REVERSE THINKING,"In this paper, we propose a novel approach named by Discriminative Principal
Component Analysis which is abbreviated as Discriminative PCA in order to
enhance separability of PCA by Linear Discriminant Analysis (LDA). The proposed
method performs feature extraction by determining a linear projection that
captures the most scattered discriminative information. The most innovation of
Discriminative PCA is performing PCA on discriminative matrix rather than
original sample matrix. For calculating the required discriminative matrix
under low complexity, we exploit LDA on a converted matrix to obtain
within-class matrix and between-class matrix thereof. During the computation
process, we utilise direct linear discriminant analysis (DLDA) to solve the
encountered SSS problem. For evaluating the performances of Discriminative PCA
in face recognition, we analytically compare it with DLAD and PCA on four well
known facial databases, they are PIE, FERET, YALE and ORL respectively. Results
in accuracy and running time obtained by nearest neighbour classifier are
compared when different number of training images per person used. Not only the
superiority and outstanding performance of Discriminative PCA showed in
recognition rate, but also the comparable results of running time.",Hanli Qiao,2019,http://arxiv.org/abs/1903.04963v1
Empirical Bayes PCA in high dimensions,"When the dimension of data is comparable to or larger than the number of data
samples, Principal Components Analysis (PCA) may exhibit problematic
high-dimensional noise. In this work, we propose an Empirical Bayes PCA method
that reduces this noise by estimating a joint prior distribution for the
principal components. EB-PCA is based on the classical Kiefer-Wolfowitz
nonparametric MLE for empirical Bayes estimation, distributional results
derived from random matrix theory for the sample PCs, and iterative refinement
using an Approximate Message Passing (AMP) algorithm. In theoretical ""spiked""
models, EB-PCA achieves Bayes-optimal estimation accuracy in the same settings
as an oracle Bayes AMP procedure that knows the true priors. Empirically,
EB-PCA significantly improves over PCA when there is strong prior structure,
both in simulation and on quantitative benchmarks constructed from the 1000
Genomes Project and the International HapMap Project. An illustration is
presented for analysis of gene expression data obtained by single-cell RNA-seq.","Xinyi Zhong, Chang Su, Zhou Fan",2020,http://arxiv.org/abs/2012.11676v3
Improved sparse PCA method for face and image recognition,"Face recognition is the very significant field in pattern recognition area.
It has multiple applications in military and finance, to name a few. In this
paper, the combination of the sparse PCA with the nearest-neighbor method (and
with the kernel ridge regression method) will be proposed and will be applied
to solve the face recognition problem. Experimental results illustrate that the
accuracy of the combination of the sparse PCA method (using the proximal
gradient method and the FISTA method) and one specific classification system
may be lower than the accuracy of the combination of the PCA method and one
specific classification system but sometimes the combination of the sparse PCA
method (using the proximal gradient method or the FISTA method) and one
specific classification system leads to better accuracy. Moreover, we recognize
that the process computing the sparse PCA algorithm using the FISTA method is
always faster than the process computing the sparse PCA algorithm using the
proximal gradient method.","Loc Hoang Tran, Tuan Tran, An Mai",2021,http://arxiv.org/abs/2112.00207v1
Ensemble Principal Component Analysis,"Efficient representations of data are essential for processing, exploration,
and human understanding, and Principal Component Analysis (PCA) is one of the
most common dimensionality reduction techniques used for the analysis of large,
multivariate datasets today. Two well-known limitations of the method include
sensitivity to outliers and noise and no clear methodology for the uncertainty
quantification of the principle components or their associated explained
variances. Whereas previous work has focused on each of these problems
individually, we propose a scalable method called Ensemble PCA (EPCA) that
addresses them simultaneously for data which has an inherently low-rank
structure. EPCA combines boostrapped PCA with k-means cluster analysis to
handle challenges associated with sign-ambiguity and the re-ordering of
components in the PCA subsamples. EPCA provides a noise-resistant extension of
PCA that lends itself naturally to uncertainty quantification. We test EPCA on
data corrupted with white noise, sparse noise, and outliers against both
classical PCA and Robust PCA (RPCA) and show that EPCA performs competitively
across different noise scenarios, with a clear advantage on datasets containing
outliers and orders of magnitude reduction in computational cost compared to
RPCA.","Olga Dorabiala, Aleksandr Aravkin, J. Nathan Kutz",2023,http://arxiv.org/abs/2311.01826v1
"Principal Component Analysis studies of turbulence in optically thick
  gas","In this work we investigate the Principal Component Analysis (PCA)
sensitivity to the velocity power spectrum in high opacity regimes of the
interstellar medium (ISM). For our analysis we use synthetic
Position-Position-Velocity (PPV) cubes of fractional Brownian motion (fBm) and
magnetohydrodynamics (MHD) simulations, post processed to include radiative
transfer effects from CO. We find that PCA analysis is very different from the
tools based on the traditional power spectrum of PPV data cubes. Our major
finding is that PCA is also sensitive to the phase information of PPV cubes and
this allows PCA to detect the changes of the underlying velocity and density
spectra at high opacities, where the spectral analysis of the maps provides the
universal -3 spectrum in accordance with the predictions of Lazarian \&
Pogosyan (2004) theory. This makes PCA potentially a valuable tool for studies
of turbulence at high opacities provided that the proper gauging of the PCA
index is made. The later, however, we found to be not easy, as the PCA results
change in an irregular way for data with high sonic Mach numbers. This is in
contrast to synthetic Brownian noise data used for velocity and density fields
that show monotonic PCA behavior. We attribute this difference to the PCA's
sensitivity to Fourier phase information.","Caio Correia, Alex Lazarian, Blakesley Burkhart, Dmitri Pogosyan, José Renan De Medeiros",2015,http://arxiv.org/abs/1511.03712v1
Distributed Estimation of Principal Eigenspaces,"Principal component analysis (PCA) is fundamental to statistical machine
learning. It extracts latent principal factors that contribute to the most
variation of the data. When data are stored across multiple machines, however,
communication cost can prohibit the computation of PCA in a central location
and distributed algorithms for PCA are thus needed. This paper proposes and
studies a distributed PCA algorithm: each node machine computes the top $K$
eigenvectors and transmits them to the central server; the central server then
aggregates the information from all the node machines and conducts a PCA based
on the aggregated information. We investigate the bias and variance for the
resulting distributed estimator of the top $K$ eigenvectors. In particular, we
show that for distributions with symmetric innovation, the empirical top
eigenspaces are unbiased and hence the distributed PCA is ""unbiased"". We derive
the rate of convergence for distributed PCA estimators, which depends
explicitly on the effective rank of covariance, eigen-gap, and the number of
machines. We show that when the number of machines is not unreasonably large,
the distributed PCA performs as well as the whole sample PCA, even without full
access of whole data. The theoretical results are verified by an extensive
simulation study. We also extend our analysis to the heterogeneous case where
the population covariance matrices are different across local machines but
share similar top eigen-structures.","Jianqing Fan, Dong Wang, Kaizheng Wang, Ziwei Zhu",2017,http://arxiv.org/abs/1702.06488v4
Revisiting PCA for time series reduction in temporal dimension,"Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,
Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series
analysis (TSA), enabling the extraction of complex patterns for tasks like
classification, forecasting, and regression. Although dimensionality reduction
has traditionally focused on the variable space-achieving notable success in
minimizing data redundancy and computational complexity-less attention has been
paid to reducing the temporal dimension. In this study, we revisit Principal
Component Analysis (PCA), a classical dimensionality reduction technique, to
explore its utility in temporal dimension reduction for time series data. It is
generally thought that applying PCA to the temporal dimension would disrupt
temporal dependencies, leading to limited exploration in this area. However,
our theoretical analysis and extensive experiments demonstrate that applying
PCA to sliding series windows not only maintains model performance, but also
enhances computational efficiency. In auto-regressive forecasting, the temporal
structure is partially preserved through windowing, and PCA is applied within
these windows to denoise the time series while retaining their statistical
information. By preprocessing time-series data with PCA, we reduce the temporal
dimensionality before feeding it into TSA models such as Linear, Transformer,
CNN, and RNN architectures. This approach accelerates training and inference
and reduces resource consumption. Notably, PCA improves Informer training and
inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,
without sacrificing model accuracy. Comparative analysis against other
reduction methods further highlights the effectiveness of PCA in improving the
efficiency of TSA models.","Jiaxin Gao, Wenbo Hu, Yuntian Chen",2024,http://arxiv.org/abs/2412.19423v1
NP-Hardness and Inapproximability of Sparse PCA,"We give a reduction from {\sc clique} to establish that sparse PCA is
NP-hard. The reduction has a gap which we use to exclude an FPTAS for sparse
PCA (unless P=NP). Under weaker complexity assumptions, we also exclude
polynomial constant-factor approximation algorithms.",Malik Magdon-Ismail,2015,http://arxiv.org/abs/1502.05675v2
A Sparse PCA Approach to Clustering,"We discuss a clustering method for Gaussian mixture model based on the sparse
principal component analysis (SPCA) method and compare it with the IF-PCA
method. We also discuss the dependent case where the covariance matrix $\Sigma$
is not necessarily diagonal.","T. Tony Cai, Linjun Zhang",2016,http://arxiv.org/abs/1602.05236v1
Self-adaptive node-based PCA encodings,"In this paper we propose an algorithm, Simple Hebbian PCA, and prove that it
is able to calculate the principal component analysis (PCA) in a distributed
fashion across nodes. It simplifies existing network structures by removing
intralayer weights, essentially cutting the number of weights that need to be
trained in half.","Leonard Johard, Victor Rivera, Manuel Mazzara, JooYoung Lee",2017,http://arxiv.org/abs/1708.04498v1
"Two derivations of Principal Component Analysis on datasets of
  distributions","In this brief note, we formulate Principal Component Analysis (PCA) over
datasets consisting not of points but of distributions, characterized by their
location and covariance. Just like the usual PCA on points can be equivalently
derived via a variance-maximization principle and via a minimization of
reconstruction error, we derive a closed-form solution for distributional PCA
from both of these perspectives.",Vlad Niculae,2023,http://arxiv.org/abs/2306.13503v1
Regularized Consensus PCA,"A new framework for many multiblock component methods (including consensus
and hierarchical PCA) is proposed. It is based on the consensus PCA model: a
scheme connecting each block of variables to a superblock obtained by
concatenation of all blocks. Regularized consensus PCA is obtained by applying
regularized generalized canonical correlation analysis to this scheme for the
function $g(x) = x^m$ where $m \ge 1$. A gradient algorithm is proposed. At
convergence, a solution of the stationary equation related to the optimization
problem is obtained. For m = 1, 2 or 4 and shrinkage constants equal to 0 or 1,
many multiblock component methods are recovered.","Michel Tenenhaus, Arthur Tenenhaus, Patrick J. F. Groenen",2015,http://arxiv.org/abs/1504.07005v1
Abnormal Subspace Sparse PCA for Anomaly Detection and Interpretation,"The main shortage of principle component analysis (PCA) based anomaly
detection models is their interpretability. In this paper, our goal is to
propose an interpretable PCA-based model for anomaly detection and
interpretation. The propose ASPCA model constructs principal components with
sparse and orthogonal loading vectors to represent the abnormal subspace, and
uses them to interpret detected anomalies. Our experiments on a synthetic
dataset and two real world datasets showed that the proposed ASPCA models
achieved comparable detection accuracies as the PCA model, and can provide
interpretations for individual anomalies.","Xingyan Bin, Ying Zhao, Bilong Shen",2016,http://arxiv.org/abs/1605.04644v1
Principal Component Analysis with Tensor Train Subspace,"Tensor train is a hierarchical tensor network structure that helps alleviate
the curse of dimensionality by parameterizing large-scale multidimensional data
via a set of network of low-rank tensors. Associated with such a construction
is a notion of Tensor Train subspace and in this paper we propose a TT-PCA
algorithm for estimating this structured subspace from the given data. By
maintaining low rank tensor structure, TT-PCA is more robust to noise comparing
with PCA or Tucker-PCA. This is borne out numerically by testing the proposed
approach on the Extended YaleFace Dataset B.","Wenqi Wang, Vaneet Aggarwal, Shuchin Aeron",2018,http://arxiv.org/abs/1803.05026v1
"An Acceleration Scheme for Memory Limited, Streaming PCA","In this paper, we propose an acceleration scheme for online memory-limited
PCA methods. Our scheme converges to the first $k>1$ eigenvectors in a single
data pass. We provide empirical convergence results of our scheme based on the
spiked covariance model. Our scheme does not require any predefined parameters
such as the eigengap and hence is well facilitated for streaming data
scenarios. Furthermore, we apply our scheme to challenging time-varying systems
where online PCA methods fail to converge. Specifically, we discuss a family of
time-varying systems that are based on Molecular Dynamics simulations where
batch PCA converges to the actual analytic solution of such systems.","Salaheddin Alakkari, John Dingliana",2018,http://arxiv.org/abs/1807.06530v1
"Determining Principal Component Cardinality through the Principle of
  Minimum Description Length","PCA (Principal Component Analysis) and its variants areubiquitous techniques
for matrix dimension reduction and reduced-dimensionlatent-factor extraction.
One significant challenge in using PCA, is thechoice of the number of principal
components. The information-theoreticMDL (Minimum Description Length) principle
gives objective compression-based criteria for model selection, but it is
difficult to analytically applyits modern definition - NML (Normalized Maximum
Likelihood) - to theproblem of PCA. This work shows a general reduction of NML
prob-lems to lower-dimension problems. Applying this reduction, it boundsthe
NML of PCA, by terms of the NML of linear regression, which areknown.",Ami Tavory,2018,http://arxiv.org/abs/1901.00059v2
On the Rotational Invariant $L_1$-Norm PCA,"Principal component analysis (PCA) is a powerful tool for dimensionality
reduction. Unfortunately, it is sensitive to outliers, so that various robust
PCA variants were proposed in the literature. Among them the so-called
rotational invariant $L_1$-norm PCA is rather popular. In this paper, we
reinterpret this robust method as conditional gradient algorithm and show
moreover that it coincides with a gradient descent algorithm on Grassmannian
manifolds. Based on this point of view, we prove for the first time convergence
of the whole series of iterates to a critical point using the
Kurdyka-{\L}ojasiewicz property of the energy functional.","Sebastian Neumayer, Max Nimmer, Simon Setzer, Gabriele Steidl",2019,http://arxiv.org/abs/1902.03840v2
"Embedding Principal Component Analysis for Data Reductionin Structural
  Health Monitoring on Low-Cost IoT Gateways","Principal component analysis (PCA) is a powerful data reductionmethod for
Structural Health Monitoring. However, its computa-tional cost and data memory
footprint pose a significant challengewhen PCA has to run on limited capability
embedded platformsin low-cost IoT gateways. This paper presents a
memory-efficientparallel implementation of the streaming History PCA
algorithm.On our dataset, it achieves 10x compression factor and 59x
memoryreduction with less than 0.15 dB degradation in the
reconstructedsignal-to-noise ratio (RSNR) compared to standard PCA. More-over,
the algorithm benefits from parallelization on multiple cores,achieving a
maximum speedup of 4.8x on Samsung ARTIK 710.","Alessio Burrello, Alex Marchioni, Davide Brunelli, Luca Benini",2019,http://arxiv.org/abs/1905.01093v1
"Extended depth of field of diffraction limited imaging system using
  spatial light modulator based intensity compensated polarization coded
  aperture","Reducing the aperture size is a conventional technique to obtain enhanced
image resolution in optics but it is obscured by depleting illumination.
Polarization coded apertures (PCAs) can be employed to circumvent this critical
artifact. We experimentally demonstrate intensity compensated polarization
encrypted apertures, which are designed using the polarization modulation
characteristics of LC-SLM. PCAs are not limited by the aperture size and hence
far-field point spread function (PSF) can be more conveniently recorded using
these PCAs. We experimentally validate that Depth of field (DOF) of a
diffraction-limited lens and axial intensity of binary Fresnel zone plate
(BFZP) is enhanced using PCAs with nominal intensity loss.","Vipin Tiwari, Nandan S. Bisht",2022,http://arxiv.org/abs/2204.04005v1
"Ergodicity of some probabilistic cellular automata with two letters
  alphabet via random walks","Ergodicity of probabilistic cellular automata is a very important issue in
the PCA theory. In particular, the question about the ergodicity of all PCA
with two-size neighbourhood, two letters alphabet and positive rates is still
open. In this article, we do not try to improve this issue, but we show a new
kind of proof (to the best knowledge of the author) about the ergodicity of
some of those PCA, including also some CA with errors. The proof is based on
the study of the boundaries of islands where the PCA is totally decorrelated
from its initial condition. The behaviours of these boundaries are the ones of
random walks.",Jérôme Casse,2022,http://arxiv.org/abs/2212.02176v1
"PCA-based Data Reduction and Signal Separation Techniques for James-Webb
  Space Telescope Data Processing","Principal Component Analysis (PCA)-based techniques can separate data into
different uncorrelated components and facilitate the statistical analysis as a
pre-processing step. Independent Component Analysis (ICA) can separate
statistically independent signal sources through a non-parametric and iterative
algorithm. Non-negative matrix factorization is another PCA-similar approach to
categorizing dimensions in physically-interpretable groups. Singular spectrum
analysis (SSA) is a time-series-related PCA-like algorithm. After an
introduction and a literature review on processing JWST data from the
Near-Infrared Camera (NIRCam) and Mid-Infrared Instrument (MIRI), potential
parts to intervene in the James Webb Space Telescope imaging data reduction
pipeline will be discussed.",Güray Hatipoğlu,2023,http://arxiv.org/abs/2301.00415v1
Asymptotic performance of PCA for high-dimensional heteroscedastic data,"Principal Component Analysis (PCA) is a classical method for reducing the
dimensionality of data by projecting them onto a subspace that captures most of
their variation. Effective use of PCA in modern applications requires
understanding its performance for data that are both high-dimensional and
heteroscedastic. This paper analyzes the statistical performance of PCA in this
setting, i.e., for high-dimensional data drawn from a low-dimensional subspace
and degraded by heteroscedastic noise. We provide simplified expressions for
the asymptotic PCA recovery of the underlying subspace, subspace amplitudes and
subspace coefficients; the expressions enable both easy and efficient
calculation and reasoning about the performance of PCA. We exploit the
structure of these expressions to show that, for a fixed average noise
variance, the asymptotic recovery of PCA for heteroscedastic data is always
worse than that for homoscedastic data (i.e., for noise variances that are
equal across samples). Hence, while average noise variance is often a
practically convenient measure for the overall quality of data, it gives an
overly optimistic estimate of the performance of PCA for heteroscedastic data.","David Hong, Laura Balzano, Jeffrey A. Fessler",2017,http://arxiv.org/abs/1703.06610v4
Steerable $e$PCA: Rotationally Invariant Exponential Family PCA,"In photon-limited imaging, the pixel intensities are affected by photon count
noise. Many applications, such as 3-D reconstruction using correlation analysis
in X-ray free electron laser (XFEL) single molecule imaging, require an
accurate estimation of the covariance of the underlying 2-D clean images.
Accurate estimation of the covariance from low-photon count images must take
into account that pixel intensities are Poisson distributed, hence the
classical sample covariance estimator is sub-optimal. Moreover, in single
molecule imaging, including in-plane rotated copies of all images could further
improve the accuracy of covariance estimation. In this paper we introduce an
efficient and accurate algorithm for covariance matrix estimation of count
noise 2-D images, including their uniform planar rotations and possibly
reflections. Our procedure, steerable $e$PCA, combines in a novel way two
recently introduced innovations. The first is a methodology for principal
component analysis (PCA) for Poisson distributions, and more generally,
exponential family distributions, called $e$PCA. The second is steerable PCA, a
fast and accurate procedure for including all planar rotations for PCA. The
resulting principal components are invariant to the rotation and reflection of
the input images. We demonstrate the efficiency and accuracy of steerable
$e$PCA in numerical experiments involving simulated XFEL datasets and rotated
Yale B face data.","Zhizhen Zhao, Lydia T. Liu, Amit Singer",2018,http://arxiv.org/abs/1812.08789v3
"Matrix Normal PCA for Interpretable Dimension Reduction and Graphical
  Noise Modeling","Principal component analysis (PCA) is one of the most widely used dimension
reduction and multivariate statistical techniques. From a probabilistic
perspective, PCA seeks a low-dimensional representation of data in the presence
of independent identical Gaussian noise. Probabilistic PCA (PPCA) and its
variants have been extensively studied for decades. Most of them assume the
underlying noise follows a certain independent identical distribution. However,
the noise in the real world is usually complicated and structured. To address
this challenge, some variants of PCA for data with non-IID noise have been
proposed. However, most of the existing methods only assume that the noise is
correlated in the feature space while there may exist two-way structured noise.
To this end, we propose a powerful and intuitive PCA method (MN-PCA) through
modeling the graphical noise by the matrix normal distribution, which enables
us to explore the structure of noise in both the feature space and the sample
space. MN-PCA obtains a low-rank representation of data and the structure of
noise simultaneously. And it can be explained as approximating data over the
generalized Mahalanobis distance. We develop two algorithms to solve this
model: one maximizes the regularized likelihood, the other exploits the
Wasserstein distance, which is more robust. Extensive experiments on various
data demonstrate their effectiveness.","Chihao Zhang, Kuo Gai, Shihua Zhang",2019,http://arxiv.org/abs/1911.10796v2
"Preliminary Results from a Peer-Led, Social Network Intervention,
  Augmented by Artificial Intelligence to Prevent HIV among Youth Experiencing
  Homelessness","Each year, there are nearly 4 million youth experiencing homelessness (YEH)
in the United States with HIV prevalence ranging from 3 to 11.5%. Peer change
agent (PCA) models for HIV prevention have been used successfully in many
populations, but there have been notable failures. In recent years, network
interventionists have suggested that these failures could be attributed to PCA
selection procedures. The change agents themselves who are selected to do the
PCA work can often be as important as the messages they convey. To address this
concern, we tested a new PCA intervention for YEH, with three arms: (1) an arm
using an artificial intelligence (AI) planning algorithm to select PCA, (2) a
popularity arm--the standard PCA approach--operationalized as highest degree
centrality (DC), and (3) an observation only comparison group (OBS). PCA models
that promote HIV testing, HIV knowledge, and condom use are efficacious for
YEH. Both the AI and DC arms showed improvements over time. AI-based PCA
selection led to better outcomes and increased the speed of intervention
effects. Specifically, the changes in behavior observed in the AI arm occurred
by 1 month, but not until 3 months in the DC arm. Given the transient nature of
YEH and the high risk for HIV infection, more rapid intervention effects are
desirable.","Eric Rice, Laura Onasch-Vera, Graham T. DiGuiseppi, Bryan Wilder, Robin Petering, Chyna Hill, Amulya Yadav, Milind Tambe",2020,http://arxiv.org/abs/2007.07747v1
"Monitoring multimode processes: a modified PCA algorithm with continual
  learning ability","For multimode processes, one generally establishes local monitoring models
corresponding to local modes. However, the significant features of previous
modes may be catastrophically forgotten when a monitoring model for the current
mode is built. It would result in an abrupt performance decrease. It could be
an effective manner to make local monitoring model remember the features of
previous modes. Choosing the principal component analysis (PCA) as a basic
monitoring model, we try to resolve this problem. A modified PCA algorithm is
built with continual learning ability for monitoring multimode processes, which
adopts elastic weight consolidation (EWC) to overcome catastrophic forgetting
of PCA for successive modes. It is called PCA-EWC, where the significant
features of previous modes are preserved when a PCA model is established for
the current mode. The optimal parameters are acquired by differences of convex
functions. Moreover, the proposed PCA-EWC is extended to general multimode
processes and the procedure is presented. The computational complexity and key
parameters are discussed to further understand the relationship between PCA and
the proposed algorithm. Potential limitations and relevant solutions are
pointed to understand the algorithm further. Numerical case study and a
practical industrial system in China are employed to illustrate the
effectiveness of the proposed algorithm.","Jingxin Zhang, Donghua Zhou, Maoyin Chen",2020,http://arxiv.org/abs/2012.07044v5
"Linear Convergence of a Proximal Alternating Minimization Method with
  Extrapolation for $\ell_1$-Norm Principal Component Analysis","A popular robust alternative of the classic principal component analysis
(PCA) is the $\ell_1$-norm PCA (L1-PCA), which aims to find a subspace that
captures the most variation in a dataset as measured by the $\ell_1$-norm.
L1-PCA has shown great promise in alleviating the effect of outliers in data
analytic applications. However, it gives rise to a challenging non-smooth
non-convex optimization problem, for which existing algorithms are either not
scalable or lack strong theoretical guarantees on their convergence behavior.
In this paper, we propose a proximal alternating minimization method with
extrapolation (PAMe) for solving a two-block reformulation of the L1-PCA
problem. We then show that for both the L1-PCA problem and its two-block
reformulation, the Kurdyka-\L ojasiewicz exponent at any of the limiting
critical points is $1/2$. This allows us to establish the linear convergence of
the sequence of iterates generated by PAMe and to determine the criticality of
the limit of the sequence with respect to both the L1-PCA problem and its
two-block reformulation. To complement our theoretical development, we show via
numerical experiments on both synthetic and real-world datasets that PAMe is
competitive with a host of existing methods. Our results not only significantly
advance the convergence theory of iterative methods for L1-PCA but also
demonstrate the potential of our proposed method in applications.","Peng Wang, Huikang Liu, Anthony Man-Cho So",2021,http://arxiv.org/abs/2107.07107v1
Generalized Principal Component Analysis,"Generalized principal component analysis (GLM-PCA) facilitates dimension
reduction of non-normally distributed data. We provide a detailed derivation of
GLM-PCA with a focus on optimization. We also demonstrate how to incorporate
covariates, and suggest post-processing transformations to improve
interpretability of latent factors.",F. William Townes,2019,http://arxiv.org/abs/1907.02647v1
"Synthetic Correlated Diffusion Imaging Hyperintensity Delineates
  Clinically Significant Prostate Cancer","Prostate cancer (PCa) is the second most common cancer in men worldwide and
the most frequently diagnosed cancer among men in more developed countries. The
prognosis of PCa is excellent if detected at an early stage, making early
screening crucial for detection and treatment. In recent years, a new form of
diffusion magnetic resonance imaging called correlated diffusion imaging (CDI)
was introduced, and preliminary results show promise as a screening tool for
PCa. In the largest study of its kind, we investigate the relationship between
PCa presence and a new variant of CDI we term synthetic correlated diffusion
imaging (CDI$^s$), as well as its performance for PCa delineation compared to
current standard MRI techniques (T2-weighted (T2w) imaging, diffusion-weighted
imaging (DWI), and dynamic contrast-enhanced (DCE) imaging) across a cohort of
200 patient cases. Statistical analyses reveal that hyperintensity in CDI$^s$
is a strong indicator of PCa presence and achieves strong delineation of
clinically significant cancerous tissue compared to T2w, DWI, and DCE. These
results suggest that CDI$^s$ hyperintensity may be a powerful biomarker for the
presence of PCa, and may have a clinical impact as a diagnostic aid for
improving PCa screening.","Alexander Wong, Hayden Gunraj, Vignesh Sivan, Masoom A. Haider",2021,http://arxiv.org/abs/2108.04427v2
Data-Free Evaluation of User Contributions in Federated Learning,"Federated learning (FL) trains a machine learning model on mobile devices in
a distributed manner using each device's private data and computing resources.
A critical issues is to evaluate individual users' contributions so that (1)
users' effort in model training can be compensated with proper incentives and
(2) malicious and low-quality users can be detected and removed. The
state-of-the-art solutions require a representative test dataset for the
evaluation purpose, but such a dataset is often unavailable and hard to
synthesize. In this paper, we propose a method called Pairwise Correlated
Agreement (PCA) based on the idea of peer prediction to evaluate user
contribution in FL without a test dataset. PCA achieves this using the
statistical correlation of the model parameters uploaded by users. We then
apply PCA to designing (1) a new federated learning algorithm called Fed-PCA,
and (2) a new incentive mechanism that guarantees truthfulness. We evaluate the
performance of PCA and Fed-PCA using the MNIST dataset and a large industrial
product recommendation dataset. The results demonstrate that our Fed-PCA
outperforms the canonical FedAvg algorithm and other baseline methods in
accuracy, and at the same time, PCA effectively incentivizes users to behave
truthfully.","Hongtao Lv, Zhenzhe Zheng, Tie Luo, Fan Wu, Shaojie Tang, Lifeng Hua, Rongfei Jia, Chengfei Lv",2021,http://arxiv.org/abs/2108.10623v1
"Penalized Optimal Scaling for Ordinal Variables with an Application to
  International Classification of Functioning Core Sets","Ordinal data occur frequently in the social sciences. When applying principal
component analysis (PCA), however, those data are often treated as numeric
implying linear relationships between the variables at hand, or non-linear PCA
is applied where the obtained quantifications are sometimes hard to interpret.
Non-linear PCA for categorical data, also called optimal scoring/scaling,
constructs new variables by assigning numerical values to categories such that
the proportion of variance in those new variables that is explained by a
predefined number of principal components is maximized. We propose a penalized
version of non-linear PCA for ordinal variables that is a smoothed intermediate
between standard PCA on category labels and non-linear PCA as used so far. The
new approach is by no means limited to monotonic effects and offers both better
interpretability of the non-linear transformation of the category labels as
well as better performance on validation data than unpenalized non-linear PCA
and/or standard linear PCA. In particular, an application of penalized optimal
scaling to ordinal data as given with the International Classification of
Functioning, Disability and Health (ICF) is provided.","Aisouda Hoshiyar, Henk A. L. Kiers, Jan Gertheiss",2021,http://arxiv.org/abs/2110.02805v3
"A co-kurtosis based dimensionality reduction method for combustion
  datasets","Principal Component Analysis (PCA) is a dimensionality reduction technique
widely used to reduce the computational cost associated with numerical
simulations of combustion phenomena. However, PCA, which transforms the
thermo-chemical state space based on eigenvectors of co-variance of the data,
could fail to capture information regarding important localized chemical
dynamics, such as the formation of ignition kernels, appearing as
\rev{extreme-valued} samples in a dataset. In this paper, we propose an
alternate dimensionality reduction procedure, co-kurtosis PCA (CoK-PCA),
wherein the required principal vectors are computed from a high-order joint
statistical moment, namely the co-kurtosis tensor, which may better identify
directions in the state space that represent stiff dynamics. We first
demonstrate the potential of the proposed CoK-PCA method using a synthetically
generated dataset that is representative of typical combustion simulations.
Thereafter, we characterize and contrast the accuracy of CoK-PCA against PCA
for datasets representing spontaneous ignition of premixed ethylene-air in a
simple homogeneous reactor and ethanol-fueled homogeneous charged compression
ignition (HCCI) engine. Specifically, we compare the low-dimensional manifolds
in terms of reconstruction errors of the original thermo-chemical state, and
species production and heat release rates computed from the reconstructed
state. \rev{The latter -- a comparison of species production and heat release
rates -- is a more rigorous assessment of the accuracy of dimensionality
reduction.} We find that, even using a simplistic linear reconstruction, the
co-kurtosis based reduced manifold represents the original thermo-chemical
state more accurately than PCA, especially in the regions where chemical
reactions are important.","Anirudh Jonnalagadda, Shubham P. Kulkarni, Akash Rodhiya, Hemanth Kolla, Konduri Aditya",2022,http://arxiv.org/abs/2204.10271v2
Streaming Kernel PCA Algorithm With Small Space,"Principal Component Analysis (PCA) is a widely used technique in machine
learning, data analysis and signal processing. With the increase in the size
and complexity of datasets, it has become important to develop low-space usage
algorithms for PCA. Streaming PCA has gained significant attention in recent
years, as it can handle large datasets efficiently. The kernel method, which is
commonly used in learning algorithms such as Support Vector Machines (SVMs),
has also been applied in PCA algorithms.
  We propose a streaming algorithm for Kernel PCA problems based on the
traditional scheme by Oja. Our algorithm addresses the challenge of reducing
the memory usage of PCA while maintaining its accuracy. We analyze the
performance of our algorithm by studying the conditions under which it
succeeds. Specifically, we show that, when the spectral ratio $R :=
\lambda_1/\lambda_2$ of the target covariance matrix is lower bounded by $C
\cdot \log n\cdot \log d$, the streaming PCA can be solved with $O(d)$ space
cost.
  Our proposed algorithm has several advantages over existing methods. First,
it is a streaming algorithm that can handle large datasets efficiently. Second,
it employs the kernel method, which allows it to capture complex nonlinear
relationships among data points. Third, it has a low-space usage, making it
suitable for applications where memory is limited.","Yichuan Deng, Zhao Song, Zifan Wang, Han Zhang",2023,http://arxiv.org/abs/2303.04555v1
Subject clustering by IF-PCA and several recent methods,"Subject clustering (i.e., the use of measured features to cluster subjects,
such as patients or cells, into multiple groups) is a problem of great
interest. In recent years, many approaches were proposed, among which
unsupervised deep learning (UDL) has received a great deal of attention. Two
interesting questions are (a) how to combine the strengths of UDL and other
approaches, and (b) how these approaches compare to one other.
  We combine Variational Auto-Encoder (VAE), a popular UDL approach, with the
recent idea of Influential Feature PCA (IF-PCA), and propose IF-VAE as a new
method for subject clustering. We study IF-VAE and compare it with several
other methods (including IF-PCA, VAE, Seurat, and SC3) on $10$ gene microarray
data sets and $8$ single-cell RNA-seq data sets. We find that IF-VAE
significantly improves over VAE, but still underperforms IF-PCA. We also find
that IF-PCA is quite competitive, which slightly outperforms Seurat and SC3
over the $8$ single-cell data sets. IF-PCA is conceptually simple and permits
delicate analysis. We demonstrate that IF-PCA is capable of achieving the phase
transition in a Rare/Weak model. Comparatively, Seurat and SC3 are more complex
and theoretically difficult to analyze (for these reasons, their optimality
remains unclear).","Dieyi Chen, Jiashun Jin, Zheng Tracy Ke",2023,http://arxiv.org/abs/2306.05363v1
"Optimizing PCA for Health and Care Research: A Reliable Approach to
  Component Selection","PCA is widely used in health and care research to analyze complex HD
datasets, such as patient health records, genetic data, and medical imaging. By
reducing dimensionality, PCA helps identify key patterns and trends, which can
aid in disease diagnosis, treatment optimization, and the discovery of new
biomarkers. However, the primary goal of any dimensional reduction technique is
to reduce the dimensionality in a data set while keeping the essential
information and variability. There are a few ways to do this in practice, such
as the Kaiser-Guttman criterion, Cattell's Scree Test, and the percent
cumulative variance approach. Unfortunately, the results of these methods are
entirely different. That means using inappropriate methods to find the optimal
number of PCs retained in PCA may lead to misinterpreted and inaccurate results
in PCA and PCA-related health and care research applications. This
contradiction becomes even more pronounced in HD settings where n < p, making
it even more critical to determine the best approach. Therefore, it is
necessary to identify the issues of different techniques to select the optimal
number of PCs retained in PCA. Kaiser-Guttman criterion retains fewer PCs,
causing overdispersion, while Cattell's scree test retains more PCs,
compromising reliability. The percentage of cumulative variation criterion
offers greater stability, consistently selecting the optimal number of
components. Therefore, the Pareto chart, which shows both the cumulative
percentage and the cut-off point for retained PCs, provides the most reliable
method of selecting components, ensuring stability and enhancing PCA
effectiveness, particularly in health-related research applications.","Nuwan Weeraratne, Lyn Hunt, Jason Kurz",2025,http://arxiv.org/abs/2503.24248v1
"Dimensionality Reduction for Binary Data through the Projection of
  Natural Parameters","Principal component analysis (PCA) for binary data, known as logistic PCA,
has become a popular alternative to dimensionality reduction of binary data. It
is motivated as an extension of ordinary PCA by means of a matrix
factorization, akin to the singular value decomposition, that maximizes the
Bernoulli log-likelihood. We propose a new formulation of logistic PCA which
extends Pearson's formulation of a low dimensional data representation with
minimum error to binary data. Our formulation does not require a matrix
factorization, as previous methods do, but instead looks for projections of the
natural parameters from the saturated model. Due to this difference, the number
of parameters does not grow with the number of observations and the principal
component scores on new data can be computed with simple matrix multiplication.
We derive explicit solutions for data matrices of special structure and provide
computationally efficient algorithms for solving for the principal component
loadings. Through simulation experiments and an analysis of medical diagnoses
data, we compare our formulation of logistic PCA to the previous formulation as
well as ordinary PCA to demonstrate its benefits.","Andrew J. Landgraf, Yoonkyung Lee",2015,http://arxiv.org/abs/1510.06112v1
Non-Greedy L21-Norm Maximization for Principal Component Analysis,"Principal Component Analysis (PCA) is one of the most important unsupervised
methods to handle high-dimensional data. However, due to the high computational
complexity of its eigen decomposition solution, it hard to apply PCA to the
large-scale data with high dimensionality. Meanwhile, the squared L2-norm based
objective makes it sensitive to data outliers. In recent research, the L1-norm
maximization based PCA method was proposed for efficient computation and being
robust to outliers. However, this work used a greedy strategy to solve the
eigen vectors. Moreover, the L1-norm maximization based objective may not be
the correct robust PCA formulation, because it loses the theoretical connection
to the minimization of data reconstruction error, which is one of the most
important intuitions and goals of PCA. In this paper, we propose to maximize
the L21-norm based robust PCA objective, which is theoretically connected to
the minimization of reconstruction error. More importantly, we propose the
efficient non-greedy optimization algorithms to solve our objective and the
more general L21-norm maximization problem with theoretically guaranteed
convergence. Experimental results on real world data sets show the
effectiveness of the proposed method for principal component analysis.","Feiping Nie, Heng Huang",2016,http://arxiv.org/abs/1603.08293v1
"Suppressing Background Radiation Using Poisson Principal Component
  Analysis","Performance of nuclear threat detection systems based on gamma-ray
spectrometry often strongly depends on the ability to identify the part of
measured signal that can be attributed to background radiation. We have
successfully applied a method based on Principal Component Analysis (PCA) to
obtain a compact null-space model of background spectra using PCA projection
residuals to derive a source detection score. We have shown the method's
utility in a threat detection system using mobile spectrometers in urban scenes
(Tandon et al 2012). While it is commonly assumed that measured photon counts
follow a Poisson process, standard PCA makes a Gaussian assumption about the
data distribution, which may be a poor approximation when photon counts are
low. This paper studies whether and in what conditions PCA with a Poisson-based
loss function (Poisson PCA) can outperform standard Gaussian PCA in modeling
background radiation to enable more sensitive and specific nuclear threat
detection.","P. Tandon, P. Huggins, A. Dubrawski, S. Labov, K. Nelson",2016,http://arxiv.org/abs/1605.08455v1
PCA-Initialized Deep Neural Networks Applied To Document Image Analysis,"In this paper, we present a novel approach for initializing deep neural
networks, i.e., by turning PCA into neural layers. Usually, the initialization
of the weights of a deep neural network is done in one of the three following
ways: 1) with random values, 2) layer-wise, usually as Deep Belief Network or
as auto-encoder, and 3) re-use of layers from another network (transfer
learning). Therefore, typically, many training epochs are needed before
meaningful weights are learned, or a rather similar dataset is required for
seeding a fine-tuning of transfer learning. In this paper, we describe how to
turn a PCA into an auto-encoder, by generating an encoder layer of the PCA
parameters and furthermore adding a decoding layer. We analyze the
initialization technique on real documents. First, we show that a PCA-based
initialization is quick and leads to a very stable initialization. Furthermore,
for the task of layout analysis we investigate the effectiveness of PCA-based
initialization and show that it outperforms state-of-the-art random weight
initialization methods.","Mathias Seuret, Michele Alberti, Rolf Ingold, Marcus Liwicki",2017,http://arxiv.org/abs/1702.00177v1
DROP: Dimensionality Reduction Optimization for Time Series,"Dimensionality reduction is a critical step in scaling machine learning
pipelines. Principal component analysis (PCA) is a standard tool for
dimensionality reduction, but performing PCA over a full dataset can be
prohibitively expensive. As a result, theoretical work has studied the
effectiveness of iterative, stochastic PCA methods that operate over data
samples. However, termination conditions for stochastic PCA either execute for
a predetermined number of iterations, or until convergence of the solution,
frequently sampling too many or too few datapoints for end-to-end runtime
improvements. We show how accounting for downstream analytics operations during
DR via PCA allows stochastic methods to efficiently terminate after operating
over small (e.g., 1%) subsamples of input data, reducing whole workload
runtime. Leveraging this, we propose DROP, a DR optimizer that enables speedups
of up to 5x over Singular-Value-Decomposition-based PCA techniques, and exceeds
conventional approaches like FFT and PAA by up to 16x in end-to-end workloads.","Sahaana Suri, Peter Bailis",2017,http://arxiv.org/abs/1708.00183v4
L1-norm Principal-Component Analysis of Complex Data,"L1-norm Principal-Component Analysis (L1-PCA) of real-valued data has
attracted significant research interest over the past decade. However, L1-PCA
of complex-valued data remains to date unexplored despite the many possible
applications (e.g., in communication systems). In this work, we establish
theoretical and algorithmic foundations of L1-PCA of complex-valued data
matrices. Specifically, we first show that, in contrast to the real-valued case
for which an optimal polynomial-cost algorithm was recently reported by
Markopoulos et al., complex L1-PCA is formally NP-hard in the number of data
points. Then, casting complex L1-PCA as a unimodular optimization problem, we
present the first two suboptimal algorithms in the literature for its solution.
Our experimental studies illustrate the sturdy resistance of complex L1-PCA
against faulty measurements/outliers in the processed data.","Nicholas Tsagkarakis, Panos P. Markopoulos, Dimitris A. Pados",2017,http://arxiv.org/abs/1708.01249v1
"Finite Sample Guarantees for PCA in Non-Isotropic and Data-Dependent
  Noise","This work obtains novel finite sample guarantees for Principal Component
Analysis (PCA). These hold even when the corrupting noise is non-isotropic, and
a part (or all of it) is data-dependent. Because of the latter, in general, the
noise and the true data are correlated. The results in this work are a
significant improvement over those given in our earlier work where this
""correlated-PCA"" problem was first studied. In fact, in certain regimes, our
results imply that the sample complexity required to achieve subspace recovery
error that is a constant fraction of the noise level is near-optimal. Useful
corollaries of our result include guarantees for PCA in sparse data-dependent
noise and for PCA with missing data. An important application of the former is
in proving correctness of the subspace update step of a popular online
algorithm for dynamic robust PCA.","Namrata Vaswani, Praneeth Narayanamurthy",2017,http://arxiv.org/abs/1709.06255v1
"Efficient Optimization Algorithms for Robust Principal Component
  Analysis and Its Variants","Robust PCA has drawn significant attention in the last decade due to its
success in numerous application domains, ranging from bio-informatics,
statistics, and machine learning to image and video processing in computer
vision. Robust PCA and its variants such as sparse PCA and stable PCA can be
formulated as optimization problems with exploitable special structures. Many
specialized efficient optimization methods have been proposed to solve robust
PCA and related problems. In this paper we review existing optimization methods
for solving convex and nonconvex relaxations/variants of robust PCA, discuss
their advantages and disadvantages, and elaborate on their convergence
behaviors. We also provide some insights for possible future research
directions including new algorithmic frameworks that might be suitable for
implementing on multi-processor setting to handle large-scale problems.","Shiqian Ma, Necdet Serhat Aybat",2018,http://arxiv.org/abs/1806.03430v1
"Bayesian inference for PCA and MUSIC algorithms with unknown number of
  sources","Principal component analysis (PCA) is a popular method for projecting data
onto uncorrelated components in lower dimension, although the optimal number of
components is not specified. Likewise, multiple signal classification (MUSIC)
algorithm is a popular PCA-based method for estimating directions of arrival
(DOAs) of sinusoidal sources, yet it requires the number of sources to be known
a priori. The accurate estimation of the number of sources is hence a crucial
issue for performance of these algorithms. In this paper, we will show that
both PCA and MUSIC actually return the exact joint maximum-a-posteriori (MAP)
estimate for uncorrelated steering vectors, although they can only compute this
MAP estimate approximately in correlated case. We then use Bayesian method to,
for the first time, compute the MAP estimate for the number of sources in PCA
and MUSIC algorithms. Intuitively, this MAP estimate corresponds to the highest
probability that signal-plus-noise's variance still dominates projected noise's
variance on signal subspace. In simulations of overlapping multi-tone sources
for linear sensor array, our exact MAP estimate is far superior to the
asymptotic Akaike information criterion (AIC), which is a popular method for
estimating the number of components in PCA and MUSIC algorithms.","Viet Hung Tran, Wenwu Wang",2018,http://arxiv.org/abs/1809.10168v1
DeEPCA: Decentralized Exact PCA with Linear Convergence Rate,"Due to the rapid growth of smart agents such as weakly connected
computational nodes and sensors, developing decentralized algorithms that can
perform computations on local agents becomes a major research direction. This
paper considers the problem of decentralized Principal components analysis
(PCA), which is a statistical method widely used for data analysis. We
introduce a technique called subspace tracking to reduce the communication
cost, and apply it to power iterations. This leads to a decentralized PCA
algorithm called \texttt{DeEPCA}, which has a convergence rate similar to that
of the centralized PCA, while achieving the best communication complexity among
existing decentralized PCA algorithms. \texttt{DeEPCA} is the first
decentralized PCA algorithm with the number of communication rounds for each
power iteration independent of target precision. Compared to existing
algorithms, the proposed method is easier to tune in practice, with an improved
overall communication cost. Our experiments validate the advantages of
\texttt{DeEPCA} empirically.","Haishan Ye, Tong Zhang",2021,http://arxiv.org/abs/2102.03990v1
Latent Space Exploration Using Generative Kernel PCA,"Kernel PCA is a powerful feature extractor which recently has seen a
reformulation in the context of Restricted Kernel Machines (RKMs). These RKMs
allow for a representation of kernel PCA in terms of hidden and visible units
similar to Restricted Boltzmann Machines. This connection has led to insights
on how to use kernel PCA in a generative procedure, called generative kernel
PCA. In this paper, the use of generative kernel PCA for exploring latent
spaces of datasets is investigated. New points can be generated by gradually
moving in the latent space, which allows for an interpretation of the
components. Firstly, examples of this feature space exploration on three
datasets are shown with one of them leading to an interpretable representation
of ECG signals. Afterwards, the use of the tool in combination with novelty
detection is shown, where the latent space around novel patterns in the data is
explored. This helps in the interpretation of why certain points are considered
as novel.","David Winant, Joachim Schreurs, Johan A. K. Suykens",2021,http://arxiv.org/abs/2105.13949v1
"AgFlow: Fast Model Selection of Penalized PCA via Implicit
  Regularization Effects of Gradient Flow","Principal component analysis (PCA) has been widely used as an effective
technique for feature extraction and dimension reduction. In the High Dimension
Low Sample Size (HDLSS) setting, one may prefer modified principal components,
with penalized loadings, and automated penalty selection by implementing model
selection among these different models with varying penalties. The earlier work
[1, 2] has proposed penalized PCA, indicating the feasibility of model
selection in $L_2$- penalized PCA through the solution path of Ridge
regression, however, it is extremely time-consuming because of the intensive
calculation of matrix inverse. In this paper, we propose a fast model selection
method for penalized PCA, named Approximated Gradient Flow (AgFlow), which
lowers the computation complexity through incorporating the implicit
regularization effect introduced by (stochastic) gradient flow [3, 4] and
obtains the complete solution path of $L_2$-penalized PCA under varying
$L_2$-regularization. We perform extensive experiments on real-world datasets.
AgFlow outperforms existing methods (Oja [5], Power [6], and Shamir [7] and the
vanilla Ridge estimators) in terms of computation costs.","Haiyan Jiang, Haoyi Xiong, Dongrui Wu, Ji Liu, Dejing Dou",2021,http://arxiv.org/abs/2110.03273v1
Extended Principal Component Analysis,"Principal Component Analysis (PCA) is a transform for finding the principal
components (PCs) that represent features of random data. PCA also provides a
reconstruction of the PCs to the original data. We consider an extension of PCA
which allows us to improve the associated accuracy and diminish the numerical
load, in comparison with known techniques. This is achieved due to the special
structure of the proposed transform which contains two matrices $T_0$ and
$T_1$, and a special transformation $\mathcal{f}$ of the so called auxiliary
random vector $\mathbf w$. For this reason, we call it the three-term PCA. In
particular, we show that the three-term PCA always exists, i.e. is applicable
to the case of singular data. Both rigorous theoretical justification of the
three-term PCA and simulations with real-world data are provided.","Pablo Soto-Quiros, Anatoli Torokhti",2021,http://arxiv.org/abs/2111.03040v1
"Robust PCA for High Dimensional Data based on Characteristic
  Transformation","In this paper, we propose a novel robust Principal Component Analysis (PCA)
for high-dimensional data in the presence of various heterogeneities,
especially the heavy-tailedness and outliers. A transformation motivated by the
characteristic function is constructed to improve the robustness of the
classical PCA. Besides the typical outliers, the proposed method has the unique
advantage of dealing with heavy-tail-distributed data, whose covariances could
be nonexistent (positively infinite, for instance). The proposed approach is
also a case of kernel principal component analysis (KPCA) method and adopts the
robust and non-linear properties via a bounded and non-linear kernel function.
The merits of the new method are illustrated by some statistical properties
including the upper bound of the excess error and the behaviors of the large
eigenvalues under a spiked covariance model. In addition, we show the
advantages of our method over the classical PCA by a variety of simulations. At
last, we apply the new robust PCA to classify mice with different genotypes in
a biological study based on their protein expression data and find that our
method is more accurately on identifying abnormal mice comparing to the
classical PCA.","Lingyu He, Yanrong Yang, Bo Zhang",2022,http://arxiv.org/abs/2204.01042v1
Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA,"We study principal component analysis (PCA), where given a dataset in
$\mathbb{R}^d$ from a distribution, the task is to find a unit vector $v$ that
approximately maximizes the variance of the distribution after being projected
along $v$. Despite being a classical task, standard estimators fail drastically
if the data contains even a small fraction of outliers, motivating the problem
of robust PCA. Recent work has developed computationally-efficient algorithms
for robust PCA that either take super-linear time or have sub-optimal error
guarantees. Our main contribution is to develop a nearly-linear time algorithm
for robust PCA with near-optimal error guarantees. We also develop a
single-pass streaming algorithm for robust PCA with memory usage nearly-linear
in the dimension.","Ilias Diakonikolas, Daniel M. Kane, Ankit Pensia, Thanasis Pittas",2023,http://arxiv.org/abs/2305.02544v1
"Fair Streaming Principal Component Analysis: Statistical and Algorithmic
  Viewpoint","Fair Principal Component Analysis (PCA) is a problem setting where we aim to
perform PCA while making the resulting representation fair in that the
projected distributions, conditional on the sensitive attributes, match one
another. However, existing approaches to fair PCA have two main problems:
theoretically, there has been no statistical foundation of fair PCA in terms of
learnability; practically, limited memory prevents us from using existing
approaches, as they explicitly rely on full access to the entire data. On the
theoretical side, we rigorously formulate fair PCA using a new notion called
\emph{probably approximately fair and optimal} (PAFO) learnability. On the
practical side, motivated by recent advances in streaming algorithms for
addressing memory limitation, we propose a new setting called \emph{fair
streaming PCA} along with a memory-efficient algorithm, fair noisy power method
(FNPM). We then provide its {\it statistical} guarantee in terms of
PAFO-learnability, which is the first of its kind in fair PCA literature.
Lastly, we verify the efficacy and memory efficiency of our algorithm on
real-world datasets.","Junghyun Lee, Hanseul Cho, Se-Young Yun, Chulhee Yun",2023,http://arxiv.org/abs/2310.18593v1
Orthogonal projections of hypercubes,"Projections of hypercubes have been applied to visualize high-dimensional
binary state spaces in various scientific fields. Conventional methods for
projecting hypercubes, however, face practical difficulties. Manual methods
require nontrivial adjustments of the projection basis, while
optimization-based algorithms limit the interpretability and reproducibility of
the resulting plots. These limitations motivate us to explore theoretically
analyzable projection algorithms such as principal component analysis (PCA).
Here, we investigate the mathematical properties of PCA-projected hypercubes.
Our numerical and analytical results show that PCA effectively captures
polarized distributions within the hypercubic state space. This property
enables the assessment of the asymptotic distribution of projected vertices and
error bounds, which characterize the performance of PCA in the projected space.
We demonstrate the application of PCA to visualize the hypercubic energy
landscapes of Ising spin systems. By adding projected hypercubic edges, these
visualizations reveal pathways of correlated spin flips. Our work provides a
better understanding of how PCA discovers hidden patterns in high-dimensional
binary data.","Yoshiaki Horiike, Shin Fujishiro",2025,http://arxiv.org/abs/2501.10257v3
"Solve sparse PCA problem by employing Hamiltonian system and leapfrog
  method","Principal Component Analysis (PCA) is a widely utilized technique for
dimensionality reduction; however, its inherent lack of
interpretability-stemming from dense linear combinations of all feature-limits
its applicability in many domains. In this paper, we propose a novel sparse PCA
algorithm that imposes sparsity through a smooth L1 penalty and leverages a
Hamiltonian formulation solved via geometric integration techniques.
Specifically, we implement two distinct numerical methods-one based on the
Proximal Gradient (ISTA) approach and another employing a leapfrog
(fourth-order Runge-Kutta) scheme-to minimize the energy function that balances
variance maximization with sparsity enforcement. To extract a subset of sparse
principal components, we further incorporate a deflation technique and
subsequently transform the original high-dimensional face data into a
lower-dimensional feature space. Experimental evaluations on a face recognition
dataset-using both k-nearest neighbor and kernel ridge regression
classifiers-demonstrate that the proposed sparse PCA methods consistently
achieve higher classification accuracy than conventional PCA. Future research
will extend this framework to integrate sparse PCA with modern deep learning
architectures for multimodal recognition tasks.",Loc Hoang Tran,2025,http://arxiv.org/abs/2503.23335v1
"Novel sparse PCA method via Runge Kutta numerical method(s) for face
  recognition","Face recognition is a crucial topic in data science and biometric security,
with applications spanning military, finance, and retail industries. This paper
explores the implementation of sparse Principal Component Analysis (PCA) using
the Proximal Gradient method (also known as ISTA) and the Runge-Kutta numerical
methods. To address the face recognition problem, we integrate sparse PCA with
either the k-nearest neighbor method or the kernel ridge regression method.
Experimental results demonstrate that combining sparse PCA-solved via the
Proximal Gradient method or the Runge-Kutta numerical approach-with a
classification system yields higher accuracy compared to standard PCA.
Additionally, we observe that the Runge-Kutta-based sparse PCA computation
consistently outperforms the Proximal Gradient method in terms of speed.","Loc Hoang Tran, Luong Anh Tuan Nguyen",2025,http://arxiv.org/abs/2504.01035v1
"$σ$-PCA: a building block for neural learning of identifiable
  linear transformations","Linear principal component analysis (PCA) learns (semi-)orthogonal
transformations by orienting the axes to maximize variance. Consequently, it
can only identify orthogonal axes whose variances are clearly distinct, but it
cannot identify the subsets of axes whose variances are roughly equal. It
cannot eliminate the subspace rotational indeterminacy: it fails to disentangle
components with equal variances (eigenvalues), resulting, in each eigen
subspace, in randomly rotated axes. In this paper, we propose $\sigma$-PCA, a
method that (1) formulates a unified model for linear and nonlinear PCA, the
latter being a special case of linear independent component analysis (ICA), and
(2) introduces a missing piece into nonlinear PCA that allows it to eliminate,
from the canonical linear PCA solution, the subspace rotational indeterminacy
-- without whitening the inputs. Whitening, a preprocessing step which converts
the inputs into unit-variance inputs, has generally been a prerequisite step
for linear ICA methods, which meant that conventional nonlinear PCA could not
necessarily preserve the orthogonality of the overall transformation, could not
directly reduce dimensionality, and could not intrinsically order by variances.
We offer insights on the relationship between linear PCA, nonlinear PCA, and
linear ICA -- three methods with autoencoder formulations for learning special
linear transformations from data, transformations that are (semi-)orthogonal
for PCA, and arbitrary unit-variance for ICA. As part of our formulation,
nonlinear PCA can be seen as a method that maximizes both variance and
statistical independence, lying in the middle between linear PCA and linear
ICA, serving as a building block for learning linear transformations that are
identifiable.","Fahdi Kanavati, Lucy Katsnith, Masayuki Tsuneki",2023,http://arxiv.org/abs/2311.13580v4
"Nearly Optimal Stochastic Approximation for Online Principal Subspace
  Estimation","Principal component analysis (PCA) has been widely used in analyzing
high-dimensional data. It converts a set of observed data points of possibly
correlated variables into a set of linearly uncorrelated variables via an
orthogonal transformation. To handle streaming data and reduce the complexities
of PCA, (subspace) online PCA iterations were proposed to iteratively update
the orthogonal transformation by taking one observed data point at a time.
Existing works on the convergence of (subspace) online PCA iterations mostly
focus on the case where sample are almost surely uniformly bounded. In this
paper, we analyze the convergence of a subspace online PCA iteration under more
practical assumption and obtain a nearly optimal finite-sample error bound. Our
convergence rate almost matches the minimax information lower bound. We prove
that the convergence is nearly global in the sense that the subspace online PCA
iteration is convergent with high probability for random initial guesses. This
work also leads to a simpler proof of the recent work on analyzing online PCA
for the first principal component only.","Xin Liang, Zhen-Chen Guo, Li Wang, Ren-Cang Li, Wen-Wei Lin",2017,http://arxiv.org/abs/1711.06644v3
History PCA: A New Algorithm for Streaming PCA,"In this paper we propose a new algorithm for streaming principal component
analysis. With limited memory, small devices cannot store all the samples in
the high-dimensional regime. Streaming principal component analysis aims to
find the $k$-dimensional subspace which can explain the most variation of the
$d$-dimensional data points that come into memory sequentially. In order to
deal with large $d$ and large $N$ (number of samples), most streaming PCA
algorithms update the current model using only the incoming sample and then
dump the information right away to save memory. However the information
contained in previously streamed data could be useful. Motivated by this idea,
we develop a new streaming PCA algorithm called History PCA that achieves this
goal. By using $O(Bd)$ memory with $B\approx 10$ being the block size, our
algorithm converges much faster than existing streaming PCA algorithms. By
changing the number of inner iterations, the memory usage can be further
reduced to $O(d)$ while maintaining a comparable convergence speed. We provide
theoretical guarantees for the convergence of our algorithm along with the rate
of convergence. We also demonstrate on synthetic and real world data sets that
our algorithm compares favorably with other state-of-the-art streaming PCA
methods in terms of the convergence speed and performance.","Puyudi Yang, Cho-Jui Hsieh, Jane-Ling Wang",2018,http://arxiv.org/abs/1802.05447v1
"High Dimensional Bayesian Optimization Assisted by Principal Component
  Analysis","Bayesian Optimization (BO) is a surrogate-assisted global optimization
technique that has been successfully applied in various fields, e.g., automated
machine learning and design optimization. Built upon a so-called
infill-criterion and Gaussian Process regression (GPR), the BO technique
suffers from a substantial computational complexity and hampered convergence
rate as the dimension of the search spaces increases. Scaling up BO for
high-dimensional optimization problems remains a challenging task. In this
paper, we propose to tackle the scalability of BO by hybridizing it with a
Principal Component Analysis (PCA), resulting in a novel PCA-assisted BO
(PCA-BO) algorithm. Specifically, the PCA procedure learns a linear
transformation from all the evaluated points during the run and selects
dimensions in the transformed space according to the variability of evaluated
points. We then construct the GPR model, and the infill-criterion in the space
spanned by the selected dimensions. We assess the performance of our PCA-BO in
terms of the empirical convergence rate and CPU time on multi-modal problems
from the COCO benchmark framework. The experimental results show that PCA-BO
can effectively reduce the CPU time incurred on high-dimensional problems, and
maintains the convergence rate on problems with an adequate global structure.
PCA-BO therefore provides a satisfactory trade-off between the convergence rate
and computational efficiency opening new ways to benefit from the strength of
BO approaches in high dimensional numerical optimization.","Elena Raponi, Hao Wang, Mariusz Bujny, Simonetta Boria, Carola Doerr",2020,http://arxiv.org/abs/2007.00925v1
3D CNN-PCA: A Deep-Learning-Based Parameterization for Complex Geomodels,"Geological parameterization enables the representation of geomodels in terms
of a relatively small set of variables. Parameterization is therefore very
useful in the context of data assimilation and uncertainty quantification. In
this study, a deep-learning-based geological parameterization algorithm,
CNN-PCA, is developed for complex 3D geomodels. CNN-PCA entails the use of
convolutional neural networks as a post-processor for the low-dimensional
principal component analysis representation of a geomodel. The 3D treatments
presented here differ somewhat from those used in the 2D CNN-PCA procedure.
Specifically, we introduce a new supervised-learning-based reconstruction loss,
which is used in combination with style loss and hard data loss. The style loss
uses features extracted from a 3D CNN pretrained for video classification. The
3D CNN-PCA algorithm is applied for the generation of conditional 3D
realizations, defined on $60\times60\times40$ grids, for three geological
scenarios (binary and bimodal channelized systems, and a three-facies
channel-levee-mud system). CNN-PCA realizations are shown to exhibit geological
features that are visually consistent with reference models generated using
object-based methods. Statistics of flow responses ($\text{P}_{10}$,
$\text{P}_{50}$, $\text{P}_{90}$ percentile results) for test sets of 3D
CNN-PCA models are shown to be in consistent agreement with those from
reference geomodels. Lastly, CNN-PCA is successfully applied for history
matching with ESMDA for the bimodal channelized system.","Yimin Liu, Louis J. Durlofsky",2020,http://arxiv.org/abs/2007.08478v1
"PCA Initialization for Approximate Message Passing in Rotationally
  Invariant Models","We study the problem of estimating a rank-$1$ signal in the presence of
rotationally invariant noise-a class of perturbations more general than
Gaussian noise. Principal Component Analysis (PCA) provides a natural
estimator, and sharp results on its performance have been obtained in the
high-dimensional regime. Recently, an Approximate Message Passing (AMP)
algorithm has been proposed as an alternative estimator with the potential to
improve the accuracy of PCA. However, the existing analysis of AMP requires an
initialization that is both correlated with the signal and independent of the
noise, which is often unrealistic in practice. In this work, we combine the two
methods, and propose to initialize AMP with PCA. Our main result is a rigorous
asymptotic characterization of the performance of this estimator. Both the AMP
algorithm and its analysis differ from those previously derived in the Gaussian
setting: at every iteration, our AMP algorithm requires a specific term to
account for PCA initialization, while in the Gaussian case, PCA initialization
affects only the first iteration of AMP. The proof is based on a two-phase
artificial AMP that first approximates the PCA estimator and then mimics the
true AMP. Our numerical simulations show an excellent agreement between AMP
results and theoretical predictions, and suggest an interesting open direction
on achieving Bayes-optimal performance.","Marco Mondelli, Ramji Venkataramanan",2021,http://arxiv.org/abs/2106.02356v2
Nonlinear PCA for Spatio-Temporal Analysis of Earth Observation Data,"Remote sensing observations, products and simulations are fundamental sources
of information to monitor our planet and its climate variability. Uncovering
the main modes of spatial and temporal variability in Earth data is essential
to analyze and understand the underlying physical dynamics and processes
driving the Earth System. Dimensionality reduction methods can work with
spatio-temporal datasets and decompose the information efficiently. Principal
Component Analysis (PCA), also known as Empirical Orthogonal Functions (EOF) in
geophysics, has been traditionally used to analyze climatic data. However, when
nonlinear feature relations are present, PCA/EOF fails. In this work, we
propose a nonlinear PCA method to deal with spatio-temporal Earth System data.
The proposed method, called Rotated Complex Kernel PCA (ROCK-PCA for short),
works in reproducing kernel Hilbert spaces to account for nonlinear processes,
operates in the complex kernel domain to account for both space and time
features, and adds an extra rotation for improved flexibility. The result is an
explicitly resolved spatio-temporal decomposition of the Earth data cube. The
method is unsupervised and computationally very efficient.We illustrate its
ability to uncover spatio-temporal patterns using synthetic experiments and
real data. Results of the decomposition of three essential climate variables
are shown: satellite-based global Gross Primary Productivity (GPP) and Soil
Moisture (SM), and reanalysis Sea Surface Temperature (SST) data. The ROCK-PCA
method allows identifying their annual and seasonal oscillations, as well as
their non-seasonal trends and spatial variability patterns.","Diego Bueso, Maria Piles, Gustau Camps-Valls",2020,http://arxiv.org/abs/2002.04539v1
"Molecular Distributions of the Disk/Envelope System of L483: Principal
  Component Analysis for the Image Cube Data","Unbiased understandings of molecular distributions in a disk/envelope system
of a low-mass protostellar source are crucial for investigating physical and
chemical evolution processes. We have observed 23 molecular lines toward the
Class 0 protostellar source L483 with ALMA and have performed principal
component analysis (PCA) for their cube data (PCA-3D) to characterize their
distributions and velocity structures in the vicinity of the protostar. The sum
of the contributions of the first three components is 63.1 %. Most
oxygen-bearing complex-organic-molecule lines have a large correlation with the
first principal component (PC1), representing the overall structure of the
disk/envelope system around the protostar. Contrary, the C18O and SiO emissions
show small and negative correlations with PC1. The NH2CHO lines stand out
conspicuously at the second principal component (PC2), revealing more compact
distribution. The HNCO lines and the high excitation line of CH3OH have a
similar trend for PC2 to NH2CHO. On the other hand, C18O is well correlated
with the third principal component (PC3). Thus, PCA-3D enables us to elucidate
the similarities and the differences of the distributions and the velocity
structures among molecular lines simultaneously, so that the chemical
differentiation between the oxygen-bearing complex organic molecules and the
nitrogen-bearing ones is revealed in this source. We have also conducted PCA
for the moment 0 maps (PCA-2D) and that for the spectral line profiles
(PCA-1D). While they can extract part of characteristics of the molecular-line
data, PCA-3D is essential for comprehensive understandings. Characteristic
features of the molecular-line distributions are discussed on NH2CHO.","Yuki Okoda, Yoko Oya, Shotaro Abe, Ayano Komaki, Yoshimasa Watanabe, Satoshi Yamamoto",2021,http://arxiv.org/abs/2110.00150v1
Local manifold learning and its link to domain-based physics knowledge,"In many reacting flow systems, the thermo-chemical state-space is known or
assumed to evolve close to a low-dimensional manifold (LDM). Various approaches
are available to obtain those manifolds and subsequently express the original
high-dimensional space with fewer parameterizing variables. Principal component
analysis (PCA) is one of the dimensionality reduction methods that can be used
to obtain LDMs. PCA does not make prior assumptions about the parameterizing
variables and retrieves them empirically from the training data. In this paper,
we show that PCA applied in local clusters of data (local PCA) is capable of
detecting the intrinsic parameterization of the thermo-chemical state-space. We
first demonstrate that utilizing three common combustion models of varying
complexity: the Burke-Schumann model, the chemical equilibrium model and the
homogeneous reactor. Parameterization of these models is known a priori which
allows for benchmarking with the local PCA approach. We further extend the
application of local PCA to a more challenging case of a turbulent non-premixed
$n$-heptane/air jet flame for which the parameterization is no longer obvious.
Our results suggest that meaningful parameterization can be obtained also for
more complex datasets. We show that local PCA finds variables that can be
linked to local stoichiometry, reaction progress and soot formation processes.","Kamila Zdybał, Giuseppe D'Alessio, Antonio Attili, Axel Coussement, James C. Sutherland, Alessandro Parente",2022,http://arxiv.org/abs/2207.00275v1
Scalable and Privacy-Preserving Federated Principal Component Analysis,"Principal component analysis (PCA) is an essential algorithm for
dimensionality reduction in many data science domains. We address the problem
of performing a federated PCA on private data distributed among multiple data
providers while ensuring data confidentiality. Our solution, SF-PCA, is an
end-to-end secure system that preserves the confidentiality of both the
original data and all intermediate results in a passive-adversary model with up
to all-but-one colluding parties. SF-PCA jointly leverages multiparty
homomorphic encryption, interactive protocols, and edge computing to
efficiently interleave computations on local cleartext data with operations on
collectively encrypted data. SF-PCA obtains results as accurate as non-secure
centralized solutions, independently of the data distribution among the
parties. It scales linearly or better with the dataset dimensions and with the
number of data providers. SF-PCA is more precise than existing approaches that
approximate the solution by combining local analysis results, and between 3x
and 250x faster than privacy-preserving alternatives based solely on secure
multiparty computation or homomorphic encryption. Our work demonstrates the
practical applicability of secure and federated PCA on private distributed
datasets.","David Froelicher, Hyunghoon Cho, Manaswitha Edupalli, Joao Sa Sousa, Jean-Philippe Bossuat, Apostolos Pyrgelis, Juan R. Troncoso-Pastoriza, Bonnie Berger, Jean-Pierre Hubaux",2023,http://arxiv.org/abs/2304.00129v1
"ALPCAH: Sample-wise Heteroscedastic PCA with Tail Singular Value
  Regularization","Principal component analysis (PCA) is a key tool in the field of data
dimensionality reduction that is useful for various data science problems.
However, many applications involve heterogeneous data that varies in quality
due to noise characteristics associated with different sources of the data.
Methods that deal with this mixed dataset are known as heteroscedastic methods.
Current methods like HePPCAT make Gaussian assumptions of the basis
coefficients that may not hold in practice. Other methods such as Weighted PCA
(WPCA) assume the noise variances are known, which may be difficult to know in
practice. This paper develops a PCA method that can estimate the sample-wise
noise variances and use this information in the model to improve the estimate
of the subspace basis associated with the low-rank structure of the data. This
is done without distributional assumptions of the low-rank component and
without assuming the noise variances are known. Simulations show the
effectiveness of accounting for such heteroscedasticity in the data, the
benefits of using such a method with all of the data versus retaining only good
data, and comparisons are made against other PCA methods established in the
literature like PCA, Robust PCA (RPCA), and HePPCAT. Code available at
https://github.com/javiersc1/ALPCAH","Javier Salazar Cavazos, Jeffrey A. Fessler, Laura Balzano",2023,http://arxiv.org/abs/2307.02745v2
Supervised Dynamic PCA: Linear Dynamic Forecasting with Many Predictors,"This paper proposes a novel dynamic forecasting method using a new supervised
Principal Component Analysis (PCA) when a large number of predictors are
available. The new supervised PCA provides an effective way to bridge the gap
between predictors and the target variable of interest by scaling and combining
the predictors and their lagged values, resulting in an effective dynamic
forecasting. Unlike the traditional diffusion-index approach, which does not
learn the relationships between the predictors and the target variable before
conducting PCA, we first re-scale each predictor according to their
significance in forecasting the targeted variable in a dynamic fashion, and a
PCA is then applied to a re-scaled and additive panel, which establishes a
connection between the predictability of the PCA factors and the target
variable. Furthermore, we also propose to use penalized methods such as the
LASSO approach to select the significant factors that have superior predictive
power over the others. Theoretically, we show that our estimators are
consistent and outperform the traditional methods in prediction under some mild
conditions. We conduct extensive simulations to verify that the proposed method
produces satisfactory forecasting results and outperforms most of the existing
methods using the traditional PCA. A real example of predicting U.S.
macroeconomic variables using a large number of predictors showcases that our
method fares better than most of the existing ones in applications. The
proposed method thus provides a comprehensive and effective approach for
dynamic forecasting in high-dimensional data analysis.","Zhaoxing Gao, Ruey S. Tsay",2023,http://arxiv.org/abs/2307.07689v1
"Prostate cancer inference via weakly-supervised learning using a large
  collection of negative MRI","Recent advances in medical imaging techniques have led to significant
improvements in the management of prostate cancer (PCa). In particular,
multi-parametric MRI (mp-MRI) continues to gain clinical acceptance as the
preferred imaging technique for non-invasive detection and grading of PCa.
However, the machine learning-based diagnosis systems for PCa are often
constrained by the limited access to accurate lesion ground truth annotations
for training. The performance of the machine learning system is highly
dependable on both quality and quantity of lesion annotations associated with
histopathologic findings, resulting in limited scalability and clinical
validation. Here, we propose the baseline MRI model to alternatively learn the
appearance of mp-MRI using radiology-confirmed negative MRI cases via weakly
supervised learning. Since PCa lesions are case-specific and highly
heterogeneous, it is assumed to be challenging to synthesize PCa lesions using
the baseline MRI model, while it would be relatively easier to synthesize the
normal appearance in mp-MRI. We then utilize the baseline MRI model to infer
the pixel-wise suspiciousness of PCa by comparing the original and synthesized
MRI with two distance functions. We trained and validated the baseline MRI
model using 1,145 negative prostate mp-MRI scans. For evaluation, we used
separated 232 mp-MRI scans, consisting of both positive and negative MRI cases.
The 116 positive MRI scans were annotated by radiologists, confirmed with
post-surgical whole-gland specimens. The suspiciousness map was evaluated by
receiver operating characteristic (ROC) analysis for PCa lesions versus non-PCa
regions classification and free-response receiver operating characteristic
(FROC) analysis for PCa localization. Our proposed method achieved 0.84 area
under the ROC curve and 77.0% sensitivity at one false positive per patient in
FROC analysis.","Ruiming Cao, Xinran Zhong, Fabien Scalzo, Steven Raman, Kyung hyun Sung",2019,http://arxiv.org/abs/1910.02185v1
Efficient Sparse PCA via Block-Diagonalization,"Sparse Principal Component Analysis (Sparse PCA) is a pivotal tool in data
analysis and dimensionality reduction. However, Sparse PCA is a challenging
problem in both theory and practice: it is known to be NP-hard and current
exact methods generally require exponential runtime. In this paper, we propose
a novel framework to efficiently approximate Sparse PCA by (i) approximating
the general input covariance matrix with a re-sorted block-diagonal matrix,
(ii) solving the Sparse PCA sub-problem in each block, and (iii) reconstructing
the solution to the original problem. Our framework is simple and powerful: it
can leverage any off-the-shelf Sparse PCA algorithm and achieve significant
computational speedups, with a minor additive error that is linear in the
approximation error of the block-diagonal matrix. Suppose $g(k, d)$ is the
runtime of an algorithm (approximately) solving Sparse PCA in dimension $d$ and
with sparsity constant $k$. Our framework, when integrated with this algorithm,
reduces the runtime to $\mathcal{O}\left(\frac{d}{d^\star} \cdot g(k, d^\star)
+ d^2\right)$, where $d^\star \leq d$ is the largest block size of the
block-diagonal matrix. For instance, integrating our framework with the
Branch-and-Bound algorithm reduces the complexity from $g(k, d) =
\mathcal{O}(k^3\cdot d^k)$ to $\mathcal{O}(k^3\cdot d \cdot (d^\star)^{k-1})$,
demonstrating exponential speedups if $d^\star$ is small. We perform
large-scale evaluations on many real-world datasets: for exact Sparse PCA
algorithm, our method achieves an average speedup factor of 100.50, while
maintaining an average approximation error of 0.61%; for approximate Sparse PCA
algorithm, our method achieves an average speedup factor of 6.00 and an average
approximation error of -0.91%, meaning that our method oftentimes finds better
solutions.","Alberto Del Pia, Dekun Zhou, Yinglun Zhu",2024,http://arxiv.org/abs/2410.14092v2
Robust Principal Component Analysis on Graphs,"Principal Component Analysis (PCA) is the most widely used tool for linear
dimensionality reduction and clustering. Still it is highly sensitive to
outliers and does not scale well with respect to the number of data samples.
Robust PCA solves the first issue with a sparse penalty term. The second issue
can be handled with the matrix factorization model, which is however
non-convex. Besides, PCA based clustering can also be enhanced by using a graph
of data similarity. In this article, we introduce a new model called ""Robust
PCA on Graphs"" which incorporates spectral graph regularization into the Robust
PCA framework. Our proposed model benefits from 1) the robustness of principal
components to occlusions and missing values, 2) enhanced low-rank recovery, 3)
improved clustering property due to the graph smoothness assumption on the
low-rank matrix, and 4) convexity of the resulting optimization problem.
Extensive experiments on 8 benchmark, 3 video and 2 artificial datasets with
corruptions clearly reveal that our model outperforms 10 other state-of-the-art
models in its clustering and low-rank recovery tasks.","Nauman Shahid, Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, Pierre Vandergheynst",2015,http://arxiv.org/abs/1504.06151v1
"Robust PCA: Optimization of the Robust Reconstruction Error over the
  Stiefel Manifold","It is well known that Principal Component Analysis (PCA) is strongly affected
by outliers and a lot of effort has been put into robustification of PCA. In
this paper we present a new algorithm for robust PCA minimizing the trimmed
reconstruction error. By directly minimizing over the Stiefel manifold, we
avoid deflation as often used by projection pursuit methods. In distinction to
other methods for robust PCA, our method has no free parameter and is
computationally very efficient. We illustrate the performance on various
datasets including an application to background modeling and subtraction. Our
method performs better or similar to current state-of-the-art methods while
being faster.","Anastasia Podosinnikova, Simon Setzer, Matthias Hein",2015,http://arxiv.org/abs/1506.00323v1
Barycentric Subspace Analysis on Manifolds,"This paper investigates the generalization of Principal Component Analysis
(PCA) to Riemannian manifolds. We first propose a new and general type of
family of subspaces in manifolds that we call barycentric subspaces. They are
implicitly defined as the locus of points which are weighted means of $k+1$
reference points. As this definition relies on points and not on tangent
vectors, it can also be extended to geodesic spaces which are not Riemannian.
For instance, in stratified spaces, it naturally allows principal subspaces
that span several strata, which is impossible in previous generalizations of
PCA. We show that barycentric subspaces locally define a submanifold of
dimension k which generalizes geodesic subspaces.Second, we rephrase PCA in
Euclidean spaces as an optimization on flags of linear subspaces (a hierarchy
of properly embedded linear subspaces of increasing dimension). We show that
the Euclidean PCA minimizes the Accumulated Unexplained Variances by all the
subspaces of the flag (AUV). Barycentric subspaces are naturally nested,
allowing the construction of hierarchically nested subspaces. Optimizing the
AUV criterion to optimally approximate data points with flags of affine spans
in Riemannian manifolds lead to a particularly appealing generalization of PCA
on manifolds called Barycentric Subspaces Analysis (BSA).",Xavier Pennec,2016,http://arxiv.org/abs/1607.02833v2
"PCA in Data-Dependent Noise (Correlated-PCA): Nearly Optimal Finite
  Sample Guarantees","We study Principal Component Analysis (PCA) in a setting where a part of the
corrupting noise is data-dependent and, as a result, the noise and the true
data are correlated. Under a bounded-ness assumption on the true data and the
noise, and a simple assumption on data-noise correlation, we obtain a nearly
optimal sample complexity bound for the most commonly used PCA solution,
singular value decomposition (SVD). This bound is a significant improvement
over the bound obtained by Vaswani and Guo in recent work (NIPS 2016) where
this ""correlated-PCA"" problem was first studied; and it holds under a
significantly weaker data-noise correlation assumption than the one used for
this earlier result.","Namrata Vaswani, Praneeth Narayanamurthy",2017,http://arxiv.org/abs/1702.03070v3
Maximum Margin Principal Components,"Principal Component Analysis (PCA) is a very successful dimensionality
reduction technique, widely used in predictive modeling. A key factor in its
widespread use in this domain is the fact that the projection of a dataset onto
its first $K$ principal components minimizes the sum of squared errors between
the original data and the projected data over all possible rank $K$
projections. Thus, PCA provides optimal low-rank representations of data for
least-squares linear regression under standard modeling assumptions. On the
other hand, when the loss function for a prediction problem is not the
least-squares error, PCA is typically a heuristic choice of dimensionality
reduction -- in particular for classification problems under the zero-one loss.
In this paper we target classification problems by proposing a straightforward
alternative to PCA that aims to minimize the difference in margin distribution
between the original and the projected data. Extensive experiments show that
our simple approach typically outperforms PCA on any particular dataset, in
terms of classification error, though this difference is not always
statistically significant, and despite being a filter method is frequently
competitive with Partial Least Squares (PLS) and Lasso on a wide range of
datasets.","Xianghui Luo, Robert J. Durrant",2017,http://arxiv.org/abs/1705.06371v1
Principal component analysis for big data,"Big data is transforming our world, revolutionizing operations and analytics
everywhere, from financial engineering to biomedical sciences. The complexity
of big data often makes dimension reduction techniques necessary before
conducting statistical inference. Principal component analysis, commonly
referred to as PCA, has become an essential tool for multivariate data analysis
and unsupervised dimension reduction, the goal of which is to find a lower
dimensional subspace that captures most of the variation in the dataset. This
article provides an overview of methodological and theoretical developments of
PCA over the last decade, with focus on its applications to big data analytics.
We first review the mathematical formulation of PCA and its theoretical
development from the view point of perturbation analysis. We then briefly
discuss the relationship between PCA and factor analysis as well as its
applications to large covariance estimation and multiple testing. PCA also
finds important applications in many modern machine learning problems, and we
focus on community detection, ranking, mixture model and manifold learning in
this paper.","Jianqing Fan, Qiang Sun, Wen-Xin Zhou, Ziwei Zhu",2018,http://arxiv.org/abs/1801.01602v1
"Machine Learning of Frustrated Classical Spin Models. II. Kernel
  Principal Component Analysis","In this work we apply the principal component analysis (PCA) method with
kernel trick to study classification of phases and phase transition in
classical XY models in frustrated lattices. Comparing to our previous work with
linear PCA method, the kernel PCA can capture non-linear function. In this
case, the Z2 chiral order of classical spins in these lattices are indeed a
non-linear function of the input spin configurations. In addition to the
principal component revealed by linear PCA, the kernel PCA can find out two
more principal components using data generated by Monte Carlo simulation at
various temperatures at input. One of them relates to the strength of the U(1)
order parameter and the other directly manifests the chiral order parameter
that characterizes the Z2 symmetry breaking. For a temperature resolved study,
the temperature dependence of the principal eigenvalue associated with the Z2
symmetry breaking clearly shows a second order phase transition behavior.","Ce Wang, Hui Zhai",2018,http://arxiv.org/abs/1803.01205v1
"Fast, Parameter free Outlier Identification for Robust PCA","Robust PCA, the problem of PCA in the presence of outliers has been
extensively investigated in the last few years. Here we focus on Robust PCA in
the column sparse outlier model. The existing methods for column sparse outlier
model assumes either the knowledge of the dimension of the lower dimensional
subspace or the fraction of outliers in the system. However in many
applications knowledge of these parameters is not available. Motivated by this
we propose a parameter free outlier identification method for robust PCA which
a) does not require the knowledge of outlier fraction, b) does not require the
knowledge of the dimension of the underlying subspace, c) is computationally
simple and fast. Further, analytical guarantees are derived for outlier
identification and the performance of the algorithm is compared with the
existing state of the art methods.","Vishnu Menon, Sheetal Kalyani",2018,http://arxiv.org/abs/1804.04791v1
"Principal Component Analysis as a Tool for Characterizing Black Hole
  Images and Variability","We explore the use of principal component analysis (PCA) to characterize
high-fidelity simulations and interferometric observations of the millimeter
emission that originates near the horizons of accreting black holes. We show
mathematically that the Fourier transforms of eigenimages derived from PCA
applied to an ensemble of images in the spatial-domain are identical to the
eigenvectors of PCA applied to the ensemble of the Fourier transforms of the
images, which suggests that this approach may be applied to modeling the sparse
interferometric Fourier-visibilities produced by an array such as the Event
Horizon Telescope (EHT). We also show that the simulations in the spatial
domain themselves can be compactly represented with a PCA-derived basis of
eigenimages allowing for detailed comparisons between variable observations and
time-dependent models, as well as for detection of outliers or rare events
within a time series of images. Furthermore, we demonstrate that the spectrum
of PCA eigenvalues is a diagnostic of the power spectrum of the structure and,
hence, of the underlying physical processes in the simulated and observed
images.","Lia Medeiros, Tod R. Lauer, Dimitrios Psaltis, Feryal Özel",2018,http://arxiv.org/abs/1804.05903v1
Interpretable Sparse Proximate Factors for Large Dimensions,"This paper proposes sparse and easy-to-interpret proximate factors to
approximate statistical latent factors. Latent factors in a large-dimensional
factor model can be estimated by principal component analysis (PCA), but are
usually hard to interpret. We obtain proximate factors that are easier to
interpret by shrinking the PCA factor weights and setting them to zero except
for the largest absolute ones. We show that proximate factors constructed with
only 5-10% of the data are usually sufficient to almost perfectly replicate the
population and PCA factors without actually assuming a sparse structure in the
weights or loadings. Using extreme value theory we explain why sparse proximate
factors can be substitutes for non-sparse PCA factors. We derive analytical
asymptotic bounds for the correlation of appropriately rotated proximate
factors with the population factors. These bounds provide guidance on how to
construct the proximate factors. In simulations and empirical analyses of
financial portfolio and macroeconomic data we illustrate that sparse proximate
factors are close substitutes for PCA factors with average correlations of
around 97.5% while being interpretable.","Markus Pelger, Ruoxuan Xiong",2018,http://arxiv.org/abs/1805.03373v4
"Structured and Unstructured Outlier Identification for Robust PCA: A Non
  iterative, Parameter free Algorithm","Robust PCA, the problem of PCA in the presence of outliers has been
extensively investigated in the last few years. Here we focus on Robust PCA in
the outlier model where each column of the data matrix is either an inlier or
an outlier. Most of the existing methods for this model assumes either the
knowledge of the dimension of the lower dimensional subspace or the fraction of
outliers in the system. However in many applications knowledge of these
parameters is not available. Motivated by this we propose a parameter free
outlier identification method for robust PCA which a) does not require the
knowledge of outlier fraction, b) does not require the knowledge of the
dimension of the underlying subspace, c) is computationally simple and fast d)
can handle structured and unstructured outliers. Further, analytical guarantees
are derived for outlier identification and the performance of the algorithm is
compared with the existing state of the art methods in both real and synthetic
data for various outlier structures.","Vishnu Menon, Sheetal Kalyani",2018,http://arxiv.org/abs/1809.04445v1
Tensor Sparse PCA and Face Recognition: A Novel Approach,"Face recognition is the important field in machine learning and pattern
recognition research area. It has a lot of applications in military, finance,
public security, to name a few. In this paper, the combination of the tensor
sparse PCA with the nearest-neighbor method (and with the kernel ridge
regression method) will be proposed and applied to the face dataset.
Experimental results show that the combination of the tensor sparse PCA with
any classification system does not always reach the best accuracy performance
measures. However, the accuracy of the combination of the sparse PCA method and
one specific classification system is always better than the accuracy of the
combination of the PCA method and one specific classification system and is
always better than the accuracy of the classification system itself.","Loc Hoang Tran, Linh Hoang Tran",2019,http://arxiv.org/abs/1904.08496v4
"Imbalanced Learning-based Automatic SAR Images Change Detection by
  Morphologically Supervised PCA-Net","Change detection is a quite challenging task due to the imbalance between
unchanged and changed class. In addition, the traditional difference map
generated by log-ratio is subject to the speckle, which will reduce the
accuracy. In this letter, an imbalanced learning-based change detection is
proposed based on PCA network (PCA-Net), where a supervised PCA-Net is designed
to obtain the robust features directly from given multitemporal SAR images
instead of a difference map. Furthermore, to tackle with the imbalance between
changed and unchanged classes, we propose a morphologically supervised learning
method, where the knowledge in the pixels near the boundary between two classes
are exploited to guide network training. Finally, our proposed PCA-Net can be
trained by the datasets with available reference maps and applied to a new
dataset, which is quite practical in change detection projects. Our proposed
method is verified on five sets of multiple temporal SAR images. It is
demonstrated from the experiment results that with the knowledge in training
samples from the boundary, the learned features benefit for change detection
and make the proposed method outperforms than supervised methods trained by
randomly drawing samples.","Rongfang Wang, Jie Zhang, Jia-Wei Chen, Licheng Jiao, Mi Wang",2019,http://arxiv.org/abs/1906.07923v1
Generalized Image Reconstruction over T-Algebra,"Principal Component Analysis (PCA) is well known for its capability of
dimension reduction and data compression. However, when using PCA for
compressing/reconstructing images, images need to be recast to vectors. The
vectorization of images makes some correlation constraints of neighboring
pixels and spatial information lost. To deal with the drawbacks of the
vectorizations adopted by PCA, we used small neighborhoods of each pixel to
form compounded pixels and use a tensorial version of PCA, called TPCA
(Tensorial Principal Component Analysis), to compress and reconstruct a
compounded image of compounded pixels. Our experiments on public data show that
TPCA compares favorably with PCA in compressing and reconstructing images. We
also show in our experiments that the performance of TPCA increases when the
order of compounded pixels increases.","Liang Liao, Xuechun Zhang, Xinqiang Wang, Sen Lin, Xin Liu",2021,http://arxiv.org/abs/2101.06650v3
"Empirical comparison between autoencoders and traditional dimensionality
  reduction methods","In order to process efficiently ever-higher dimensional data such as images,
sentences, or audio recordings, one needs to find a proper way to reduce the
dimensionality of such data. In this regard, SVD-based methods including PCA
and Isomap have been extensively used. Recently, a neural network alternative
called autoencoder has been proposed and is often preferred for its higher
flexibility. This work aims to show that PCA is still a relevant technique for
dimensionality reduction in the context of classification. To this purpose, we
evaluated the performance of PCA compared to Isomap, a deep autoencoder, and a
variational autoencoder. Experiments were conducted on three commonly used
image datasets: MNIST, Fashion-MNIST, and CIFAR-10. The four different
dimensionality reduction techniques were separately employed on each dataset to
project data into a low-dimensional space. Then a k-NN classifier was trained
on each projection with a cross-validated random search over the number of
neighbours. Interestingly, our experiments revealed that k-NN achieved
comparable accuracy on PCA and both autoencoders' projections provided a big
enough dimension. However, PCA computation time was two orders of magnitude
faster than its neural network counterparts.","Quentin Fournier, Daniel Aloise",2021,http://arxiv.org/abs/2103.04874v1
"A Communication-Efficient and Privacy-Aware Distributed Algorithm for
  Sparse PCA","Sparse principal component analysis (PCA) improves interpretability of the
classic PCA by introducing sparsity into the dimension-reduction process.
Optimization models for sparse PCA, however, are generally non-convex,
non-smooth and more difficult to solve, especially on large-scale datasets
requiring distributed computation over a wide network. In this paper, we
develop a distributed and centralized algorithm called DSSAL1 for sparse PCA
that aims to achieve low communication overheads by adapting a newly proposed
subspace-splitting strategy to accelerate convergence. Theoretically,
convergence to stationary points is established for DSSAL1. Extensive numerical
results show that DSSAL1 requires far fewer rounds of communication than
state-of-the-art peer methods. In addition, we make the case that since
messages exchanged in DSSAL1 are well-masked, the possibility of private-data
leakage in DSSAL1 is much lower than in some other distributed algorithms.","Lei Wang, Xin Liu, Yin Zhang",2021,http://arxiv.org/abs/2106.03320v2
"8 x 8 Terahertz Photoconductive Antenna Array and Parallelized Signal
  Acquisition System for Fast Spatially Resolved Time Domain Spectroscopy","Terahertz (THz) technology is promising in several applications such as
imaging, spectroscopy and communications. Among several methods in the
generation and detection of THz waves, a THz time domain system (TDS) that is
developed using photoconductive antennas (PCA) as emitter and detector presents
several advantages such as simple alignment, low cost, high performance etc. In
this work, we report the design, fabrication and characterization of a 2-D PCA
array that is capable of detecting both the amplitude and phase of the THz
pulse. The PCA array is fabricated using LT-GaAs and has 8 channels with 64
pixels (8x8). The infrared probe beam is steered and focused towards each pixel
of the PCA array using a spatial light modulator (SLM). The measured
photocurrent (amplitude and phase) from each channel is recorded separately and
the frequencies up to 1.4 THz can be detected. Furthermore, the parameters such
as directional time delay of the THz pulse, crosstalk between the channels
etc., were characterized. Finally, we show that the proposed 2D PCA array
design is flexible and can be used for accelerated THz spectral image
acquisition.","R. Henri, K. Nallappan, D. S. Ponomarev, D. V. Lavrukhin, A. E. Yachmenev, R. A. Khabibullin, M. Skorobogatiy",2021,http://arxiv.org/abs/2106.09457v1
Kernel PCA with the Nyström method,"The Nystr\""om method is one of the most popular techniques for improving the
scalability of kernel methods. However, it has not yet been derived for kernel
PCA in line with classical PCA. In this paper we derive kernel PCA with the
Nystr\""om method, thereby providing one of the few available options to make
kernel PCA scalable. We further study its statistical accuracy through a
finite-sample confidence bound on the empirical reconstruction error compared
to the full method. The behaviours of the method and bound are illustrated
through computer experiments on multiple real-world datasets. As an application
of the method we present kernel principal component regression with the
Nystr\""om method, as an alternative to Nystr\""om kernel ridge regression for
efficient regularized regression with kernels.",Fredrik Hallgren,2021,http://arxiv.org/abs/2109.05578v3
"Detecting Anomaly in Chemical Sensors via L1-Kernels based Principal
  Component Analysis","We propose a kernel-PCA based method to detect anomaly in chemical sensors.
We use temporal signals produced by chemical sensors to form vectors to perform
the Principal Component Analysis (PCA). We estimate the kernel-covariance
matrix of the sensor data and compute the eigenvector corresponding to the
largest eigenvalue of the covariance matrix. The anomaly can be detected by
comparing the difference between the actual sensor data and the reconstructed
data from the dominant eigenvector. In this paper, we introduce a new
multiplication-free kernel, which is related to the l1-norm for the anomaly
detection task. The l1-kernel PCA is not only computationally efficient but
also energy-efficient because it does not require any actual multiplications
during the kernel covariance matrix computation. Our experimental results show
that our kernel-PCA method achieves a higher area under curvature (AUC) score
(0.7483) than the baseline regular PCA method (0.7366).","Hongyi Pan, Diaa Badawi, Ishaan Bassi, Sule Ozev, Ahmet Enis Cetin",2022,http://arxiv.org/abs/2201.02709v2
Gain with no Pain: Efficient Kernel-PCA by Nyström Sampling,"In this paper, we propose and study a Nystr\""om based approach to efficient
large scale kernel principal component analysis (PCA). The latter is a natural
nonlinear extension of classical PCA based on considering a nonlinear feature
map or the corresponding kernel. Like other kernel approaches, kernel PCA
enjoys good mathematical and statistical properties but, numerically, it scales
poorly with the sample size. Our analysis shows that Nystr\""om sampling greatly
improves computational efficiency without incurring any loss of statistical
accuracy. While similar effects have been observed in supervised learning, this
is the first such result for PCA. Our theoretical findings, which are also
illustrated by numerical results, are based on a combination of analytic and
concentration of measure techniques. Our study is more broadly motivated by the
question of understanding the interplay between statistical and computational
requirements for learning.","Nicholas Sterge, Bharath Sriperumbudur, Lorenzo Rosasco, Alessandro Rudi",2019,http://arxiv.org/abs/1907.05226v1
"Optimal Structured Principal Subspace Estimation: Metric Entropy and
  Minimax Rates","Driven by a wide range of applications, many principal subspace estimation
problems have been studied individually under different structural constraints.
This paper presents a unified framework for the statistical analysis of a
general structured principal subspace estimation problem which includes as
special cases non-negative PCA/SVD, sparse PCA/SVD, subspace constrained
PCA/SVD, and spectral clustering. General minimax lower and upper bounds are
established to characterize the interplay between the information-geometric
complexity of the structural set for the principal subspaces, the
signal-to-noise ratio (SNR), and the dimensionality. The results yield
interesting phase transition phenomena concerning the rates of convergence as a
function of the SNRs and the fundamental limit for consistent estimation.
Applying the general results to the specific settings yields the minimax rates
of convergence for those problems, including the previous unknown optimal rates
for non-negative PCA/SVD, sparse SVD and subspace constrained PCA/SVD.","T. Tony Cai, Hongzhe Li, Rong Ma",2020,http://arxiv.org/abs/2002.07624v3
"Improved Dimensionality Reduction of various Datasets using Novel
  Multiplicative Factoring Principal Component Analysis (MPCA)","Principal Component Analysis (PCA) is known to be the most widely applied
dimensionality reduction approach. A lot of improvements have been done on the
traditional PCA, in order to obtain optimal results in the dimensionality
reduction of various datasets. In this paper, we present an improvement to the
traditional PCA approach called Multiplicative factoring Principal Component
Analysis (MPCA). The advantage of MPCA over the traditional PCA is that a
penalty is imposed on the occurrence space through a multiplier to make
negligible the effect of outliers in seeking out projections. Here we apply two
multiplier approaches, total distance and cosine similarity metrics. These two
approaches can learn the relationship that exists between each of the data
points and the principal projections in the feature space. As a result of this,
improved low-rank projections are gotten through multiplying the data
iteratively to make negligible the effect of corrupt data in the training set.
Experiments were carried out on YaleB, MNIST, AR, and Isolet datasets and the
results were compared to results gotten from some popular dimensionality
reduction methods such as traditional PCA, RPCA-OM, and also some recently
published methods such as IFPCA-1 and IFPCA-2.",Chisom Ezinne Ogbuanya,2020,http://arxiv.org/abs/2009.12179v1
Scaled torus principal component analysis,"A particularly challenging context for dimensionality reduction is
multivariate circular data, i.e., data supported on a torus. Such kind of data
appears, e.g., in the analysis of various phenomena in ecology and astronomy,
as well as in molecular structures. This paper introduces Scaled Torus
Principal Component Analysis (ST-PCA), a novel approach to perform
dimensionality reduction with toroidal data. ST-PCA finds a data-driven map
from a torus to a sphere of the same dimension and a certain radius. The map is
constructed with multidimensional scaling to minimize the discrepancy between
pairwise geodesic distances in both spaces. ST-PCA then resorts to principal
nested spheres to obtain a nested sequence of subspheres that best fits the
data, which can afterwards be inverted back to the torus. Numerical experiments
illustrate how ST-PCA can be used to achieve meaningful dimensionality
reduction on low-dimensional torii, particularly with the purpose of clusters
separation, while two data applications in astronomy (three-dimensional torus)
and molecular biology (on a seven-dimensional torus) show that ST-PCA
outperforms existing methods for the investigated datasets.","Pavlos Zoubouloglou, Eduardo García-Portugués, J. S. Marron",2021,http://arxiv.org/abs/2110.04758v1
Principal Component Analysis versus Factor Analysis,"The article discusses selected problems related to both principal component
analysis (PCA) and factor analysis (FA). In particular, both types of analysis
were compared. A vector interpretation for both PCA and FA has also been
proposed. The problem of determining the number of principal components in PCA
and factors in FA was discussed in detail. A new criterion for determining the
number of factors and principal components is discussed, which will allow to
present most of the variance of each of the analyzed primary variables. An
efficient algorithm for determining the number of factors in FA, which complies
with this criterion, was also proposed. This algorithm was adapted to find the
number of principal components in PCA. It was also proposed to modify the PCA
algorithm using a new method of determining the number of principal components.
The obtained results were discussed.",Zenon Gniazdowski,2021,http://arxiv.org/abs/2110.11261v1
"Robust self-tuning semiparametric PCA for contaminated elliptical
  distribution","Principal component analysis (PCA) is one of the most popular dimension
reduction methods. The usual PCA is known to be sensitive to the presence of
outliers, and thus many robust PCA methods have been developed. Among them, the
Tyler's M-estimator is shown to be the most robust scatter estimator under the
elliptical distribution. However, when the underlying distribution is
contaminated and deviates from ellipticity, Tyler's M-estimator might not work
well. In this article, we apply the semiparametric theory to propose a robust
semiparametric PCA. The merits of our proposal are twofold. First, it is robust
to heavy-tailed elliptical distributions as well as robust to non-elliptical
outliers. Second, it pairs well with a data-driven tuning procedure, which is
based on active ratio and can adapt to different degrees of data outlyingness.
Theoretical properties are derived, including the influence functions for
various statistical functionals and asymptotic normality. Simulation studies
and a data analysis demonstrate the superiority of our method.","Hung Hung, Su-Yun Huang, Shinto Eguchi",2022,http://arxiv.org/abs/2206.03662v1
"A novel approach for Fair Principal Component Analysis based on
  eigendecomposition","Principal component analysis (PCA), a ubiquitous dimensionality reduction
technique in signal processing, searches for a projection matrix that minimizes
the mean squared error between the reduced dataset and the original one. Since
classical PCA is not tailored to address concerns related to fairness, its
application to actual problems may lead to disparity in the reconstruction
errors of different groups (e.g., men and women, whites and blacks, etc.), with
potentially harmful consequences such as the introduction of bias towards
sensitive groups. Although several fair versions of PCA have been proposed
recently, there still remains a fundamental gap in the search for algorithms
that are simple enough to be deployed in real systems. To address this, we
propose a novel PCA algorithm which tackles fairness issues by means of a
simple strategy comprising a one-dimensional search which exploits the
closed-form solution of PCA. As attested by numerical experiments, the proposal
can significantly improve fairness with a very small loss in the overall
reconstruction error and without resorting to complex optimization schemes.
Moreover, our findings are consistent in several real situations as well as in
scenarios with both unbalanced and balanced datasets.","Guilherme Dean Pelegrina, Leonardo Tomazeli Duarte",2022,http://arxiv.org/abs/2208.11362v1
Kernel PCA for multivariate extremes,"We propose kernel PCA as a method for analyzing the dependence structure of
multivariate extremes and demonstrate that it can be a powerful tool for
clustering and dimension reduction. Our work provides some theoretical insight
into the preimages obtained by kernel PCA, demonstrating that under certain
conditions they can effectively identify clusters in the data. We build on
these new insights to characterize rigorously the performance of kernel PCA
based on an extremal sample, i.e., the angular part of random vectors for which
the radius exceeds a large threshold. More specifically, we focus on the
asymptotic dependence of multivariate extremes characterized by the angular or
spectral measure in extreme value theory and provide a careful analysis in the
case where the extremes are generated from a linear factor model. We give
theoretical guarantees on the performance of kernel PCA preimages of such
extremes by leveraging their asymptotic distribution together with Davis-Kahan
perturbation bounds. Our theoretical findings are complemented with numerical
experiments illustrating the finite sample performance of our methods.","Marco Avella-Medina, Richard A. Davis, Gennady Samorodnitsky",2022,http://arxiv.org/abs/2211.13172v2
"A Decentralized Framework for Kernel PCA with Projection Consensus
  Constraints","This paper studies kernel PCA in a decentralized setting, where data are
distributively observed with full features in local nodes and a fusion center
is prohibited. Compared with linear PCA, the use of kernel brings challenges to
the design of decentralized consensus optimization: the local projection
directions are data-dependent. As a result, the consensus constraint in
distributed linear PCA is no longer valid. To overcome this problem, we propose
a projection consensus constraint and obtain an effective decentralized
consensus framework, where local solutions are expected to be the projection of
the global solution on the column space of local dataset. We also derive a
fully non-parametric, fast and convergent algorithm based on alternative
direction method of multiplier, of which each iteration is analytic and
communication-effcient. Experiments on a truly parallel architecture are
conducted on real-world data, showing that the proposed decentralized algorithm
is effective to utilize information of other nodes and takes great advantages
in running time over the central kernel PCA.","Fan He, Ruikai Yang, Lei Shi, Xiaolin Huang",2022,http://arxiv.org/abs/2211.15953v1
On the Multiway Principal Component Analysis,"Multiway data are becoming more and more common. While there are many
approaches to extending principal component analysis (PCA) from usual data
matrices to multiway arrays, their conceptual differences from the usual PCA,
and the methodological implications of such differences remain largely unknown.
This work aims to specifically address these questions. In particular, we
clarify the subtle difference between PCA and singular value decomposition
(SVD) for multiway data, and show that multiway principal components (PCs) can
be estimated reliably in absence of the eigengaps required by the usual PCA,
and in general much more efficiently than the usual PCs. Furthermore, the
sample multiway PCs are asymptotically independent and hence allow for separate
and more accurate inferences about the population PCs. The practical merits of
multiway PCA are further demonstrated through numerical, both simulated and
real data, examples.","Jialin Ouyang, Ming Yuan",2023,http://arxiv.org/abs/2302.07216v1
"Searching for anomalous quartic gauge couplings at muon colliders using
  principle component analysis","Searching for new physics~(NP) is one of the areas of high-energy physics
that requires the most processing of large amounts of data. At the same time,
quantum computing has huge potential advantages when dealing with large amounts
of data. The principal component analysis~(PCA) algorithm may be one of the
bridges connecting these two aspects. On the one hand, it can be used for
anomaly detection, and on the other hand, there are corresponding quantum
algorithms for PCA. In this paper, we investigate how to use PCA to search for
NP. Taking the example of anomalous quartic gauge couplings in the tri-photon
process at muon colliders, we find that PCA can be used to search for NP.
Compared with the traditional event selection strategy, the expected
constraints on the operator coefficients obtained by PCA based event selection
strategy are even better.","Yi-Fei Dong, Ying-Chen Mao, Ji-Chong Yang",2023,http://arxiv.org/abs/2304.01505v4
"Improved Privacy-Preserving PCA Using Optimized Homomorphic Matrix
  Multiplication","Principal Component Analysis (PCA) is a pivotal technique widely utilized in
the realms of machine learning and data analysis. It aims to reduce the
dimensionality of a dataset while minimizing the loss of information. In recent
years, there have been endeavors to utilize homomorphic encryption in
privacy-preserving PCA algorithms for the secure cloud computing scenario.
These approaches commonly employ a PCA routine known as PowerMethod, which
takes the covariance matrix as input and generates an approximate eigenvector
corresponding to the primary component of the dataset. However, their
performance is constrained by the absence of an efficient homomorphic
covariance matrix computation circuit and an accurate homomorphic vector
normalization strategy in the PowerMethod algorithm. In this study, we propose
a novel approach to privacy-preserving PCA that addresses these limitations,
resulting in superior efficiency, accuracy, and scalability compared to
previous approaches",Xirong Ma,2023,http://arxiv.org/abs/2305.17341v4
PLPCA: Persistent Laplacian Enhanced-PCA for Microarray Data Analysis,"Over the years, Principal Component Analysis (PCA) has served as the baseline
approach for dimensionality reduction in gene expression data analysis. It
primary objective is to identify a subset of disease-causing genes from a vast
pool of thousands of genes. However, PCA possesses inherent limitations that
hinder its interpretability, introduce classification ambiguity, and fail to
capture complex geometric structures in the data. Although these limitations
have been partially addressed in the literature by incorporating various
regularizers such as graph Laplacian regularization, existing improved PCA
methods still face challenges related to multiscale analysis and capturing
higher-order interactions in the data. To address these challenges, we propose
a novel approach called Persistent Laplacian-enhanced Principal Component
Analysis (PLPCA). PLPCA amalgamates the advantages of earlier regularized PCA
methods with persistent spectral graph theory, specifically persistent
Laplacians derived from algebraic topology. In contrast to graph Laplacians,
persistent Laplacians enable multiscale analysis through filtration and
incorporate higher-order simplicial complexes to capture higher-order
interactions in the data. We evaluate and validate the performance of PLPCA
using benchmark microarray datasets that involve normal tissue samples and four
different cancer tissues. Our extensive studies demonstrate that PLPCA
outperforms all other state-of-the-art models for classification tasks after
dimensionality reduction.","Sean Cottrell, Rui Wang, Guowei Wei",2023,http://arxiv.org/abs/2306.06292v1
Robust Principal Component Analysis using Density Power Divergence,"Principal component analysis (PCA) is a widely employed statistical tool used
primarily for dimensionality reduction. However, it is known to be adversely
affected by the presence of outlying observations in the sample, which is quite
common. Robust PCA methods using M-estimators have theoretical benefits, but
their robustness drop substantially for high dimensional data. On the other end
of the spectrum, robust PCA algorithms solving principal component pursuit or
similar optimization problems have high breakdown, but lack theoretical
richness and demand high computational power compared to the M-estimators. We
introduce a novel robust PCA estimator based on the minimum density power
divergence estimator. This combines the theoretical strength of the
M-estimators and the minimum divergence estimators with a high breakdown
guarantee regardless of data dimension. We present a computationally efficient
algorithm for this estimate. Our theoretical findings are supported by
extensive simulations and comparisons with existing robust PCA methods. We also
showcase the proposed algorithm's applicability on two benchmark datasets and a
credit card transactions dataset for fraud detection.","Subhrajyoty Roy, Ayanendranath Basu, Abhik Ghosh",2023,http://arxiv.org/abs/2309.13531v1
Sparse PCA with False Discovery Rate Controlled Variable Selection,"Sparse principal component analysis (PCA) aims at mapping large dimensional
data to a linear subspace of lower dimension. By imposing loading vectors to be
sparse, it performs the double duty of dimension reduction and variable
selection. Sparse PCA algorithms are usually expressed as a trade-off between
explained variance and sparsity of the loading vectors (i.e., number of
selected variables). As a high explained variance is not necessarily synonymous
with relevant information, these methods are prone to select irrelevant
variables. To overcome this issue, we propose an alternative formulation of
sparse PCA driven by the false discovery rate (FDR). We then leverage the
Terminating-Random Experiments (T-Rex) selector to automatically determine an
FDR-controlled support of the loading vectors. A major advantage of the
resulting T-Rex PCA is that no sparsity parameter tuning is required. Numerical
experiments and a stock market data example demonstrate a significant
performance improvement.","Jasin Machkour, Arnaud Breloy, Michael Muma, Daniel P. Palomar, Frédéric Pascal",2024,http://arxiv.org/abs/2401.08375v1
"TeachTune: Reviewing Pedagogical Agents Against Diverse Student Profiles
  with Simulated Students","Large language models (LLMs) can empower teachers to build pedagogical
conversational agents (PCAs) customized for their students. As students have
different prior knowledge and motivation levels, teachers must review the
adaptivity of their PCAs to diverse students. Existing chatbot reviewing
methods (e.g., direct chat and benchmarks) are either manually intensive for
multiple iterations or limited to testing only single-turn interactions. We
present TeachTune, where teachers can create simulated students and review PCAs
by observing automated chats between PCAs and simulated students. Our technical
pipeline instructs an LLM-based student to simulate prescribed knowledge levels
and traits, helping teachers explore diverse conversation patterns. Our
pipeline could produce simulated students whose behaviors correlate highly to
their input knowledge and motivation levels within 5% and 10% accuracy gaps.
Thirty science teachers designed PCAs in a between-subjects study, and using
TeachTune resulted in a lower task load and higher student profile coverage
over a baseline.","Hyoungwook Jin, Minju Yoo, Jeongeon Park, Yokyung Lee, Xu Wang, Juho Kim",2024,http://arxiv.org/abs/2410.04078v3
"Sparse Principal Component Analysis with Non-Oblivious Adversarial
  Perturbations","Sparse Principal Component Analysis (sparse PCA) is a fundamental
dimension-reduction tool that enhances interpretability in various
high-dimensional settings. An important variant of sparse PCA studies the
scenario when samples are adversarially perturbed. Notably, most existing
statistical studies on this variant focus on recovering the ground truth and
verifying the robustness of classical algorithms when the given samples are
corrupted under oblivious adversarial perturbations. In contrast, this paper
aims to find a robust sparse principal component that maximizes the variance of
the given samples corrupted by non-oblivious adversarial perturbations, say
sparse PCA with Non-Oblivious Adversarial Perturbations (sparse PCA-NOAP).
Specifically, we introduce a general formulation for the proposed sparse
PCA-NOAP. We then derive Mixed-Integer Programming (MIP) reformulations to
upper bound it with provable worst-case guarantees when adversarial
perturbations are controlled by two typical norms, i.e., $\ell_{2 \rightarrow
\infty}$-norm (sample-wise $\ell_2$-norm perturbation) and $\ell_{1 \rightarrow
2}$-norm (feature-wise $\ell_2$-norm perturbation). Moreover, when samples are
drawn from the spiked Wishart model, we show that the proposed MIP
reformulations ensure vector recovery properties under a more general parameter
region compared with existing results. Numerical simulations are also provided
to validate the theoretical findings and demonstrate the accuracy of the
proposed formulations.","Yuqing He, Guanyi Wang, Yu Yang",2024,http://arxiv.org/abs/2411.05332v1
"Anomaly Detection in California Electricity Price Forecasting: Enhancing
  Accuracy and Reliability Using Principal Component Analysis","Accurate and reliable electricity price forecasting has significant practical
implications for grid management, renewable energy integration, power system
planning, and price volatility management. This study focuses on enhancing
electricity price forecasting in California's grid, addressing challenges from
complex generation data and heteroskedasticity. Utilizing principal component
analysis (PCA), we analyze CAISO's hourly electricity prices and demand from
2016-2021 to improve day-ahead forecasting accuracy. Initially, we apply
traditional outlier analysis with the interquartile range method, followed by
robust PCA (RPCA) for more effective outlier elimination. This approach
improves data symmetry and reduces skewness. We then construct multiple linear
regression models using both raw and PCA-transformed features. The model with
transformed features, refined through traditional and SAS Sparse Matrix outlier
removal methods, shows superior forecasting performance. The SAS Sparse Matrix
method, in particular, significantly enhances model accuracy. Our findings
demonstrate that PCA-based methods are key in advancing electricity price
forecasting, supporting renewable integration and grid management in day-ahead
markets.
  Keywords: Electricity price forecasting, principal component analysis (PCA),
power system planning, heteroskedasticity, renewable energy integration.","Joseph Nyangon, Ruth Akintunde",2024,http://arxiv.org/abs/2412.07787v1
Achieving Fair PCA Using Joint Eigenvalue Decomposition,"Principal Component Analysis (PCA) is a widely used method for dimensionality
reduction, but it often overlooks fairness, especially when working with data
that includes demographic characteristics. This can lead to biased
representations that disproportionately affect certain groups. To address this
issue, our approach incorporates Joint Eigenvalue Decomposition (JEVD), a
technique that enables the simultaneous diagonalization of multiple matrices,
ensuring fair and efficient representations. We formally show that the optimal
solution of JEVD leads to a fair PCA solution. By integrating JEVD with PCA, we
strike an optimal balance between preserving data structure and promoting
fairness across diverse groups. We demonstrate that our method outperforms
existing baseline approaches in fairness and representational quality on
various datasets. It retains the core advantages of PCA while ensuring that
sensitive demographic attributes do not create disparities in the reduced
representation.","Vidhi Rathore, Naresh Manwani",2025,http://arxiv.org/abs/2502.16933v1
Hidden Convexity of Fair PCA and Fast Solver via Eigenvalue Optimization,"Principal Component Analysis (PCA) is a foundational technique in machine
learning for dimensionality reduction of high-dimensional datasets. However,
PCA could lead to biased outcomes that disadvantage certain subgroups of the
underlying datasets. To address the bias issue, a Fair PCA (FPCA) model was
introduced by Samadi et al. (2018) for equalizing the reconstruction loss
between subgroups. The semidefinite relaxation (SDR) based approach proposed by
Samadi et al. (2018) is computationally expensive even for suboptimal
solutions. To improve efficiency, several alternative variants of the FPCA
model have been developed. These variants often shift the focus away from
equalizing the reconstruction loss. In this paper, we identify a hidden
convexity in the FPCA model and introduce an algorithm for convex optimization
via eigenvalue optimization. Our approach achieves the desired fairness in
reconstruction loss without sacrificing performance. As demonstrated in
real-world datasets, the proposed FPCA algorithm runs $8\times$ faster than the
SDR-based algorithm, and only at most 85% slower than the standard PCA.","Junhui Shen, Aaron J. Davis, Ding Lu, Zhaojun Bai",2025,http://arxiv.org/abs/2503.00299v1
"Principal component analysis based clustering for high-dimension,
  low-sample-size data","In this paper, we consider clustering based on principal component analysis
(PCA) for high-dimension, low-sample-size (HDLSS) data. We give theoretical
reasons why PCA is effective for clustering HDLSS data. First, we derive a
geometric representation of HDLSS data taken from a two-class mixture model.
With the help of the geometric representation, we give geometric consistency
properties of sample principal component scores in the HDLSS context. We
develop ideas of the geometric representation and geometric consistency
properties to multiclass mixture models. We show that PCA can classify HDLSS
data under certain conditions in a surprisingly explicit way. Finally, we
demonstrate the performance of the clustering by using microarray data sets.","Kazuyoshi Yata, Makoto Aoshima",2015,http://arxiv.org/abs/1503.04525v1
On the Worst-Case Approximability of Sparse PCA,"It is well known that Sparse PCA (Sparse Principal Component Analysis) is
NP-hard to solve exactly on worst-case instances. What is the complexity of
solving Sparse PCA approximately? Our contributions include: 1) a simple and
efficient algorithm that achieves an $n^{-1/3}$-approximation; 2) NP-hardness
of approximation to within $(1-\varepsilon)$, for some small constant
$\varepsilon > 0$; 3) SSE-hardness of approximation to within any constant
factor; and 4) an $\exp\exp\left(\Omega\left(\sqrt{\log \log n}\right)\right)$
(""quasi-quasi-polynomial"") gap for the standard semidefinite program.","Siu On Chan, Dimitris Papailiopoulos, Aviad Rubinstein",2015,http://arxiv.org/abs/1507.05950v1
"RELARM: A rating model based on relative PCA attributes and k-means
  clustering","Following widely used in visual recognition concept of relative attributes,
the article establishes definition of the relative PCA attributes for a class
of objects defined by vectors of their parameters. A new rating model (RELARM)
is built using relative PCA attribute ranking functions for rating object
description and k-means clustering algorithm. Rating assignment of each rating
object to a rating category is derived as a result of cluster centers
projection on the specially selected rating vector. Empirical study has shown a
high level of approximation to the existing S & P, Moody's and Fitch ratings.",Elnura Irmatova,2016,http://arxiv.org/abs/1608.06416v1
"Application of Principal Component Analysis to establish proper basis
  for flow studies in heavy-ion collisions","It is shown that Principal Component Analysis (PCA) applied to event-by-event
single-particle distributions in A-A collisions allows establishing the most
optimal basis for anisotropic flow studies from data itself, in contrast to
manual selection of the basis functions. PCA coefficients for azimuthal
particle distributions are identical to Fourier coefficients from a
conventional analysis techniques. PCA applied in longitudinal dimension reveals
optimal basis that is similar to Legendre polynomial series. Analysis in both
dimensions simultaneously allows studying the coupling of the longitudinal
structure of events with the azimuthal anisotropy of particle emission.",Igor Altsybeev,2019,http://arxiv.org/abs/1909.03979v1
Hierarchical PCA and Applications to Portfolio Management,"It is widely known that the common risk-factors derived from PCA beyond the
first eigenportfolio are generally difficult to interpret and thus to use in
practical portfolio management. We explore a alternative approach (HPCA) which
makes strong use of the partition of the market into sectors. We show that this
approach leads to no loss of information with respect to PCA in the case of
equities (constituents of the S&P 500) and also that the associated common
factors admit simple interpretations. The model can also be used in markets in
which the sectors have asynchronous price information, such as single-name
credit default swaps, generalizing the works of Cont and Kan (2011) and Ivanov
(2016).",Marco Avellaneda,2019,http://arxiv.org/abs/1910.02310v1
Optimal principal component Analysis of STEM XEDS spectrum images,"STEM XEDS spectrum images can be drastically denoised by application of the
principal component analysis (PCA). This paper looks inside the PCA workflow
step by step on an example of a complex semiconductor structure consisting of a
number of different phases. Typical problems distorting the principal
components decomposition are highlighted and solutions for the successful PCA
are described. Particular attention is paid to the optimal truncation of
principal components in the course of reconstructing denoised data. A novel
accurate and robust method, which overperforms the existing truncation methods
is suggested for the first time and described in details.","Pavel Potapov, Axel Lubk",2019,http://arxiv.org/abs/1910.06781v1
A Generalization of Principal Component Analysis,"Conventional principal component analysis (PCA) finds a principal vector that
maximizes the sum of second powers of principal components. We consider a
generalized PCA that aims at maximizing the sum of an arbitrary convex function
of principal components. We present a gradient ascent algorithm to solve the
problem. For the kernel version of generalized PCA, we show that the solutions
can be obtained as fixed points of a simple single-layer recurrent neural
network. We also evaluate our algorithms on different datasets.","Samuele Battaglino, Erdem Koyuncu",2019,http://arxiv.org/abs/1910.13511v2
"Multi-PCA based Fault Detection Model Combined with Prior knowledge of
  HVAC","The traditional PCA fault detection methods completely depend on the training
data. The prior knowledge such as the physical principle of the system has not
been taken into account. In this paper, we propose a new multi-PCA fault
detection model combined with prior knowledge. This new model can adapt to the
variable operating conditions of the central air conditioning system, and it
can detect small deviation faults of sensors and significantly shorten the time
delay of detecting drift faults. We also conducted enough ablation experiments
to demonstrate that our model is more robust and efficient.","Ziming Liu, Xiaobo Liu",2019,http://arxiv.org/abs/1911.13263v1
"Conservation Laws and Spin System Modeling through Principal Component
  Analysis","This paper examines several applications of principal component analysis
(PCA) to physical systems. The first of these demonstrates that the principal
components in a basis of appropriate system variables can be employed to
identify physically conserved quantities. That is, if the general form of a
physical symmetry law is known, the PCA can identify an algebraic expression
for the symmetry from the observed system trajectories. Secondly, the
eigenvalue spectrum of the principal component spectrum for homogeneous
periodic spin systems is found to reflect the geometric shape of the boundary.
Finally, the PCA is employed to generate synthetic spin realizations with
probability distributions in energy-magnetization space that closely resemble
that of the input realizations although statistical quantities are inaccurately
reproduced.",David Yevick,2020,http://arxiv.org/abs/2005.01613v1
Ordinal analysis of partial combinatory algebras,"For every partial combinatory algebra (pca), we define a hierarchy of
extensionality relations using ordinals. We investigate the closure ordinals of
pca's, i.e. the smallest ordinals where these relations become equal. We show
that the closure ordinal of Kleene's first model is $\omega_1^\textit{CK}$ and
that the closure ordinal of Kleene's second model is $\omega_1$. We calculate
the exact complexities of the extensionality relations in Kleene's first model,
showing that they exhaust the hyperarithmetical hierarchy. We also discuss
embeddings of pca's.","Paul Shafer, Sebastiaan A. Terwijn",2020,http://arxiv.org/abs/2010.12452v2
"Ergodicity versus non-ergodicity for Probabilistic Cellular Automata on
  rooted trees","In this article we study a class of shift-invariant and positive rate
probabilistic cellular automata (PCA) on rooted d-regular trees $\mathbb{T}^d$.
  In a first result we extend the results of [10] on trees, namely we prove
that to every stationary measure $\nu$ of the PCA we can associate a space-time
Gibbs measure $\mu_{\nu}$ on
  $\mathbb{Z} \times \mathbb{T}^d$. Under certain assumptions on the dynamics
the converse is also true.
  A second result concerns proving sufficient conditions for ergodicity and
non-ergodicity of our PCA on d-ary trees for $d\in \{ 1,2,3\}$ and
characterizing the invariant product Bernoulli measures.","Bruno Kimura, Wioletta Ruszel, Cristian Spitoni",2017,http://arxiv.org/abs/1710.00084v2
Iterated and exponentially weighted moving principal component analysis,"The principal component analysis (PCA) is a staple statistical and
unsupervised machine learning technique in finance. The application of PCA in a
financial setting is associated with several technical difficulties, such as
numerical instability and nonstationarity. We attempt to resolve them by
proposing two new variants of PCA: an iterated principal component analysis
(IPCA) and an exponentially weighted moving principal component analysis
(EWMPCA). Both variants rely on the Ogita-Aishima iteration as a crucial step.","Paul Bilokon, David Finkelstein",2021,http://arxiv.org/abs/2108.13072v1
Entrywise Recovery Guarantees for Sparse PCA via Sparsistent Algorithms,"Sparse Principal Component Analysis (PCA) is a prevalent tool across a
plethora of subfields of applied statistics. While several results have
characterized the recovery error of the principal eigenvectors, these are
typically in spectral or Frobenius norms. In this paper, we provide entrywise
$\ell_{2,\infty}$ bounds for Sparse PCA under a general high-dimensional
subgaussian design. In particular, our results hold for any algorithm that
selects the correct support with high probability, those that are sparsistent.
Our bound improves upon known results by providing a finer characterization of
the estimation error, and our proof uses techniques recently developed for
entrywise subspace perturbation theory.","Joshua Agterberg, Jeremias Sulam",2022,http://arxiv.org/abs/2202.04061v1
Distributed Robust Principal Component Analysis,"We study the robust principal component analysis (RPCA) problem in a
distributed setting. The goal of RPCA is to find an underlying low-rank
estimation for a raw data matrix when the data matrix is subject to the
corruption of gross sparse errors. Previous studies have developed RPCA
algorithms that provide stable solutions with fast convergence. However, these
algorithms are typically hard to scale and cannot be implemented distributedly,
due to the use of either SVD or large matrix multiplication. In this paper, we
propose the first distributed robust principal analysis algorithm based on
consensus factorization, dubbed DCF-PCA. We prove the convergence of DCF-PCA
and evaluate DCF-PCA on various problem setting",Wenda Chu,2022,http://arxiv.org/abs/2207.11669v2
"Utility of PCA and Other Data Transformation Techniques in Exoplanet
  Research","This paper focuses on the utility of various data transformation techniques,
which might be under the principal component analysis (PCA) category, on
exoplanet research. The first section introduces the methodological background
of PCA and related techniques. The second section reviews the studies which
utilized these techniques in the exoplanet research field and compiles the
focuses in the literature under different items in the overview, with future
research direction recommendations at the end.",Güray Hatipoğlu,2022,http://arxiv.org/abs/2211.14683v1
Completions of Kleene's second model,"We investigate completions of partial combinatory algebras (pcas), in
particular of Kleene's second model $\mathcal{K}_2$ and generalizations
thereof. We consider weak and strong notions of embeddability and completion
that have been studied before in the literature. It is known that every
countable pca can be weakly embedded into $\mathcal{K}_2$, and we generalize
this to arbitrary cardinalities by considering generalizations of
$\mathcal{K}_2$ for larger cardinals. This emphasizes the central role of
$\mathcal{K}_2$ in the study of pcas. We also show that $\mathcal{K}_2$ and its
generalizations have strong completions.",Sebastiaan A. Terwijn,2023,http://arxiv.org/abs/2312.14656v6
"Regional House Price Dynamics in Australia: Insights into Lifestyle and
  Mining Dynamics through PCA","This report applies Principal Component Analysis (PCA) to regional house
price indexes to uncover dominant trends in Australia's housing market. Regions
are assigned PCA-derived scores that reveal which underlying market forces are
most influential in each area, enabling broad classification of local housing
markets. The approach highlights where price movements tend to align across
regions, even those geographically distant. The three most dominant trends are
described in detail and, together with the regional scores, provide objective
tools for policymakers, researchers, and real estate professionals.",Willem Sijp,2025,http://arxiv.org/abs/2503.18332v1
Phase Transitions for High Dimensional Clustering and Related Problems,"Consider a two-class clustering problem where we observe $X_i = \ell_i \mu +
Z_i$, $Z_i \stackrel{iid}{\sim} N(0, I_p)$, $1 \leq i \leq n$. The feature
vector $\mu\in R^p$ is unknown but is presumably sparse. The class labels
$\ell_i\in\{-1, 1\}$ are also unknown and the main interest is to estimate
them.
  We are interested in the statistical limits. In the two-dimensional phase
space calibrating the rarity and strengths of useful features, we find the
precise demarcation for the Region of Impossibility and Region of Possibility.
In the former, useful features are too rare/weak for successful clustering. In
the latter, useful features are strong enough to allow successful clustering.
The results are extended to the case of colored noise using Le Cam's idea on
comparison of experiments.
  We also extend the study on statistical limits for clustering to that for
signal recovery and that for hypothesis testing. We compare the statistical
limits for three problems and expose some interesting insight.
  We propose classical PCA and Important Features PCA (IF-PCA) for clustering.
For a threshold $t > 0$, IF-PCA clusters by applying classical PCA to all
columns of $X$ with an $L^2$-norm larger than $t$. We also propose two
aggregation methods. For any parameter in the Region of Possibility, some of
these methods yield successful clustering. We find an interesting phase
transition for IF-PCA.
  Our results require delicate analysis, especially on post-selection Random
Matrix Theory and on lower bound arguments.","Jiashun Jin, Zheng Tracy Ke, Wanjie Wang",2015,http://arxiv.org/abs/1502.06952v4
Bayesian Variable Selection for Globally Sparse Probabilistic PCA,"Sparse versions of principal component analysis (PCA) have imposed themselves
as simple, yet powerful ways of selecting relevant features of high-dimensional
data in an unsupervised manner. However, when several sparse principal
components are computed, the interpretation of the selected variables is
difficult since each axis has its own sparsity pattern and has to be
interpreted separately. To overcome this drawback, we propose a Bayesian
procedure called globally sparse probabilistic PCA (GSPPCA) that allows to
obtain several sparse components with the same sparsity pattern. This allows
the practitioner to identify the original variables which are relevant to
describe the data. To this end, using Roweis' probabilistic interpretation of
PCA and a Gaussian prior on the loading matrix, we provide the first exact
computation of the marginal likelihood of a Bayesian PCA model. To avoid the
drawbacks of discrete model selection, a simple relaxation of this framework is
presented. It allows to find a path of models using a variational
expectation-maximization algorithm. The exact marginal likelihood is then
maximized over this path. This approach is illustrated on real and synthetic
data sets. In particular, using unlabeled microarray data, GSPPCA infers much
more relevant gene subsets than traditional sparse PCA algorithms.","Charles Bouveyron, Pierre Latouche, Pierre-Alexandre Mattei",2016,http://arxiv.org/abs/1605.05918v2
Neural Component Analysis for Fault Detection,"Principal component analysis (PCA) is largely adopted for chemical process
monitoring and numerous PCA-based systems have been developed to solve various
fault detection and diagnosis problems. Since PCA-based methods assume that the
monitored process is linear, nonlinear PCA models, such as autoencoder models
and kernel principal component analysis (KPCA), has been proposed and applied
to nonlinear process monitoring. However, KPCA-based methods need to perform
eigen-decomposition (ED) on the kernel Gram matrix whose dimensions depend on
the number of training data. Moreover, prefixed kernel parameters cannot be
most effective for different faults which may need different parameters to
maximize their respective detection performances. Autoencoder models lack the
consideration of orthogonal constraints which is crucial for PCA-based
algorithms. To address these problems, this paper proposes a novel nonlinear
method, called neural component analysis (NCA), which intends to train a
feedforward neural work with orthogonal constraints such as those used in PCA.
NCA can adaptively learn its parameters through backpropagation and the
dimensionality of the nonlinear features has no relationship with the number of
training samples. Extensive experimental results on the Tennessee Eastman (TE)
benchmark process show the superiority of NCA in terms of missed detection rate
(MDR) and false alarm rate (FAR). The source code of NCA can be found in
https://github.com/haitaozhao/Neural-Component-Analysis.git.",Haitao Zhao,2017,http://arxiv.org/abs/1712.04118v1
"Extension of PCA to Higher Order Data Structures: An Introduction to
  Tensors, Tensor Decompositions, and Tensor PCA","The widespread use of multisensor technology and the emergence of big data
sets have brought the necessity to develop more versatile tools to represent
higher-order data with multiple aspects and high dimensionality. Data in the
form of multidimensional arrays, also referred to as tensors, arises in a
variety of applications including chemometrics, hyperspectral imaging, high
resolution videos, neuroimaging, biometrics, and social network analysis. Early
multiway data analysis approaches reformatted such tensor data as large vectors
or matrices and then resorted to dimensionality reduction methods developed for
classical two-way analysis such as PCA. However, one cannot discover hidden
components within multiway data using conventional PCA. To this end, tensor
decomposition methods which are flexible in the choice of the constraints and
that extract more general latent components have been proposed. In this paper,
we review the major tensor decomposition methods with a focus on problems
targeted by classical PCA. In particular, we present tensor methods that aim to
solve three important challenges typically addressed by PCA: dimensionality
reduction, i.e. low-rank tensor approximation, supervised learning, i.e.
learning linear subspaces for feature extraction, and robust low-rank tensor
recovery. We also provide experimental results to compare different tensor
models for both dimensionality reduction and supervised learning applications.","Ali Zare, Alp Ozdemir, Mark A. Iwen, Selin Aviyente",2018,http://arxiv.org/abs/1803.00704v2
"SuperPCA: A Superpixelwise PCA Approach for Unsupervised Feature
  Extraction of Hyperspectral Imagery","As an unsupervised dimensionality reduction method, principal component
analysis (PCA) has been widely considered as an efficient and effective
preprocessing step for hyperspectral image (HSI) processing and analysis tasks.
It takes each band as a whole and globally extracts the most representative
bands. However, different homogeneous regions correspond to different objects,
whose spectral features are diverse. It is obviously inappropriate to carry out
dimensionality reduction through a unified projection for an entire HSI. In
this paper, a simple but very effective superpixelwise PCA approach, called
SuperPCA, is proposed to learn the intrinsic low-dimensional features of HSIs.
In contrast to classical PCA models, SuperPCA has four main properties. (1)
Unlike the traditional PCA method based on a whole image, SuperPCA takes into
account the diversity in different homogeneous regions, that is, different
regions should have different projections. (2) Most of the conventional feature
extraction models cannot directly use the spatial information of HSIs, while
SuperPCA is able to incorporate the spatial context information into the
unsupervised dimensionality reduction by superpixel segmentation. (3) Since the
regions obtained by superpixel segmentation have homogeneity, SuperPCA can
extract potential low-dimensional features even under noise. (4) Although
SuperPCA is an unsupervised method, it can achieve competitive performance when
compared with supervised approaches. The resulting features are discriminative,
compact, and noise resistant, leading to improved HSI classification
performance. Experiments on three public datasets demonstrate that the SuperPCA
model significantly outperforms the conventional PCA based dimensionality
reduction baselines for HSI classification. The Matlab source code is available
at https://github.com/junjun-jiang/SuperPCA","Junjun Jiang, Jiayi Ma, Chen Chen, Zhongyuan Wang, Zhihua Cai, Lizhe Wang",2018,http://arxiv.org/abs/1806.09807v2
"Shaping the spectrum of terahertz photoconductive antenna by
  frequency-dependent impedance modulation","In this paper, we report on an approach for shaping the spectra of THz pulse
generation in photoconductive antennas (PCAs) by frequency-dependent impedance
modulation. We introduce a theoretical model describing the THz pulse
generation in PCAs and accounting for impedances of the photoconductor and of
the antenna. In order to showcase an impact of frequency-dependent impedance
modulation on the spectra of THz pulse generation, we applied this model to
simulating broadband PCAs with log-spiral topology. Finally, we fabricated two
different log-spiral PCAs and characterized them experimentally using the THz
pulsed spectroscopy. The observed results demonstrate excellent agreement
between the theoretical model and experiment, justifying a potential of shaping
the spectra of THz pulse generation in PCA by modulation of frequency-dependent
impedances. This approach makes possible optimizing the PCA performance and
thus accommodating the needs of THz pulsed spectroscopy and imaging in
fundamental and applied branches of THz science and technologies.","D. V. Lavrukhin, A. E. Yachmenev, A. Yu. Pavlov, R. A. Khabibullin, Yu. G. Goncharov, I. E. Spektor, G. A. Komandin, S. O. Yurchenko, N. V. Chernomyrdin, K. I. Zaytsev, D. S. Ponomarev",2018,http://arxiv.org/abs/1808.06592v1
"XPCA: Extending PCA for a Combination of Discrete and Continuous
  Variables","Principal component analysis (PCA) is arguably the most popular tool in
multivariate exploratory data analysis. In this paper, we consider the question
of how to handle heterogeneous variables that include continuous, binary, and
ordinal. In the probabilistic interpretation of low-rank PCA, the data has a
normal multivariate distribution and, therefore, normal marginal distributions
for each column. If some marginals are continuous but not normal, the
semiparametric copula-based principal component analysis (COCA) method is an
alternative to PCA that combines a Gaussian copula with nonparametric
marginals. If some marginals are discrete or semi-continuous, we propose a new
extended PCA (XPCA) method that also uses a Gaussian copula and nonparametric
marginals and accounts for discrete variables in the likelihood calculation by
integrating over appropriate intervals. Like PCA, the factors produced by XPCA
can be used to find latent structure in data, build predictive models, and
perform dimensionality reduction. We present the new model, its induced
likelihood function, and a fitting algorithm which can be applied in the
presence of missing data. We demonstrate how to use XPCA to produce an
estimated full conditional distribution for each data point, and use this to
produce to provide estimates for missing data that are automatically range
respecting. We compare the methods as applied to simulated and real-world data
sets that have a mixture of discrete and continuous variables.","Clifford Anderson-Bergman, Tamara G. Kolda, Kina Kincher-Winoto",2018,http://arxiv.org/abs/1808.07510v1
Modeling baryonic physics in future weak lensing surveys,"Modifications of the matter power spectrum due to baryonic physics are one of
the major theoretical uncertainties in cosmological weak lensing measurements.
Developing robust mitigation schemes for this source of systematic uncertainty
increases the robustness of cosmological constraints, and may increase their
precision if they enable the use of information from smaller scales. Here we
explore the performance of two mitigation schemes for baryonic effects in weak
lensing cosmic shear: the PCA method and the halo-model approach in
\textsc{HMcode}. We construct mock tomographic shear power spectra from four
hydrodynamical simulations, and run simulated likelihood analyses with
\textsc{CosmoLike} assuming LSST-like survey statistics. With an angular scale
cut of $\ell_{\rm max}<2000$, both methods successfully remove the biases in
cosmological parameters due to the various baryonic physics scenarios, with the
PCA method causing less degradation in the parameter constraints than
\textsc{HMcode}. For a more aggressive $\ell_{\rm max}=$5000, the PCA method
performs well for all but one baryonic physics scenario, requiring additional
training simulations to account for the extreme baryonic physics scenario of
Illustris; \textsc{HMcode} exhibits tensions in the 2D posterior distributions
of cosmological parameters due to lack of freedom in describing the power
spectrum for $k > 10\ h^{-1}\mathrm{Mpc}$. We investigate variants of the PCA
method and improve the bias mitigation through PCA by accounting for the noise
properties in the data via Cholesky decomposition of the covariance matrix. Our
improved PCA method allows us to retain more statistical constraining power
while effectively mitigating baryonic uncertainties even for a broad range of
baryonic physics scenarios.","Hung-Jin Huang, Tim Eifler, Rachel Mandelbaum, Scott Dodelson",2018,http://arxiv.org/abs/1809.01146v2
Diagonally-Dominant Principal Component Analysis,"We consider the problem of decomposing a large covariance matrix into the sum
of a low-rank matrix and a diagonally dominant matrix, and we call this problem
the ""Diagonally-Dominant Principal Component Analysis (DD-PCA)"". DD-PCA is an
effective tool for designing statistical methods for strongly correlated data.
We showcase the use of DD-PCA in two statistical problems: covariance matrix
estimation, and global detection in multiple testing. Using the output of
DD-PCA, we propose a new estimator for estimating a large covariance matrix
with factor structure. Thanks to a nice property of diagonally dominant
matrices, this estimator enjoys the advantage of simultaneous good estimation
of the covariance matrix and the precision matrix (by a plain inversion). A
plug-in of this estimator to linear discriminant analysis and portfolio
optimization yields appealing performance in real data. We also propose two new
tests for testing the global null hypothesis in multiple testing when the
$z$-scores have a factor covariance structure. Both tests first use DD-PCA to
adjust the individual $p$-values and then plug in the adjusted $p$-values to
the Higher Criticism (HC) test. These new tests significantly improve over the
HC test and compare favorably with other existing tests. For computation of
DD-PCA, we propose an iterative projection algorithm and an ADMM algorithm.","Zheng Tracy Ke, Lingzhou Xue, Fan Yang",2019,http://arxiv.org/abs/1906.00051v1
Roweis Discriminant Analysis: A Generalized Subspace Learning Method,"We present a new method which generalizes subspace learning based on
eigenvalue and generalized eigenvalue problems. This method, Roweis
Discriminant Analysis (RDA), is named after Sam Roweis to whom the field of
subspace learning owes significantly. RDA is a family of infinite number of
algorithms where Principal Component Analysis (PCA), Supervised PCA (SPCA), and
Fisher Discriminant Analysis (FDA) are special cases. One of the extreme
special cases, which we name Double Supervised Discriminant Analysis (DSDA),
uses the labels twice; it is novel and has not appeared elsewhere. We propose a
dual for RDA for some special cases. We also propose kernel RDA, generalizing
kernel PCA, kernel SPCA, and kernel FDA, using both dual RDA and representation
theory. Our theoretical analysis explains previously known facts such as why
SPCA can use regression but FDA cannot, why PCA and SPCA have duals but FDA
does not, why kernel PCA and kernel SPCA use kernel trick but kernel FDA does
not, and why PCA is the best linear method for reconstruction. Roweisfaces and
kernel Roweisfaces are also proposed generalizing eigenfaces, Fisherfaces,
supervised eigenfaces, and their kernel variants. We also report experiments
showing the effectiveness of RDA and kernel RDA on some benchmark datasets.","Benyamin Ghojogh, Fakhri Karray, Mark Crowley",2019,http://arxiv.org/abs/1910.05437v1
"Schrödinger PCA: On the Duality between Principal Component Analysis
  and Schrödinger Equation","Principal component analysis (PCA) has achieved great success in unsupervised
learning by identifying covariance correlations among features. If the data
collection fails to capture the covariance information, PCA will not be able to
discover meaningful modes. In particular, PCA will fail the spatial Gaussian
Process (GP) model in the undersampling regime, i.e. the averaged distance of
neighboring anchor points (spatial features) is greater than the correlation
length of GP. Counterintuitively, by drawing the connection between PCA and
Schr\""odinger equation, we can not only attack the undersampling challenge but
also compute in an efficient and decoupled way with the proposed algorithm
called Schr\""odinger PCA. Our algorithm only requires variances of features and
estimated correlation length as input, constructs the corresponding
Schr\""odinger equation, and solves it to obtain the energy eigenstates, which
coincide with principal components. We will also establish the connection of
our algorithm to the model reduction techniques in the partial differential
equation (PDE) community, where the steady-state Schr\""odinger operator is
identified as a second-order approximation to the covariance function.
Numerical experiments are implemented to testify the validity and efficiency of
the proposed algorithm, showing its potential for unsupervised learning tasks
on general graphs and manifolds.","Ziming Liu, Sitian Qian, Yixuan Wang, Yuxuan Yan, Tianyi Yang",2020,http://arxiv.org/abs/2006.04379v2
"Adversarial Attacks against Neural Networks in Audio Domain: Exploiting
  Principal Components","Adversarial attacks are inputs that are similar to original inputs but
altered on purpose. Speech-to-text neural networks that are widely used today
are prone to misclassify adversarial attacks. In this study, first, we
investigate the presence of targeted adversarial attacks by altering wave forms
from Common Voice data set. We craft adversarial wave forms via Connectionist
Temporal Classification Loss Function, and attack DeepSpeech, a speech-to-text
neural network implemented by Mozilla. We achieve 100% adversarial success rate
(zero successful classification by DeepSpeech) on all 25 adversarial wave forms
that we crafted. Second, we investigate the use of PCA as a defense mechanism
against adversarial attacks. We reduce dimensionality by applying PCA to these
25 attacks that we created and test them against DeepSpeech. We observe zero
successful classification by DeepSpeech, which suggests PCA is not a good
defense mechanism in audio domain. Finally, instead of using PCA as a defense
mechanism, we use PCA this time to craft adversarial inputs under a black-box
setting with minimal adversarial knowledge. With no knowledge regarding the
model, parameters, or weights, we craft adversarial attacks by applying PCA to
samples from Common Voice data set and achieve 100% adversarial success under
black-box setting again when tested against DeepSpeech. We also experiment with
different percentage of components necessary to result in a classification
during attacking process. In all cases, adversary becomes successful.","Ken Alparslan, Yigit Alparslan, Matthew Burlick",2020,http://arxiv.org/abs/2007.07001v3
"A Linearly Convergent Algorithm for Distributed Principal Component
  Analysis","Principal Component Analysis (PCA) is the workhorse tool for dimensionality
reduction in this era of big data. While often overlooked, the purpose of PCA
is not only to reduce data dimensionality, but also to yield features that are
uncorrelated. Furthermore, the ever-increasing volume of data in the modern
world often requires storage of data samples across multiple machines, which
precludes the use of centralized PCA algorithms. This paper focuses on the dual
objective of PCA, namely, dimensionality reduction and decorrelation of
features, but in a distributed setting. This requires estimating the
eigenvectors of the data covariance matrix, as opposed to only estimating the
subspace spanned by the eigenvectors, when data is distributed across a network
of machines. Although a few distributed solutions to the PCA problem have been
proposed recently, convergence guarantees and/or communications overhead of
these solutions remain a concern. With an eye towards communications
efficiency, this paper introduces a feedforward neural network-based one
time-scale distributed PCA algorithm termed Distributed Sanger's Algorithm
(DSA) that estimates the eigenvectors of the data covariance matrix when data
is distributed across an undirected and arbitrarily connected network of
machines. Furthermore, the proposed algorithm is shown to converge linearly to
a neighborhood of the true solution. Numerical results are also provided to
demonstrate the efficacy of the proposed solution.","Arpita Gang, Waheed U. Bajwa",2021,http://arxiv.org/abs/2101.01300v4
HePPCAT: Probabilistic PCA for Data with Heteroscedastic Noise,"Principal component analysis (PCA) is a classical and ubiquitous method for
reducing data dimensionality, but it is suboptimal for heterogeneous data that
are increasingly common in modern applications. PCA treats all samples
uniformly so degrades when the noise is heteroscedastic across samples, as
occurs, e.g., when samples come from sources of heterogeneous quality. This
paper develops a probabilistic PCA variant that estimates and accounts for this
heterogeneity by incorporating it in the statistical model. Unlike in the
homoscedastic setting, the resulting nonconvex optimization problem is not
seemingly solved by singular value decomposition. This paper develops a
heteroscedastic probabilistic PCA technique (HePPCAT) that uses efficient
alternating maximization algorithms to jointly estimate both the underlying
factors and the unknown noise variances. Simulation experiments illustrate the
comparative speed of the algorithms, the benefit of accounting for
heteroscedasticity, and the seemingly favorable optimization landscape of this
problem. Real data experiments on environmental air quality data show that
HePPCAT can give a better PCA estimate than techniques that do not account for
heteroscedasticity.","David Hong, Kyle Gilman, Laura Balzano, Jeffrey A. Fessler",2021,http://arxiv.org/abs/2101.03468v3
The Complexity of Sparse Tensor PCA,"We study the problem of sparse tensor principal component analysis: given a
tensor $\pmb Y = \pmb W + \lambda x^{\otimes p}$ with $\pmb W \in
\otimes^p\mathbb{R}^n$ having i.i.d. Gaussian entries, the goal is to recover
the $k$-sparse unit vector $x \in \mathbb{R}^n$. The model captures both sparse
PCA (in its Wigner form) and tensor PCA.
  For the highly sparse regime of $k \leq \sqrt{n}$, we present a family of
algorithms that smoothly interpolates between a simple polynomial-time
algorithm and the exponential-time exhaustive search algorithm. For any $1 \leq
t \leq k$, our algorithms recovers the sparse vector for signal-to-noise ratio
$\lambda \geq \tilde{\mathcal{O}} (\sqrt{t} \cdot (k/t)^{p/2})$ in time
$\tilde{\mathcal{O}}(n^{p+t})$, capturing the state-of-the-art guarantees for
the matrix settings (in both the polynomial-time and sub-exponential time
regimes).
  Our results naturally extend to the case of $r$ distinct $k$-sparse signals
with disjoint supports, with guarantees that are independent of the number of
spikes. Even in the restricted case of sparse PCA, known algorithms only
recover the sparse vectors for $\lambda \geq \tilde{\mathcal{O}}(k \cdot r)$
while our algorithms require $\lambda \geq \tilde{\mathcal{O}}(k)$.
  Finally, by analyzing the low-degree likelihood ratio, we complement these
algorithmic results with rigorous evidence illustrating the trade-offs between
signal-to-noise ratio and running time. This lower bound captures the known
lower bounds for both sparse PCA and tensor PCA. In this general model, we
observe a more intricate three-way trade-off between the number of samples $n$,
the sparsity $k$, and the tensor power $p$.","Davin Choo, Tommaso d'Orsi",2021,http://arxiv.org/abs/2106.06308v2
"Statistical-Computational Trade-offs in Tensor PCA and Related Problems
  via Communication Complexity","Tensor PCA is a stylized statistical inference problem introduced by
Montanari and Richard to study the computational difficulty of estimating an
unknown parameter from higher-order moment tensors. Unlike its matrix
counterpart, Tensor PCA exhibits a statistical-computational gap, i.e., a
sample size regime where the problem is information-theoretically solvable but
conjectured to be computationally hard. This paper derives computational lower
bounds on the run-time of memory bounded algorithms for Tensor PCA using
communication complexity. These lower bounds specify a trade-off among the
number of passes through the data sample, the sample size, and the memory
required by any algorithm that successfully solves Tensor PCA. While the lower
bounds do not rule out polynomial-time algorithms, they do imply that many
commonly-used algorithms, such as gradient descent and power method, must have
a higher iteration count when the sample size is not large enough. Similar
lower bounds are obtained for Non-Gaussian Component Analysis, a family of
statistical estimation problems in which low-order moment tensors carry no
information about the unknown parameter. Finally, stronger lower bounds are
obtained for an asymmetric variant of Tensor PCA and related statistical
estimation problems. These results explain why many estimators for these
problems use a memory state that is significantly larger than the effective
dimensionality of the parameter of interest.","Rishabh Dudeja, Daniel Hsu",2022,http://arxiv.org/abs/2204.07526v2
"PCA-Boosted Autoencoders for Nonlinear Dimensionality Reduction in Low
  Data Regimes","Autoencoders (AE) provide a useful method for nonlinear dimensionality
reduction but are ill-suited for low data regimes. Conversely, Principal
Component Analysis (PCA) is data-efficient but is limited to linear
dimensionality reduction, posing a problem when data exhibits inherent
nonlinearity. This presents a challenge in various scientific and engineering
domains such as the nanophotonic component design, where data exhibits
nonlinear features while being expensive to obtain due to costly real
measurements or resource-consuming solutions of partial differential equations.
  To address this difficulty, we propose a technique that harnesses the best of
both worlds: an autoencoder that leverages PCA to perform well on scarce
nonlinear data. Specifically, we outline a numerically robust PCA-based
initialization of AE, which, together with the parameterized ReLU activation
function, allows the training process to start from an exact PCA solution and
improve upon it. A synthetic example is presented first to study the effects of
data nonlinearity and size on the performance of the proposed method. We then
evaluate our method on several nanophotonic component design problems where
obtaining useful data is expensive. To demonstrate universality, we also apply
it to tasks in other scientific domains: a benchmark breast cancer dataset and
a gene expression dataset.
  We show that our proposed approach is substantially better than both PCA and
randomly initialized AE in the majority of low-data regime cases we consider,
or at least is comparable to the best of either of the other two methods.","Muhammad Al-Digeil, Yuri Grinberg, Daniele Melati3, Mohsen Kamandar Dezfouli, Jens H. Schmid, Pavel Cheben, Siegfried Janz, Dan-Xia Xu",2022,http://arxiv.org/abs/2205.11673v1
"MP-PCA denoising of fMRI time-series data can lead to artificial
  activation ""spreading""","MP-PCA denoising has become the method of choice for denoising in MRI since
it provides an objective threshold to separate the desired signal from unwanted
thermal noise components. In rodents, thermal noise in the coils is an
important source of noise that can reduce the accuracy of activation mapping in
fMRI. Further confounding this problem, vendor data often contains zero-filling
and other effects that may violate MP-PCA assumptions. Here, we develop an
approach to denoise vendor data and assess activation ""spreading"" caused by
MP-PCA denoising in rodent task-based fMRI data. Data was obtained from N = 3
mice using conventional multislice and ultrafast acquisitions (1 s and 50 ms
temporal resolution, respectively), during visual stimulation. MP-PCA denoising
produced SNR gains of 64% and 39% and Fourier spectral amplitude (FSA)
increases in BOLD maps of 9% and 7% for multislice and ultrafast data,
respectively, when using a small [2 2] denoising window. Larger windows
provided higher SNR and FSA gains with increased spatial extent of activation
that may or may not represent real activation. Simulations showed that MP-PCA
denoising causes activation ""spreading"" with an increase in false positive rate
and smoother functional maps due to local ""bleeding"" of principal components,
and that the optimal denoising window for improved specificity of functional
mapping, based on Dice score calculations, depends on the data's tSNR and
functional CNR. This ""spreading"" effect applies also to another recently
proposed low-rank denoising method (NORDIC). Our results bode well for
dramatically enhancing spatial and/or temporal resolution in future fMRI work,
while taking into account the sensitivity/specificity trade-offs of low-rank
denoising methods.","Francisca F. Fernandes, Jonas L. Olesen, Sune N. Jespersen, Noam Shemesh",2022,http://arxiv.org/abs/2211.15401v1
"Federated PCA on Grassmann Manifold for Anomaly Detection in IoT
  Networks","In the era of Internet of Things (IoT), network-wide anomaly detection is a
crucial part of monitoring IoT networks due to the inherent security
vulnerabilities of most IoT devices. Principal Components Analysis (PCA) has
been proposed to separate network traffics into two disjoint subspaces
corresponding to normal and malicious behaviors for anomaly detection. However,
the privacy concerns and limitations of devices' computing resources compromise
the practical effectiveness of PCA. We propose a federated PCA-based
Grassmannian optimization framework that coordinates IoT devices to aggregate a
joint profile of normal network behaviors for anomaly detection. First, we
introduce a privacy-preserving federated PCA framework to simultaneously
capture the profile of various IoT devices' traffic. Then, we investigate the
alternating direction method of multipliers gradient-based learning on the
Grassmann manifold to guarantee fast training and the absence of detecting
latency using limited computational resources. Empirical results on the NSL-KDD
dataset demonstrate that our method outperforms baseline approaches. Finally,
we show that the Grassmann manifold algorithm is highly adapted for IoT anomaly
detection, which permits drastically reducing the analysis time of the system.
To the best of our knowledge, this is the first federated PCA algorithm for
anomaly detection meeting the requirements of IoT networks.","Tung-Anh Nguyen, Jiayu He, Long Tan Le, Wei Bao, Nguyen H. Tran",2022,http://arxiv.org/abs/2212.12121v2
Adaptive and Scalable Compression of Multispectral Images using VVC,"The VVC codec is applied to the task of multispectral image (MSI) compression
using adaptive and scalable coding structures. In a 'plain' VVC approach,
concepts from picture-to-picture temporal prediction are employed for
decorrelation along the MSI's spectral dimension. The popular principle
component analysis (PCA) for spectral decorrelation is further evaluated in
combination with VVC intra-coding for spatial decorrelation. This approach is
referred to as PCA-VVC. A novel adaptive MSI compression algorithm, named
HPCLS, is introduced, that uses PCA and inter-prediction for spectral and VVC
intra-coding for spatial decorrelation. Further, a novel adaptive scalable
approach is proposed, that provides a separately decodable spectrally scaled
preview of the MSI in the compressed file. Information contained in the preview
is exploited in order to reduce the overall file size. All schemes are
evaluated on images from the ARAD HS data set containing outdoor scenes with a
high variety in brightness and color. We found that 'Plain' VVC is outperformed
by both PCA-VVC and HPCLS. HPCLS shows advantageous rate-distortion (RD)
behavior compared to PCA-VVC for reconstruction quality above 51dB PSNR. The
performance of the scalable approach is compared to the combination of an
independent RGB preview and one of HPCLS or PCA-VVC. The scalable approach
shows significant benefit especially at higher preview qualities.","Philipp Seltsam, Priyanka Das, Mathias Wien",2023,http://arxiv.org/abs/2301.04117v1
A Unified Framework for Fast Large-Scale Portfolio Optimization,"We introduce a unified framework for rapid, large-scale portfolio
optimization that incorporates both shrinkage and regularization techniques.
This framework addresses multiple objectives, including minimum variance,
mean-variance, and the maximum Sharpe ratio, and also adapts to various
portfolio weight constraints. For each optimization scenario, we detail the
translation into the corresponding quadratic programming (QP) problem and then
integrate these solutions into a new open-source Python library. Using 50 years
of return data from US mid to large-sized companies, and 33 distinct
firm-specific characteristics, we utilize our framework to assess the
out-of-sample monthly rebalanced portfolio performance of widely-adopted
covariance matrix estimators and factor models, examining both daily and
monthly returns. These estimators include the sample covariance matrix, linear
and nonlinear shrinkage estimators, and factor portfolios based on Asset
Pricing (AP) Trees, Principal Component Analysis (PCA), Risk Premium PCA
(RP-PCA), and Instrumented PCA (IPCA). Our findings emphasize that AP-Trees and
PCA-based factor models consistently outperform all other approaches in
out-of-sample portfolio performance. Finally, we develop new l1 and l2
regularizations of factor portfolio norms which not only elevate the portfolio
performance of AP-Trees and PCA-based factor models but they have a potential
to reduce an excessive turnover and transaction costs often associated with
these models.","Weichuan Deng, Pawel Polak, Abolfazl Safikhani, Ronakdilip Shah",2023,http://arxiv.org/abs/2303.12751v2
Operator learning with PCA-Net: upper and lower complexity bounds,"PCA-Net is a recently proposed neural operator architecture which combines
principal component analysis (PCA) with neural networks to approximate
operators between infinite-dimensional function spaces. The present work
develops approximation theory for this approach, improving and significantly
extending previous work in this direction: First, a novel universal
approximation result is derived, under minimal assumptions on the underlying
operator and the data-generating distribution. Then, two potential obstacles to
efficient operator learning with PCA-Net are identified, and made precise
through lower complexity bounds; the first relates to the complexity of the
output distribution, measured by a slow decay of the PCA eigenvalues. The other
obstacle relates to the inherent complexity of the space of operators between
infinite-dimensional input and output spaces, resulting in a rigorous and
quantifiable statement of a ""curse of parametric complexity"", an
infinite-dimensional analogue of the well-known curse of dimensionality
encountered in high-dimensional approximation problems. In addition to these
lower bounds, upper complexity bounds are finally derived. A suitable
smoothness criterion is shown to ensure an algebraic decay of the PCA
eigenvalues. Furthermore, it is shown that PCA-Net can overcome the general
curse for specific operators of interest, arising from the Darcy flow and the
Navier-Stokes equations.",Samuel Lanthaler,2023,http://arxiv.org/abs/2303.16317v5
"PCA, SVD, and Centering of Data","The research detailed in this paper scrutinizes Principal Component Analysis
(PCA), a seminal method employed in statistics and machine learning for the
purpose of reducing data dimensionality. Singular Value Decomposition (SVD) is
often employed as the primary means for computing PCA, a process that
indispensably includes the step of centering - the subtraction of the mean
location from the data set. In our study, we delve into a detailed exploration
of the influence of this critical yet often ignored or downplayed data
centering step. Our research meticulously investigates the conditions under
which two PCA embeddings, one derived from SVD with centering and the other
without, can be viewed as aligned. As part of this exploration, we analyze the
relationship between the first singular vector and the mean direction,
subsequently linking this observation to the congruity between two SVDs of
centered and uncentered matrices. Furthermore, we explore the potential
implications arising from the absence of centering in the context of performing
PCA via SVD from a spectral analysis standpoint. Our investigation emphasizes
the importance of a comprehensive understanding and acknowledgment of the
subtleties involved in the computation of PCA. As such, we believe this paper
offers a crucial contribution to the nuanced understanding of this foundational
statistical method and stands as a valuable addition to the academic literature
in the field of statistics.","Donggun Kim, Kisung You",2023,http://arxiv.org/abs/2307.15213v2
Fun with Flags: Robust Principal Directions via Flag Manifolds,"Principal component analysis (PCA), along with its extensions to manifolds
and outlier contaminated data, have been indispensable in computer vision and
machine learning. In this work, we present a unifying formalism for PCA and its
variants, and introduce a framework based on the flags of linear subspaces, ie
a hierarchy of nested linear subspaces of increasing dimension, which not only
allows for a common implementation but also yields novel variants, not explored
previously. We begin by generalizing traditional PCA methods that either
maximize variance or minimize reconstruction error. We expand these
interpretations to develop a wide array of new dimensionality reduction
algorithms by accounting for outliers and the data manifold. To devise a common
computational approach, we recast robust and dual forms of PCA as optimization
problems on flag manifolds. We then integrate tangent space approximations of
principal geodesic analysis (tangent-PCA) into this flag-based framework,
creating novel robust and dual geodesic PCA variations. The remarkable
flexibility offered by the 'flagification' introduced here enables even more
algorithmic variants identified by specific flag types. Last but not least, we
propose an effective convergent solver for these flag-formulations employing
the Stiefel manifold. Our empirical results on both real-world and synthetic
scenarios, demonstrate the superiority of our novel algorithms, especially in
terms of robustness to outliers on manifolds.","Nathan Mankovich, Gustau Camps-Valls, Tolga Birdal",2024,http://arxiv.org/abs/2401.04071v4
"The Multi-parameter Test of Gravitational Wave Dispersion with Principal
  Component Analysis","In this work, we consider a conventional test of gravitational wave (GW)
propagation which is based on the phenomenological parameterized dispersion
relation to describe potential departures from General Relativity (GR) along
the propagation of GWs. But different from tests conventionally performed
previously, we vary multiple deformation coefficients simultaneously and employ
the principal component analysis (PCA) method to remedy the strong degeneracy
among deformation coefficients and obtain informative posteriors. The dominant
PCA components can be better measured and constrained, thus are expected to be
more sensitive to potential departures from the waveform model. Using this
method we analyze 10 selected events and get the result that the combined
posteriors of the dominant PCA parameters are consistent with GR within
3-$\sigma$ uncertainty. The standard deviation of the first dominant PCA
parameter is 3 times smaller than that of the original dispersion parameter of
the leading order. However, the multi-parameter test with PCA is more sensitive
to not only potential deviations from GR but also systematic errors of waveform
models. the difference in results obtained by using different waveform
templates indicates that the demands of waveform accuracy are higher to perform
the multi-parameter test with PCA.","Zhi-Chu Ma, Rui Niu, Wen Zhao",2024,http://arxiv.org/abs/2401.17666v1
"PCA-Bench: Evaluating Multimodal Large Language Models in
  Perception-Cognition-Action Chain","We present PCA-Bench, a multimodal decision-making benchmark for evaluating
the integrated capabilities of Multimodal Large Language Models (MLLMs).
Departing from previous benchmarks focusing on simplistic tasks and individual
model capability, PCA-Bench introduces three complex scenarios: autonomous
driving, domestic robotics, and open-world games. Given task instructions and
diverse contexts, the model is required to seamlessly integrate multiple
capabilities of Perception, Cognition, and Action in a reasoning chain to make
accurate decisions. Moreover, PCA-Bench features error localization
capabilities, scrutinizing model inaccuracies in areas such as perception,
knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To
balance accuracy and efficiency in evaluation, we propose PCA-Eval, an
automatic evaluation protocol, and assess 10 prevalent MLLMs. The results
reveal significant performance disparities between open-source models and
powerful proprietary models like GPT-4 Vision. To address this, we introduce
Embodied-Instruction-Evolution (EIE), an automatic framework for synthesizing
instruction tuning examples in multimodal embodied environments. EIE generates
7,510 training examples in PCA-Bench and enhances the performance of
open-source MLLMs, occasionally surpassing GPT-4 Vision (+3\% in decision
accuracy), thereby validating the effectiveness of EIE. Our findings suggest
that robust MLLMs like GPT4-Vision show promise for decision-making in embodied
agents, opening new avenues for MLLM research.","Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, Baobao Chang",2024,http://arxiv.org/abs/2402.15527v1
"High-Dimensional PCA Revisited: Insights from General Spiked Models and
  Data Normalization Effects","Principal Component Analysis (PCA) is a critical tool for dimensionality
reduction and data analysis. This paper revisits PCA through the lens of
generalized spiked covariance and correlation models, which allow for more
realistic and complex data structures. We explore the asymptotic properties of
the sample principal components (PCs) derived from both the sample covariance
and correlation matrices, focusing on how data normalization-an essential step
for scale-invariant analysis-affects these properties. Our results reveal that
while normalization does not alter the first-order limits of spiked eigenvalues
and eigenvectors, it significantly influences their second-order behavior. We
establish new theoretical findings, including a joint central limit theorem for
bilinear forms of the sample covariance matrix's resolvent and diagonal
entries, providing a robust framework for understanding spiked models in high
dimensions. Our theoretical results also reveal an intriguing phenomenon
regarding the effect of data normalization when the variances of covariates are
equal. Specifically, they suggest that high-dimensional PCA based on the
correlation matrix may not only perform comparably to, but potentially even
outperform, PCA based on the covariance matrix-particularly when the leading
principal component is sufficiently large. This study not only extends the
existing literature on spiked models but also offers practical guidance for
applying PCA in real-world scenarios, particularly when dealing with normalized
data.","Yanqing Yin, Wang Zhou",2024,http://arxiv.org/abs/2408.13848v1
"An a-posteriori analysis of co-kurtosis PCA based dimensionality
  reduction using a neural ODE solver","A low-dimensional representation of thermochemical scalars based on
cokurtosis principal component analysis (CoK-PCA) has been shown to effectively
capture stiff chemical dynamics in reacting flows relative to the widely used
principal component analysis (PCA). The effectiveness of the reduced manifold
was evaluated in a priori analyses using both linear and nonlinear
reconstructions of thermochemical scalars from aggressively truncated principal
components (PCs). In this study, we demonstrate the efficacy of a CoK-PCA-based
reduced manifold using a posteriori analysis. Simulations of spontaneous
ignition in a homogeneous reactor that pose a challenge in accurately capturing
the ignition delay time as well as the scalar profiles within the reaction zone
are considered. The governing ordinary differential equations (ODEs) in the PC
space were evolved from the initial conditions using two ODE solvers. First, a
standard ODE solver that uses a pre-trained artificial neural network (ANN) to
estimate the source terms and integrates the solution in time. Second, a neural
ODE solver that incorporates the time integration of PCs into the ANN training.
The time-evolved profiles of the PCs and reconstructed thermochemical scalars
demonstrate the robustness of the CoK-PCA-based low-dimensional manifold in
accurately capturing the ignition process. Furthermore, we observed that the
neural ODE solver minimized propagation errors across time steps and provided
more accurate results than the standard ODE solver. The results of this study
demonstrate the potential of CoK-PCA-based manifolds to be implemented in
massively parallel reacting flow solvers.","Tadikonda Shiva Sai, Hemanth Kolla, Konduri Aditya",2025,http://arxiv.org/abs/2501.02797v1
"Cancer-Net PCa-Seg: Benchmarking Deep Learning Models for Prostate
  Cancer Segmentation Using Synthetic Correlated Diffusion Imaging","Prostate cancer (PCa) is the most prevalent cancer among men in the United
States, accounting for nearly 300,000 cases, 29% of all diagnoses and 35,000
total deaths in 2024. Traditional screening methods such as prostate-specific
antigen (PSA) testing and magnetic resonance imaging (MRI) have been pivotal in
diagnosis, but have faced limitations in specificity and generalizability. In
this paper, we explore the potential of enhancing PCa lesion segmentation using
a novel MRI modality called synthetic correlated diffusion imaging (CDI$^s$).
We employ several state-of-the-art deep learning models, including U-Net,
SegResNet, Swin UNETR, Attention U-Net, and LightM-UNet, to segment PCa lesions
from a 200 CDI$^s$ patient cohort. We find that SegResNet achieved superior
segmentation performance with a Dice-Sorensen coefficient (DSC) of $76.68 \pm
0.8$. Notably, the Attention U-Net, while slightly less accurate (DSC $74.82
\pm 2.0$), offered a favorable balance between accuracy and computational
efficiency. Our findings demonstrate the potential of deep learning models in
improving PCa lesion segmentation using CDI$^s$ to enhance PCa management and
clinical support.","Jarett Dewbury, Chi-en Amy Tai, Alexander Wong",2025,http://arxiv.org/abs/2501.09185v1
Principal component analysis within nuclear structure,"The principal component analysis (PCA) of different parameters affecting
collectivity of nuclei predicted to be candidate of the interacting boson model
dynamical symmetries are performed. The results show that, the use of PCA
within nuclear structure can give us a simple way to identify collectivity
together with the parameters simultaneously affecting it.",A. Al-Sayed,2015,http://arxiv.org/abs/1504.03833v1
"Estimating stellar fundamental parameters using PCA: application to
  early type stars of GES data","This work addresses a procedure to estimate fundamental stellar parameters
such as T eff , logg, [Fe/H], and v sin i using a dimensionality reduction
technique called Principal Component Analysis (PCA), applied to a large
database of synthetic spectra. This technique shows promising results for
inverting stellar parameters of observed targets from Gaia ESO Survey.","W. Farah, M. Gebran, F. Paletou, R. Blomme",2015,http://arxiv.org/abs/1508.03978v1
word2vec Skip-Gram with Negative Sampling is a Weighted Logistic PCA,"We show that the skip-gram formulation of word2vec trained with negative
sampling is equivalent to a weighted logistic PCA. This connection allows us to
better understand the objective, compare it to other word embedding methods,
and extend it to higher dimensional models.","Andrew J. Landgraf, Jeremy Bellay",2017,http://arxiv.org/abs/1705.09755v1
"Adaptive Strategies For Efficient Model Reduction In High-Dimensional
  Inverse Problems","This work explores a novel approach for adaptive, differentiable
parametrization of large-scale non-stationary random fields. Coupled with any
gradient-based algorithm, the method can be applied to variety of optimization
problems, including history matching. The developed technique is based on
principal component analysis (PCA), but, in contrast to other PCA-based
methods, allows to amend parametrization process regarding objective function
behaviour.","Andrei Mukhin, Aleksey Khlyupin",2019,http://arxiv.org/abs/1903.07220v1
High-probability bounds for the reconstruction error of PCA,"We derive high-probability bounds for the reconstruction error of PCA in
infinite dimensions. We apply our bounds in the case that the eigenvalues of
the covariance operator satisfy polynomial or exponential upper bounds.","Cassandra Milbradt, Martin Wahl",2019,http://arxiv.org/abs/1909.10787v2
Partial combinatory algebra and generalized numberings,"Generalized numberings are an extension of Ershov's notion of numbering,
based on partial combinatory algebra (pca) instead of the natural numbers. We
study various algebraic properties of generalized numberings, relating
properties of the numbering to properties of the pca. As in the lambda
calculus, extensionality is a key notion here.","H. P. Barendregt, S. A. Terwijn",2019,http://arxiv.org/abs/1910.07750v3
Computability in partial combinatory algebras,"We prove a number of elementary facts about computability in partial
combinatory algebras (pca's). We disprove a suggestion made by Kreisel about
using Friedberg numberings to construct extensional pca's. We then discuss
separability and elements without total extensions. We relate this to Ershov's
notion of precompleteness, and we show that precomplete numberings are not 1-1
in general.",S. A. Terwijn,2019,http://arxiv.org/abs/1910.09258v3
"Discussion contribution ""Functional models for time-varying random
  objects'' by Dubey and Müller (to appear in JRSS-B)","In an inspiring paper Dubey and M\""uller (DM) extend PCA to the case that
observations are metric-valued functions. As an alternative, we develop a
kernel PCA approach, which we show is closely related to the DM approach. While
kernel principal components (kPCs) are simply defined, DM require added
complexity in the form of ""object FPCs'' and ""Fr\'echet scores"".",Wicher Bergsma,2019,http://arxiv.org/abs/1911.08468v1
Hierarchical PCA and Modeling Asset Correlations,"Modeling cross-sectional correlations between thousands of stocks, across
countries and industries, can be challenging. In this paper, we demonstrate the
advantages of using Hierarchical Principal Component Analysis (HPCA) over the
classic PCA. We also introduce a statistical clustering algorithm for
identifying of homogeneous clusters of stocks, or ""synthetic sectors"". We apply
these methods to study cross-sectional correlations in the US, Europe, China,
and Emerging Markets.","Marco Avellaneda, Juan Andrés Serur",2020,http://arxiv.org/abs/2010.04140v1
Projection Pursuit with Applications to scRNA Sequencing Data,"In this paper, we explore the limitations of PCA as a dimension reduction
technique and study its extension, projection pursuit (PP), which is a broad
class of linear dimension reduction methods. We first discuss the relevant
concepts and theorems and then apply PCA and PP (with negative standardized
Shannon's entropy as the projection index) on single cell RNA sequencing data.","Elvis Han Cui, Heather Zhou",2019,http://arxiv.org/abs/1912.07602v2
"A geometric framework for asymptotic inference of principal subspaces in
  PCA","In this article, we develop an asymptotic method for constructing confidence
regions for the set of all linear subspaces arising from PCA, from which we
derive hypothesis tests on this set. Our method is based on the geometry of
Riemannian manifolds with which some sets of linear subspaces are endowed.","Dimbihery Rabenoro, Xavier Pennec",2022,http://arxiv.org/abs/2209.02025v3
"Patient-specific computational forecasting of prostate cancer growth
  during active surveillance using an imaging-informed biomechanistic model","Active surveillance (AS) is a suitable management option for newly-diagnosed
prostate cancer (PCa), which usually presents low to intermediate clinical
risk. Patients enrolled in AS have their tumor closely monitored via
longitudinal multiparametric magnetic resonance imaging (mpMRI), serum
prostate-specific antigen tests, and biopsies. Hence, the patient is prescribed
treatment when these tests identify progression to higher-risk PCa. However,
current AS protocols rely on detecting tumor progression through direct
observation according to standardized monitoring strategies. This approach
limits the design of patient-specific AS plans and may lead to the late
detection and treatment of tumor progression. Here, we propose to address these
issues by leveraging personalized computational predictions of PCa growth. Our
forecasts are obtained with a spatiotemporal biomechanistic model informed by
patient-specific longitudinal mpMRI data. Our results show that our predictive
technology can represent and forecast the global tumor burden for individual
patients, achieving concordance correlation coefficients ranging from 0.93 to
0.99 across our cohort (n=7). Additionally, we identify a model-based biomarker
of higher-risk PCa: the mean proliferation activity of the tumor (p=0.041).
Using logistic regression, we construct a PCa risk classifier based on this
biomarker that achieves an area under the receiver operating characteristic
curve of 0.83. We further show that coupling our tumor forecasts with this PCa
risk classifier enables the early identification of PCa progression to
higher-risk disease by more than one year. Thus, we posit that our predictive
technology constitutes a promising clinical decision-making tool to design
personalized AS plans for PCa patients.","Guillermo Lorenzo, Jon S. Heiselman, Michael A. Liss, Michael I. Miga, Hector Gomez, Thomas E. Yankeelov, Alessandro Reali, Thomas J. R. Hughes",2023,http://arxiv.org/abs/2310.00060v1
PBES: PCA Based Exemplar Sampling Algorithm for Continual Learning,"We propose a novel exemplar selection approach based on Principal Component
Analysis (PCA) and median sampling, and a neural network training regime in the
setting of class-incremental learning. This approach avoids the pitfalls due to
outliers in the data and is both simple to implement and use across various
incremental machine learning models. It also has independent usage as a
sampling algorithm. We achieve better performance compared to state-of-the-art
methods.","Sahil Nokhwal, Nirman Kumar",2023,http://arxiv.org/abs/2312.09352v1
Low-M-Rank Tensor Completion and Robust Tensor PCA,"In this paper, we propose a new approach to solve low-rank tensor completion
and robust tensor PCA. Our approach is based on some novel notion of
(even-order) tensor ranks, to be called the M-rank, the symmetric M-rank, and
the strongly symmetric M-rank. We discuss the connections between these new
tensor ranks and the CP-rank and the symmetric CP-rank of an even-order tensor.
We show that the M-rank provides a reliable and easy-computable approximation
to the CP-rank. As a result, we propose to replace the CP-rank by the M-rank in
the low-CP-rank tensor completion and robust tensor PCA. Numerical results
suggest that our new approach based on the M-rank outperforms existing methods
that are based on low-n-rank, t-SVD and KBR approaches for solving low-rank
tensor completion and robust tensor PCA when the underlying tensor has low
CP-rank.","Bo Jiang, Shiqian Ma, Shuzhong Zhang",2015,http://arxiv.org/abs/1501.03689v5
Optimal Sparse Linear Auto-Encoders and Sparse PCA,"Principal components analysis (PCA) is the optimal linear auto-encoder of
data, and it is often used to construct features. Enforcing sparsity on the
principal components can promote better generalization, while improving the
interpretability of the features. We study the problem of constructing optimal
sparse linear auto-encoders. Two natural questions in such a setting are: i)
Given a level of sparsity, what is the best approximation to PCA that can be
achieved? ii) Are there low-order polynomial-time algorithms which can
asymptotically achieve this optimal tradeoff between the sparsity and the
approximation quality?
  In this work, we answer both questions by giving efficient low-order
polynomial-time algorithms for constructing asymptotically \emph{optimal}
linear auto-encoders (in particular, sparse features with near-PCA
reconstruction error) and demonstrate the performance of our algorithms on real
data.","Malik Magdon-Ismail, Christos Boutsidis",2015,http://arxiv.org/abs/1502.06626v1
"Robust estimation of principal components from depth-based multivariate
  rank covariance matrix","Analyzing principal components for multivariate data from its spatial sign
covariance matrix (SCM) has been proposed as a computationally simple and
robust alternative to normal PCA, but it suffers from poor efficiency
properties and is actually inadmissible with respect to the maximum likelihood
estimator. Here we use data depth-based spatial ranks in place of spatial signs
to obtain the orthogonally equivariant Depth Covariance Matrix (DCM) and use
its eigenvector estimates for PCA. We derive asymptotic properties of the
sample DCM and influence functions of its eigenvectors. The shapes of these
influence functions indicate robustness of estimated principal components, and
good efficiency properties compared to the SCM. Finite sample simulation
studies show that principal components of the sample DCM are robust with
respect to deviations from normality, as well as are more efficient than the
SCM and its affine equivariant version, Tyler's shape matrix. Through two real
data examples, we also show the effectiveness of DCM-based PCA in analyzing
high-dimensional data and outlier detection, and compare it with other methods
of robust PCA.",Subhabrata Majumdar,2015,http://arxiv.org/abs/1502.07042v3
Analysis of PCA Algorithms in Distributed Environments,"Classical machine learning algorithms often face scalability bottlenecks when
they are applied to large-scale data. Such algorithms were designed to work
with small data that is assumed to fit in the memory of one machine. In this
report, we analyze different methods for computing an important machine learing
algorithm, namely Principal Component Analysis (PCA), and we comment on its
limitations in supporting large datasets. The methods are analyzed and compared
across two important metrics: time complexity and communication complexity. We
consider the worst-case scenarios for both metrics, and we identify the
software libraries that implement each method. The analysis in this report
helps researchers and engineers in (i) understanding the main bottlenecks for
scalability in different PCA algorithms, (ii) choosing the most appropriate
method and software library for a given application and data set
characteristics, and (iii) designing new scalable PCA algorithms.","Tarek Elgamal, Mohamed Hefeeda",2015,http://arxiv.org/abs/1503.05214v2
"Online Principal Component Analysis in High Dimension: Which Algorithm
  to Choose?","In the current context of data explosion, online techniques that do not
require storing all data in memory are indispensable to routinely perform tasks
like principal component analysis (PCA). Recursive algorithms that update the
PCA with each new observation have been studied in various fields of research
and found wide applications in industrial monitoring, computer vision,
astronomy, and latent semantic indexing, among others. This work provides
guidance for selecting an online PCA algorithm in practice. We present the main
approaches to online PCA, namely, perturbation techniques, incremental methods,
and stochastic optimization, and compare their statistical accuracy,
computation time, and memory requirements using artificial and real data.
Extensions to missing data and to functional data are discussed. All studied
algorithms are available in the R package onlinePCA on CRAN.","Hervé Cardot, David Degras",2015,http://arxiv.org/abs/1511.03688v1
"Multi-centrality Graph Spectral Decompositions and their Application to
  Cyber Intrusion Detection","Many modern datasets can be represented as graphs and hence spectral
decompositions such as graph principal component analysis (PCA) can be useful.
Distinct from previous graph decomposition approaches based on subspace
projection of a single topological feature, e.g., the Fiedler vector of
centered graph adjacency matrix (graph Laplacian), we propose spectral
decomposition approaches to graph PCA and graph dictionary learning that
integrate multiple features, including graph walk statistics, centrality
measures and graph distances to reference nodes. In this paper we propose a new
PCA method for single graph analysis, called multi-centrality graph PCA
(MC-GPCA), and a new dictionary learning method for ensembles of graphs, called
multi-centrality graph dictionary learning (MC-GDL), both based on spectral
decomposition of multi-centrality matrices. As an application to cyber
intrusion detection, MC-GPCA can be an effective indicator of anomalous
connectivity pattern and MC-GDL can provide discriminative basis for attack
classification.","Pin-Yu Chen, Sutanay Choudhury, Alfred O. Hero",2015,http://arxiv.org/abs/1512.07372v2
"Near-Optimal Stochastic Approximation for Online Principal Component
  Estimation","Principal component analysis (PCA) has been a prominent tool for
high-dimensional data analysis. Online algorithms that estimate the principal
component by processing streaming data are of tremendous practical and
theoretical interests. Despite its rich applications, theoretical convergence
analysis remains largely open. In this paper, we cast online PCA into a
stochastic nonconvex optimization problem, and we analyze the online PCA
algorithm as a stochastic approximation iteration. The stochastic approximation
iteration processes data points incrementally and maintains a running estimate
of the principal component. We prove for the first time a nearly optimal
finite-sample error bound for the online PCA algorithm. Under the subgaussian
assumption, we show that the finite-sample error bound closely matches the
minimax information lower bound.","Chris Junchi Li, Mengdi Wang, Han Liu, Tong Zhang",2016,http://arxiv.org/abs/1603.05305v4
"Correlated-PCA: Principal Components' Analysis when Data and Noise are
  Correlated","Given a matrix of observed data, Principal Components Analysis (PCA) computes
a small number of orthogonal directions that contain most of its variability.
Provably accurate solutions for PCA have been in use for a long time. However,
to the best of our knowledge, all existing theoretical guarantees for it assume
that the data and the corrupting noise are mutually independent, or at least
uncorrelated. This is valid in practice often, but not always. In this paper,
we study the PCA problem in the setting where the data and noise can be
correlated. Such noise is often also referred to as ""data-dependent noise"". We
obtain a correctness result for the standard eigenvalue decomposition (EVD)
based solution to PCA under simple assumptions on the data-noise correlation.
We also develop and analyze a generalization of EVD, cluster-EVD, that improves
upon EVD in certain regimes.","Namrata Vaswani, Han Guo",2016,http://arxiv.org/abs/1608.04320v2
"Pilot Testing an Artificial Intelligence Algorithm That Selects Homeless
  Youth Peer Leaders Who Promote HIV Testing","Objective. To pilot test an artificial intelligence (AI) algorithm that
selects peer change agents (PCA) to disseminate HIV testing messaging in a
population of homeless youth. Methods. We recruited and assessed 62 youth at
baseline, 1 month (n = 48), and 3 months (n = 38). A Facebook app collected
preliminary social network data. Eleven PCAs selected by AI attended a 1-day
training and 7 weekly booster sessions. Mixed-effects models with random
effects were used to assess change over time. Results. Significant change over
time was observed in past 6-month HIV testing (57.9%, 82.4%, 76.3%; p < .05)
but not condom use (63.9%, 65.7%, 65.8%). Most youth reported speaking to a PCA
about HIV prevention (72.0% at 1 month, 61.5% at 3 months). Conclusions. AI is
a promising avenue for implementing PCA models for homeless youth. Increasing
rates of regular HIV testing is critical to HIV prevention and linking homeless
youth to treatment.","Eric Rice, Robin Petering, Jaih Craddock, Amanda Yoshioka-Maxwell, Amulya Yadav, Milind Tambe",2016,http://arxiv.org/abs/1608.05701v1
Minimal and minimum unit circular-arc models,"A proper circular-arc (PCA) model is a pair ${\cal M} = (C, \cal A)$ where
$C$ is a circle and $\cal A$ is a family of inclusion-free arcs on $C$ in which
no two arcs of $\cal A$ cover $C$. A PCA model $\cal U = (C,\cal A)$ is a $(c,
\ell)$-CA model when $C$ has circumference $c$, all the arcs in $\cal A$ have
length $\ell$, and all the extremes of the arcs in $\cal A$ are at a distance
at least $1$. If $c \leq c'$ and $\ell \leq \ell'$ for every $(c', \ell')$-CA
model equivalent (resp. isomorphic) to $\cal U$, then $\cal U$ is minimal
(resp. minimum). In this article we prove that every PCA model is isomorphic to
a minimum model. Our main tool is a new characterization of those PCA models
that are equivalent to $(c,\ell)$-CA models, that allows us to conclude that
$c$ and $\ell$ are integer when $\cal U$ is minimal. As a consequence, we
obtain an $O(n^3)$ time and $O(n^2)$ space algorithm to solve the minimal
representation problem, while we prove that the minimum representation problem
is NP-complete.","Francisco J. Soulignac, Pablo Terlisky",2016,http://arxiv.org/abs/1609.01266v3
"Iteratively Reweighted Least Squares Algorithms for L1-Norm Principal
  Component Analysis","Principal component analysis (PCA) is often used to reduce the dimension of
data by selecting a few orthonormal vectors that explain most of the variance
structure of the data. L1 PCA uses the L1 norm to measure error, whereas the
conventional PCA uses the L2 norm. For the L1 PCA problem minimizing the
fitting error of the reconstructed data, we propose an exact reweighted and an
approximate algorithm based on iteratively reweighted least squares. We provide
convergence analyses, and compare their performance against benchmark
algorithms in the literature. The computational experiment shows that the
proposed algorithms consistently perform best.","Young Woong Park, Diego Klabjan",2016,http://arxiv.org/abs/1609.02997v2
"Correlated-PCA: Principal Components' Analysis when Data and Noise are
  Correlated","Given a matrix of observed data, Principal Components Analysis (PCA) computes
a small number of orthogonal directions that contain most of its variability.
Provably accurate solutions for PCA have been in use for a long time. However,
to the best of our knowledge, all existing theoretical guarantees for it assume
that the data and the corrupting noise are mutually independent, or at least
uncorrelated. This is valid in practice often, but not always. In this paper,
we study the PCA problem in the setting where the data and noise can be
correlated. Such noise is often also referred to as ""data-dependent noise"". We
obtain a correctness result for the standard eigenvalue decomposition (EVD)
based solution to PCA under simple assumptions on the data-noise correlation.
We also develop and analyze a generalization of EVD, cluster-EVD, that improves
upon EVD in certain regimes.","Namrata Vaswani, Han Guo",2016,http://arxiv.org/abs/1610.09307v2
"A simple method to construct confidence bands in functional linear
  regression","This paper develops a simple method to construct confidence bands, centered
at a principal component analysis (PCA) based estimator, for the slope function
in a functional linear regression model with a scalar response variable and a
functional predictor variable. The PCA-based estimator is a series estimator
with estimated basis functions, and so construction of valid confidence bands
for it is a non-trivial challenge. We propose a confidence band that aims at
covering the slope function at ""most"" of points with a prespecified probability
(level), and prove its asymptotic validity under suitable regularity
conditions. Importantly, this is the first paper that derives confidence bands
having theoretical justifications for the PCA-based estimator. We also propose
a practical method to choose the cut-off level used in PCA-based estimation,
and conduct numerical studies to verify the finite sample performance of the
proposed confidence band. Finally, we apply our methodology to spectrometric
data, and discuss extensions of our methodology to cases where additional
vector-valued regressors are present.","Masaaki Imaizumi, Kengo Kato",2016,http://arxiv.org/abs/1612.07490v3
"Feature Selection based on PCA and PSO for Multimodal Medical Image
  Fusion using DTCWT","Multimodal medical image fusion helps to increase efficiency in medical
diagnosis. This paper presents multimodal medical image fusion by selecting
relevant features using Principle Component Analysis (PCA) and Particle Swarm
Optimization techniques (PSO). DTCWT is used for decomposition of the images
into low and high frequency coefficients. Fusion rules such as combination of
minimum, maximum and simple averaging are applied to approximate and detailed
coefficients. The fused image is reconstructed by inverse DTCWT. Performance
metrics are evaluated and it shows that DTCWT-PCA performs better than
DTCWT-PSO in terms of Structural Similarity Index Measure (SSIM) and Cross
Correlation (CC). Computation time and feature vector size is reduced in
DTCWT-PCA compared to DTCWT-PSO for feature selection which proves robustness
and storage capacity.","Padmavathi K, Mahima Bhat, Maya V Karki",2017,http://arxiv.org/abs/1701.08918v1
Thresholding based Efficient Outlier Robust PCA,"We consider the problem of outlier robust PCA (OR-PCA) where the goal is to
recover principal directions despite the presence of outlier data points. That
is, given a data matrix $M^*$, where $(1-\alpha)$ fraction of the points are
noisy samples from a low-dimensional subspace while $\alpha$ fraction of the
points can be arbitrary outliers, the goal is to recover the subspace
accurately. Existing results for \OR-PCA have serious drawbacks: while some
results are quite weak in the presence of noise, other results have runtime
quadratic in dimension, rendering them impractical for large scale
applications.
  In this work, we provide a novel thresholding based iterative algorithm with
per-iteration complexity at most linear in the data size. Moreover, the
fraction of outliers, $\alpha$, that our method can handle is tight up to
constants while providing nearly optimal computational complexity for a general
noise setting. For the special case where the inliers are obtained from a
low-dimensional subspace with additive Gaussian noise, we show that a
modification of our thresholding based method leads to significant improvement
in recovery error (of the subspace) even in the presence of a large fraction of
outliers.","Yeshwanth Cherapanamjeri, Prateek Jain, Praneeth Netrapalli",2017,http://arxiv.org/abs/1702.05571v1
"Approximate probabilistic cellular automata for the dynamics of
  single-species populations under discrete logisticlike growth with and
  without weak Allee effects","We investigate one-dimensional elementary probabilistic cellular automata
(PCA) whose dynamics in first-order mean-field approximation yields discrete
logisticlike growth models for a single-species unstructured population with
nonoverlapping generations. Beginning with a general six-parameter model, we
find constraints on the transition probabilities of the PCA that guarantee that
the ensuing approximations make sense in terms of population dynamics and
classify the valid combinations thereof. Several possible models display a
negative cubic term that can be interpreted as a weak Allee factor. We also
investigate the conditions under which a one-parameter PCA derived from the
more general six-parameter model can generate valid population growth dynamics.
Numerical simulations illustrate the behavior of some of the PCA found.","J. Ricardo G. Mendonça, Yeva Gevorgyan",2017,http://arxiv.org/abs/1703.06007v3
"Revealing the ultra-fast outflow in IRAS 13224-3809 through spectral
  variability","We present an analysis of the long-term X-ray variability of the extreme
narrow-line Seyfert 1 (NLS1) galaxy IRAS 13224-3809 using principal component
analysis (PCA) and fractional excess variability (Fvar) spectra to identify
model-independent spectral components. We identify a series of variability
peaks in both the first PCA component and Fvar spectrum which correspond to the
strongest predicted absorption lines from the ultra-fast outflow (UFO)
discovered by Parker et al. (2017). We also find higher order PCA components,
which correspond to variability of the soft excess and reflection features. The
subtle differences between RMS and PCA results argue that the observed
flux-dependence of the absorption is due to increased ionization of the gas,
rather than changes in column density or covering fraction. This result
demonstrates that we can detect outflows from variability alone, and that
variability studies of UFOs are an extremely promising avenue for future
research.","Michael L. Parker, William N. Alston, Douglas J. K. Buisson, Andrew C. Fabian, Jiachen Jiang, Erin Kara, Anne Lohfink, Ciro Pinto, Christopher S. Reynolds",2017,http://arxiv.org/abs/1704.05545v1
Single-Pass PCA of Large High-Dimensional Data,"Principal component analysis (PCA) is a fundamental dimension reduction tool
in statistics and machine learning. For large and high-dimensional data,
computing the PCA (i.e., the singular vectors corresponding to a number of
dominant singular values of the data matrix) becomes a challenging task. In
this work, a single-pass randomized algorithm is proposed to compute PCA with
only one pass over the data. It is suitable for processing extremely large and
high-dimensional data stored in slow memory (hard disk) or the data generated
in a streaming fashion. Experiments with synthetic and real data validate the
algorithm's accuracy, which has orders of magnitude smaller error than an
existing single-pass algorithm. For a set of high-dimensional data stored as a
150 GB file, the proposed algorithm is able to compute the first 50 principal
components in just 24 minutes on a typical 24-core computer, with less than 1
GB memory cost.","Wenjian Yu, Yu Gu, Jian Li, Shenghua Liu, Yaohang Li",2017,http://arxiv.org/abs/1704.07669v1
Optimal Projected Variance Group-Sparse Block PCA,"We address the problem of defining a group sparse formulation for Principal
Components Analysis (PCA) - or its equivalent formulations as Low Rank
approximation or Dictionary Learning problems - which achieves a compromise
between maximizing the variance explained by the components and promoting
sparsity of the loadings. So we propose first a new definition of the variance
explained by non necessarily orthogonal components, which is optimal in some
aspect and compatible with the principal components situation. Then we use a
specific regularization of this variance by the group-$\ell_{1}$ norm to define
a Group Sparse Maximum Variance (GSMV) formulation of PCA. The GSMV formulation
achieves our objective by construction, and has the nice property that the
inner non smooth optimization problem can be solved analytically, thus reducing
GSMV to the maximization of a smooth and convex function under unit norm and
orthogonality constraints, which generalizes Journee et al. (2010) to group
sparsity. Numerical comparison with deflation on synthetic data shows that GSMV
produces steadily slightly better and more robust results for the retrieval of
hidden sparse structures, and is about three times faster on these examples.
Application to real data shows the interest of group sparsity for variables
selection in PCA of mixed data (categorical/numerical) .","Marie Chavent, Guy Chavent",2017,http://arxiv.org/abs/1705.00461v2
"Implications of Computer Vision Driven Assistive Technologies Towards
  Individuals with Visual Impairment","Computer vision based technology is becoming ubiquitous in society. One
application area that has seen an increase in computer vision is assistive
technologies, specifically for those with visual impairment. Research has shown
the ability of computer vision models to achieve tasks such provide scene
captions, detect objects and recognize faces. Although assisting individuals
with visual impairment with these tasks increases their independence and
autonomy, concerns over bias, privacy and potential usefulness arise. This
paper addresses the positive and negative implications computer vision based
assistive technologies have on individuals with visual impairment, as well as
considerations for computer vision researchers and developers in order to
mitigate the amount of negative implications.","Linda Wang, Alexander Wong",2019,http://arxiv.org/abs/1905.07844v1
"Multiband NFC for High-Throughput Wireless Computer Vision Sensor
  Network","Vision sensors lie in the heart of computer vision. In many computer vision
applications, such as AR/VR, non-contacting near-field communication (NFC) with
high throughput is required to transfer information to algorithms. In this
work, we proposed a novel NFC system which utilizes multiple frequency bands to
achieve high throughput.","F. Li, J. Du",2017,http://arxiv.org/abs/1707.03720v1
Deep Learning vs. Traditional Computer Vision,"Deep Learning has pushed the limits of what was possible in the domain of
Digital Image Processing. However, that is not to say that the traditional
computer vision techniques which had been undergoing progressive development in
years prior to the rise of DL have become obsolete. This paper will analyse the
benefits and drawbacks of each approach. The aim of this paper is to promote a
discussion on whether knowledge of classical computer vision techniques should
be maintained. The paper will also explore how the two sides of computer vision
can be combined. Several recent hybrid methodologies are reviewed which have
demonstrated the ability to improve computer vision performance and to tackle
problems not suited to Deep Learning. For example, combining traditional
computer vision techniques with Deep Learning has been popular in emerging
domains such as Panoramic Vision and 3D vision for which Deep Learning models
have not yet been fully optimised","Niall O' Mahony, Sean Campbell, Anderson Carvalho, Suman Harapanahalli, Gustavo Velasco-Hernandez, Lenka Krpalkova, Daniel Riordan, Joseph Walsh",2019,http://arxiv.org/abs/1910.13796v1
Enhancing camera surveillance using computer vision: a research note,"$\mathbf{Purpose}$ - The growth of police operated surveillance cameras has
out-paced the ability of humans to monitor them effectively. Computer vision is
a possible solution. An ongoing research project on the application of computer
vision within a municipal police department is described. The paper aims to
discuss these issues.
  $\mathbf{Design/methodology/approach}$ - Following the demystification of
computer vision technology, its potential for police agencies is developed
within a focus on computer vision as a solution for two common surveillance
camera tasks (live monitoring of multiple surveillance cameras and summarizing
archived video files). Three unaddressed research questions (can specialized
computer vision applications for law enforcement be developed at this time, how
will computer vision be utilized within existing public safety camera
monitoring rooms, and what are the system-wide impacts of a computer vision
capability on local criminal justice systems) are considered.
  $\mathbf{Findings}$ - Despite computer vision becoming accessible to law
enforcement agencies the impact of computer vision has not been discussed or
adequately researched. There is little knowledge of computer vision or its
potential in the field.
  $\mathbf{Originality/value}$ - This paper introduces and discusses computer
vision from a law enforcement perspective and will be valuable to police
personnel tasked with monitoring large camera networks and considering computer
vision as a system upgrade.","Haroon Idrees, Mubarak Shah, Ray Surette",2018,http://arxiv.org/abs/1808.03998v1
"Are object detection assessment criteria ready for maritime computer
  vision?","Maritime vessels equipped with visible and infrared cameras can complement
other conventional sensors for object detection. However, application of
computer vision techniques in maritime domain received attention only recently.
The maritime environment offers its own unique requirements and challenges.
Assessment of the quality of detections is a fundamental need in computer
vision. However, the conventional assessment metrics suitable for usual object
detection are deficient in the maritime setting. Thus, a large body of related
work in computer vision appears inapplicable to the maritime setting at the
first sight. We discuss the problem of defining assessment metrics suitable for
maritime computer vision. We consider new bottom edge proximity metrics as
assessment metrics for maritime computer vision. These metrics indicate that
existing computer vision approaches are indeed promising for maritime computer
vision and can play a foundational role in the emerging field of maritime
computer vision.","Dilip K. Prasad, Huixu Dong, Deepu Rajan, Chai Quek",2018,http://arxiv.org/abs/1809.04659v2
BMVC 2019: Workshop on Interpretable and Explainable Machine Vision,"Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable
Machine Vision, Cardiff, UK, September 12, 2019.",Alun Preece,2019,http://arxiv.org/abs/1909.07245v1
"SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for
  Large-scale Vision-Language Models","Large-scale Vision-Language Models (LVLMs) have significantly advanced with
text-aligned vision inputs. They have made remarkable progress in computer
vision tasks by aligning text modality with vision inputs. There are also
endeavors to incorporate multi-vision sensors beyond RGB, including thermal,
depth, and medical X-ray images. However, we observe that current LVLMs view
images taken from multi-vision sensors as if they were in the same RGB domain
without considering the physical characteristics of multi-vision sensors. They
fail to convey the fundamental multi-vision sensor information from the dataset
and the corresponding contextual knowledge properly. Consequently, alignment
between the information from the actual physical environment and the text is
not achieved correctly, making it difficult to answer complex sensor-related
questions that consider the physical environment. In this paper, we aim to
establish a multi-vision Sensor Perception And Reasoning benchmarK called SPARK
that can reduce the fundamental multi-vision sensor information gap between
images and multi-vision sensors. We generated 6,248 vision-language test
samples to investigate multi-vision sensory perception and multi-vision sensory
reasoning on physical sensor knowledge proficiency across different formats,
covering different types of sensor-related questions. We utilized these samples
to assess ten leading LVLMs. The results showed that most models displayed
deficiencies in multi-vision sensory reasoning to varying extents. Codes and
data are available at https://github.com/top-yun/SPARK","Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee, Yong Man Ro",2024,http://arxiv.org/abs/2408.12114v3
"Vision Transformers in Medical Computer Vision -- A Contemplative
  Retrospection","Recent escalation in the field of computer vision underpins a huddle of
algorithms with the magnificent potential to unravel the information contained
within images. These computer vision algorithms are being practised in medical
image analysis and are transfiguring the perception and interpretation of
Imaging data. Among these algorithms, Vision Transformers are evolved as one of
the most contemporary and dominant architectures that are being used in the
field of computer vision. These are immensely utilized by a plenty of
researchers to perform new as well as former experiments. Here, in this article
we investigate the intersection of Vision Transformers and Medical images and
proffered an overview of various ViTs based frameworks that are being used by
different researchers in order to decipher the obstacles in Medical Computer
Vision. We surveyed the application of Vision transformers in different areas
of medical computer vision such as image-based disease classification,
anatomical structure segmentation, registration, region-based lesion Detection,
captioning, report generation, reconstruction using multiple medical imaging
modalities that greatly assist in medical diagnosis and hence treatment
process. Along with this, we also demystify several imaging modalities used in
Medical Computer Vision. Moreover, to get more insight and deeper
understanding, self-attention mechanism of transformers is also explained
briefly. Conclusively, we also put some light on available data sets, adopted
methodology, their performance measures, challenges and their solutions in form
of discussion. We hope that this review article will open future directions for
researchers in medical computer vision.","Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar, Huma Ameer, Muhammad Ali, Muhammad Moazam Fraz",2022,http://arxiv.org/abs/2203.15269v1
Adapting Computer Vision Algorithms for Omnidirectional Video,"Omnidirectional (360{\deg}) video has got quite popular because it provides a
highly immersive viewing experience. For computer vision algorithms, it poses
several challenges, like the special (equirectangular) projection commonly
employed and the huge image size. In this work, we give a high-level overview
of these challenges and outline strategies how to adapt computer vision
algorithm for the specifics of omnidirectional video.",Hannes Fassold,2019,http://arxiv.org/abs/1907.09233v1
Some Insights into Lifelong Reinforcement Learning Systems,"A lifelong reinforcement learning system is a learning system that has the
ability to learn through trail-and-error interaction with the environment over
its lifetime. In this paper, I give some arguments to show that the traditional
reinforcement learning paradigm fails to model this type of learning system.
Some insights into lifelong reinforcement learning are provided, along with a
simplistic prototype lifelong reinforcement learning system.",Changjian Li,2020,http://arxiv.org/abs/2001.09608v1
"Counterexample-Guided Repair of Reinforcement Learning Systems Using
  Safety Critics","Naively trained Deep Reinforcement Learning agents may fail to satisfy vital
safety constraints. To avoid costly retraining, we may desire to repair a
previously trained reinforcement learning agent to obviate unsafe behaviour. We
devise a counterexample-guided repair algorithm for repairing reinforcement
learning systems leveraging safety critics. The algorithm jointly repairs a
reinforcement learning agent and a safety critic using gradient-based
constrained optimisation.","David Boetius, Stefan Leue",2024,http://arxiv.org/abs/2405.15430v1
Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey,"Deep reinforcement learning augments the reinforcement learning framework and
utilizes the powerful representation of deep neural networks. Recent works have
demonstrated the remarkable successes of deep reinforcement learning in various
domains including finance, medicine, healthcare, video games, robotics, and
computer vision. In this work, we provide a detailed review of recent and
state-of-the-art research advances of deep reinforcement learning in computer
vision. We start with comprehending the theories of deep learning,
reinforcement learning, and deep reinforcement learning. We then propose a
categorization of deep reinforcement learning methodologies and discuss their
advantages and limitations. In particular, we divide deep reinforcement
learning into seven main categories according to their applications in computer
vision, i.e. (i)landmark localization (ii) object detection; (iii) object
tracking; (iv) registration on both 2D image and 3D image volumetric data (v)
image segmentation; (vi) videos analysis; and (vii) other applications. Each of
these categories is further analyzed with reinforcement learning techniques,
network design, and performance. Moreover, we provide a comprehensive analysis
of the existing publicly available datasets and examine source code
availability. Finally, we present some open issues and discuss future research
directions on deep reinforcement learning in computer vision","Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki, Khoa Luu, Marios Savvides",2021,http://arxiv.org/abs/2108.11510v1
Causal Reinforcement Learning: A Survey,"Reinforcement learning is an essential paradigm for solving sequential
decision problems under uncertainty. Despite many remarkable achievements in
recent decades, applying reinforcement learning methods in the real world
remains challenging. One of the main obstacles is that reinforcement learning
agents lack a fundamental understanding of the world and must therefore learn
from scratch through numerous trial-and-error interactions. They may also face
challenges in providing explanations for their decisions and generalizing the
acquired knowledge. Causality, however, offers a notable advantage as it can
formalize knowledge in a systematic manner and leverage invariance for
effective knowledge transfer. This has led to the emergence of causal
reinforcement learning, a subfield of reinforcement learning that seeks to
enhance existing algorithms by incorporating causal relationships into the
learning process. In this survey, we comprehensively review the literature on
causal reinforcement learning. We first introduce the basic concepts of
causality and reinforcement learning, and then explain how causality can
address core challenges in non-causal reinforcement learning. We categorize and
systematically review existing causal reinforcement learning approaches based
on their target problems and methodologies. Finally, we outline open issues and
future directions in this emerging field.","Zhihong Deng, Jing Jiang, Guodong Long, Chengqi Zhang",2023,http://arxiv.org/abs/2307.01452v2
"Distributed Deep Reinforcement Learning: A Survey and A Multi-Player
  Multi-Agent Learning Toolbox","With the breakthrough of AlphaGo, deep reinforcement learning becomes a
recognized technique for solving sequential decision-making problems. Despite
its reputation, data inefficiency caused by its trial and error learning
mechanism makes deep reinforcement learning hard to be practical in a wide
range of areas. Plenty of methods have been developed for sample efficient deep
reinforcement learning, such as environment modeling, experience transfer, and
distributed modifications, amongst which, distributed deep reinforcement
learning has shown its potential in various applications, such as
human-computer gaming, and intelligent transportation. In this paper, we
conclude the state of this exciting field, by comparing the classical
distributed deep reinforcement learning methods, and studying important
components to achieve efficient distributed learning, covering single player
single agent distributed deep reinforcement learning to the most complex
multiple players multiple agents distributed deep reinforcement learning.
Furthermore, we review recently released toolboxes that help to realize
distributed deep reinforcement learning without many modifications of their
non-distributed versions. By analyzing their strengths and weaknesses, a
multi-player multi-agent distributed deep reinforcement learning toolbox is
developed and released, which is further validated on Wargame, a complex
environment, showing usability of the proposed toolbox for multiple players and
multiple agents distributed deep reinforcement learning under complex games.
Finally, we try to point out challenges and future trends, hoping this brief
review can provide a guide or a spark for researchers who are interested in
distributed deep reinforcement learning.","Qiyue Yin, Tongtong Yu, Shengqi Shen, Jun Yang, Meijing Zhao, Kaiqi Huang, Bin Liang, Liang Wang",2022,http://arxiv.org/abs/2212.00253v1
Transfer Learning in Deep Reinforcement Learning: A Survey,"Reinforcement learning is a learning paradigm for solving sequential
decision-making problems. Recent years have witnessed remarkable progress in
reinforcement learning upon the fast development of deep neural networks. Along
with the promising prospects of reinforcement learning in numerous domains such
as robotics and game-playing, transfer learning has arisen to tackle various
challenges faced by reinforcement learning, by transferring knowledge from
external expertise to facilitate the efficiency and effectiveness of the
learning process. In this survey, we systematically investigate the recent
progress of transfer learning approaches in the context of deep reinforcement
learning. Specifically, we provide a framework for categorizing the
state-of-the-art transfer learning approaches, under which we analyze their
goals, methodologies, compatible reinforcement learning backbones, and
practical applications. We also draw connections between transfer learning and
other relevant topics from the reinforcement learning perspective and explore
their potential challenges that await future research progress.","Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain, Jiayu Zhou",2020,http://arxiv.org/abs/2009.07888v7
"Memory-two strategies forming symmetric mutual reinforcement learning
  equilibrium in repeated prisoners' dilemma game","We investigate symmetric equilibria of mutual reinforcement learning when
both players alternately learn the optimal memory-two strategies against the
opponent in the repeated prisoners' dilemma game. We provide a necessary
condition for memory-two deterministic strategies to form symmetric equilibria.
We then provide three examples of memory-two deterministic strategies which
form symmetric mutual reinforcement learning equilibria. We also prove that
mutual reinforcement learning equilibria formed by memory-two strategies are
also mutual reinforcement learning equilibria when both players use
reinforcement learning of memory-$n$ strategies with $n>2$.",Masahiko Ueda,2021,http://arxiv.org/abs/2108.03258v2
Implementing Online Reinforcement Learning with Temporal Neural Networks,"A Temporal Neural Network (TNN) architecture for implementing efficient
online reinforcement learning is proposed and studied via simulation. The
proposed T-learning system is composed of a frontend TNN that implements online
unsupervised clustering and a backend TNN that implements online reinforcement
learning. The reinforcement learning paradigm employs biologically plausible
neo-Hebbian three-factor learning rules. As a working example, a prototype
implementation of the cart-pole problem (balancing an inverted pendulum) is
studied via simulation.",James E. Smith,2022,http://arxiv.org/abs/2204.05437v1
Deep Reinforcement Learning for Conversational AI,"Deep reinforcement learning is revolutionizing the artificial intelligence
field. Currently, it serves as a good starting point for constructing
intelligent autonomous systems which offer a better knowledge of the visual
world. It is possible to scale deep reinforcement learning with the use of deep
learning and do amazing tasks such as use of pixels in playing video games. In
this paper, key concepts of deep reinforcement learning including reward
function, differences between reinforcement learning and supervised learning
and models for implementation of reinforcement are discussed. Key challenges
related to the implementation of reinforcement learning in conversational AI
domain are identified as well as discussed in detail. Various conversational
models which are based on deep reinforcement learning (as well as deep
learning) are also discussed. In summary, this paper discusses key aspects of
deep reinforcement learning which are crucial for designing an efficient
conversational AI.","Mahipal Jadeja, Neelanshi Varia, Agam Shah",2017,http://arxiv.org/abs/1709.05067v1
"On the Opportunities and Challenges of Offline Reinforcement Learning
  for Recommender Systems","Reinforcement learning serves as a potent tool for modeling dynamic user
interests within recommender systems, garnering increasing research attention
of late. However, a significant drawback persists: its poor data efficiency,
stemming from its interactive nature. The training of reinforcement
learning-based recommender systems demands expensive online interactions to
amass adequate trajectories, essential for agents to learn user preferences.
This inefficiency renders reinforcement learning-based recommender systems a
formidable undertaking, necessitating the exploration of potential solutions.
Recent strides in offline reinforcement learning present a new perspective.
Offline reinforcement learning empowers agents to glean insights from offline
datasets and deploy learned policies in online settings. Given that recommender
systems possess extensive offline datasets, the framework of offline
reinforcement learning aligns seamlessly. Despite being a burgeoning field,
works centered on recommender systems utilizing offline reinforcement learning
remain limited. This survey aims to introduce and delve into offline
reinforcement learning within recommender systems, offering an inclusive review
of existing literature in this domain. Furthermore, we strive to underscore
prevalent challenges, opportunities, and future pathways, poised to propel
research in this evolving field.","Xiaocong Chen, Siyu Wang, Julian McAuley, Dietmar Jannach, Lina Yao",2023,http://arxiv.org/abs/2308.11336v1
A Survey Analyzing Generalization in Deep Reinforcement Learning,"Reinforcement learning research obtained significant success and attention
with the utilization of deep neural networks to solve problems in high
dimensional state or action spaces. While deep reinforcement learning policies
are currently being deployed in many different fields from medical applications
to large language models, there are still ongoing questions the field is trying
to answer on the generalization capabilities of deep reinforcement learning
policies. In this paper, we will formalize and analyze generalization in deep
reinforcement learning. We will explain the fundamental reasons why deep
reinforcement learning policies encounter overfitting problems that limit their
generalization capabilities. Furthermore, we will categorize and explain the
manifold solution approaches to increase generalization, and overcome
overfitting in deep reinforcement learning policies. From exploration to
adversarial analysis and from regularization to robustness our paper provides
an analysis on a wide range of subfields within deep reinforcement learning
with a broad scope and in-depth view. We believe our study can provide a
compact guideline for the current advancements in deep reinforcement learning,
and help to construct robust deep neural policies with higher generalization
skills.",Ezgi Korkmaz,2024,http://arxiv.org/abs/2401.02349v2
Reinforcement Teaching,"Machine learning algorithms learn to solve a task, but are unable to improve
their ability to learn. Meta-learning methods learn about machine learning
algorithms and improve them so that they learn more quickly. However, existing
meta-learning methods are either hand-crafted to improve one specific component
of an algorithm or only work with differentiable algorithms. We develop a
unifying meta-learning framework, called Reinforcement Teaching, to improve the
learning process of \emph{any} algorithm. Under Reinforcement Teaching, a
teaching policy is learned, through reinforcement, to improve a student's
learning algorithm. To learn an effective teaching policy, we introduce the
parametric-behavior embedder that learns a representation of the student's
learnable parameters from its input/output behavior. We further use learning
progress to shape the teacher's reward, allowing it to more quickly maximize
the student's performance. To demonstrate the generality of Reinforcement
Teaching, we conduct experiments in which a teacher learns to significantly
improve both reinforcement and supervised learning algorithms. Reinforcement
Teaching outperforms previous work using heuristic reward functions and state
representations, as well as other parameter representations.","Calarina Muslimani, Alex Lewandowski, Dale Schuurmans, Matthew E. Taylor, Jun Luo",2022,http://arxiv.org/abs/2204.11897v3
Generative Adversarial Imitation Learning,"Consider learning a policy from example expert behavior, without interaction
with the expert or access to reinforcement signal. One approach is to recover
the expert's cost function with inverse reinforcement learning, then extract a
policy from that cost function with reinforcement learning. This approach is
indirect and can be slow. We propose a new general framework for directly
extracting a policy from data, as if it were obtained by reinforcement learning
following inverse reinforcement learning. We show that a certain instantiation
of our framework draws an analogy between imitation learning and generative
adversarial networks, from which we derive a model-free imitation learning
algorithm that obtains significant performance gains over existing model-free
methods in imitating complex behaviors in large, high-dimensional environments.","Jonathan Ho, Stefano Ermon",2016,http://arxiv.org/abs/1606.03476v1
Two-Memory Reinforcement Learning,"While deep reinforcement learning has shown important empirical success, it
tends to learn relatively slow due to slow propagation of rewards information
and slow update of parametric neural networks. Non-parametric episodic memory,
on the other hand, provides a faster learning alternative that does not require
representation learning and uses maximum episodic return as state-action values
for action selection. Episodic memory and reinforcement learning both have
their own strengths and weaknesses. Notably, humans can leverage multiple
memory systems concurrently during learning and benefit from all of them. In
this work, we propose a method called Two-Memory reinforcement learning agent
(2M) that combines episodic memory and reinforcement learning to distill both
of their strengths. The 2M agent exploits the speed of the episodic memory part
and the optimality and the generalization capacity of the reinforcement
learning part to complement each other. Our experiments demonstrate that the 2M
agent is more data efficient and outperforms both pure episodic memory and pure
reinforcement learning, as well as a state-of-the-art memory-augmented RL
agent. Moreover, the proposed approach provides a general framework that can be
used to combine any episodic memory agent with other off-policy reinforcement
learning algorithms.","Zhao Yang, Thomas. M. Moerland, Mike Preuss, Aske Plaat",2023,http://arxiv.org/abs/2304.10098v2
Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning,"Reinforcement learning, evolutionary algorithms and imitation learning are
three principal methods to deal with continuous control tasks. Reinforcement
learning is sample efficient, yet sensitive to hyper-parameters setting and
needs efficient exploration; Evolutionary algorithms are stable, but with low
sample efficiency; Imitation learning is both sample efficient and stable,
however it requires the guidance of expert data. In this paper, we propose
Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning,
a scalable framework that combines advantages of the three methods mentioned
above. The core of this framework is a dual-actors and single critic
reinforcement learning agent. This agent can recruit high-fitness actors from
the population of evolutionary algorithms, which instructs itself to learn from
experience replay buffer. At the same time, low-fitness actors in the
evolutionary population can imitate behavior patterns of the reinforcement
learning agent and improve their adaptability. Reinforcement and imitation
learners in this framework can be replaced with any off-policy actor-critic
reinforcement learner or data-driven imitation learner. We evaluate RIM on a
series of benchmarks for continuous control tasks in Mujoco. The experimental
results show that RIM outperforms prior evolutionary or reinforcement learning
methods. The performance of RIM's components is significantly better than
components of previous evolutionary reinforcement learning algorithm, and the
recruitment using soft update enables reinforcement learning agent to learn
faster than that using hard update.","Shuai Lü, Shuai Han, Wenbo Zhou, Junwei Zhang",2019,http://arxiv.org/abs/1912.06310v1
"Accelerate Reinforcement Learning with PID Controllers in the Pendulum
  Simulations","We propose a Proportional Integral Derivative (PID) controller-based coaching
scheme to expedite reinforcement learning (RL).",Liping Bai,2022,http://arxiv.org/abs/2210.00770v1
"Dex: Incremental Learning for Complex Environments in Deep Reinforcement
  Learning","This paper introduces Dex, a reinforcement learning environment toolkit
specialized for training and evaluation of continual learning methods as well
as general reinforcement learning problems. We also present the novel continual
learning method of incremental learning, where a challenging environment is
solved using optimal weight initialization learned from first solving a similar
easier environment. We show that incremental learning can produce vastly
superior results than standard methods by providing a strong baseline method
across ten Dex environments. We finally develop a saliency method for
qualitative analysis of reinforcement learning, which shows the impact
incremental learning has on network attention.","Nick Erickson, Qi Zhao",2017,http://arxiv.org/abs/1706.05749v1
Augmented Q Imitation Learning (AQIL),"The study of unsupervised learning can be generally divided into two
categories: imitation learning and reinforcement learning. In imitation
learning the machine learns by mimicking the behavior of an expert system
whereas in reinforcement learning the machine learns via direct environment
feedback. Traditional deep reinforcement learning takes a significant time
before the machine starts to converge to an optimal policy. This paper proposes
Augmented Q-Imitation-Learning, a method by which deep reinforcement learning
convergence can be accelerated by applying Q-imitation-learning as the initial
training process in traditional Deep Q-learning.","Xiao Lei Zhang, Anish Agarwal",2020,http://arxiv.org/abs/2004.00993v2
Interpretable Reinforcement Learning with Ensemble Methods,"We propose to use boosted regression trees as a way to compute
human-interpretable solutions to reinforcement learning problems. Boosting
combines several regression trees to improve their accuracy without
significantly reducing their inherent interpretability. Prior work has focused
independently on reinforcement learning and on interpretable machine learning,
but there has been little progress in interpretable reinforcement learning. Our
experimental results show that boosted regression trees compute solutions that
are both interpretable and match the quality of leading reinforcement learning
methods.","Alexander Brown, Marek Petrik",2018,http://arxiv.org/abs/1809.06995v1
Unsupervised Meta-Learning for Reinforcement Learning,"Meta-learning algorithms use past experience to learn to quickly solve new
tasks. In the context of reinforcement learning, meta-learning algorithms
acquire reinforcement learning procedures to solve new problems more
efficiently by utilizing experience from prior tasks. The performance of
meta-learning algorithms depends on the tasks available for meta-training: in
the same way that supervised learning generalizes best to test points drawn
from the same distribution as the training points, meta-learning methods
generalize best to tasks from the same distribution as the meta-training tasks.
In effect, meta-reinforcement learning offloads the design burden from
algorithm design to task design. If we can automate the process of task design
as well, we can devise a meta-learning algorithm that is truly automated. In
this work, we take a step in this direction, proposing a family of unsupervised
meta-learning algorithms for reinforcement learning. We motivate and describe a
general recipe for unsupervised meta-reinforcement learning, and present an
instantiation of this approach. Our conceptual and theoretical contributions
consist of formulating the unsupervised meta-reinforcement learning problem and
describing how task proposals based on mutual information can be used to train
optimal meta-learners. Our experimental results indicate that unsupervised
meta-reinforcement learning effectively acquires accelerated reinforcement
learning procedures without the need for manual task design and these
procedures exceed the performance of learning from scratch.","Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, Sergey Levine",2018,http://arxiv.org/abs/1806.04640v3
Lineage Evolution Reinforcement Learning,"We propose a general agent population learning system, and on this basis, we
propose lineage evolution reinforcement learning algorithm. Lineage evolution
reinforcement learning is a kind of derivative algorithm which accords with the
general agent population learning system. We take the agents in DQN and its
related variants as the basic agents in the population, and add the selection,
mutation and crossover modules in the genetic algorithm to the reinforcement
learning algorithm. In the process of agent evolution, we refer to the
characteristics of natural genetic behavior, add lineage factor to ensure the
retention of potential performance of agent, and comprehensively consider the
current performance and lineage value when evaluating the performance of agent.
Without changing the parameters of the original reinforcement learning
algorithm, lineage evolution reinforcement learning can optimize different
reinforcement learning algorithms. Our experiments show that the idea of
evolution with lineage improves the performance of original reinforcement
learning algorithm in some games in Atari 2600.","Zeyu Zhang, Guisheng Yin",2020,http://arxiv.org/abs/2010.14616v1
"Robust Reinforcement Learning with Distributional Risk-averse
  formulation","Robust Reinforcement Learning tries to make predictions more robust to
changes in the dynamics or rewards of the system. This problem is particularly
important when the dynamics and rewards of the environment are estimated from
the data. In this paper, we approximate the Robust Reinforcement Learning
constrained with a $\Phi$-divergence using an approximate Risk-Averse
formulation. We show that the classical Reinforcement Learning formulation can
be robustified using standard deviation penalization of the objective. Two
algorithms based on Distributional Reinforcement Learning, one for discrete and
one for continuous action spaces are proposed and tested in a classical Gym
environment to demonstrate the robustness of the algorithms.","Pierre Clavier, Stéphanie Allassonière, Erwan Le Pennec",2022,http://arxiv.org/abs/2206.06841v1
A survey of benchmarking frameworks for reinforcement learning,"Reinforcement learning has recently experienced increased prominence in the
machine learning community. There are many approaches to solving reinforcement
learning problems with new techniques developed constantly. When solving
problems using reinforcement learning, there are various difficult challenges
to overcome. To ensure progress in the field, benchmarks are important for
testing new algorithms and comparing with other approaches. The reproducibility
of results for fair comparison is therefore vital in ensuring that improvements
are accurately judged. This paper provides an overview of different
contributions to reinforcement learning benchmarking and discusses how they can
assist researchers to address the challenges facing reinforcement learning. The
contributions discussed are the most used and recent in the literature. The
paper discusses the contributions in terms of implementation, tasks and
provided algorithm implementations with benchmarks. The survey aims to bring
attention to the wide range of reinforcement learning benchmarking tasks
available and to encourage research to take place in a standardised manner.
Additionally, this survey acts as an overview for researchers not familiar with
the different tasks that can be used to develop and test new reinforcement
learning algorithms.","Belinda Stapelberg, Katherine M. Malan",2020,http://arxiv.org/abs/2011.13577v1
"Distilling Neuron Spike with High Temperature in Reinforcement Learning
  Agents","Spiking neural network (SNN), compared with depth neural network (DNN), has
faster processing speed, lower energy consumption and more biological
interpretability, which is expected to approach Strong AI. Reinforcement
learning is similar to learning in biology. It is of great significance to
study the combination of SNN and RL. We propose the reinforcement learning
method of spike distillation network (SDN) with STBP. This method uses
distillation to effectively avoid the weakness of STBP, which can achieve SOTA
performance in classification, and can obtain a smaller, faster convergence and
lower power consumption SNN reinforcement learning model. Experiments show that
our method can converge faster than traditional SNN reinforcement learning and
DNN reinforcement learning methods, about 1000 epochs faster, and obtain SNN
200 times smaller than DNN. We also deploy SDN to the PKU nc64c chip, which
proves that SDN has lower power consumption than DNN, and the power consumption
of SDN is more than 600 times lower than DNN on large-scale devices. SDN
provides a new way of SNN reinforcement learning, and can achieve SOTA
performance, which proves the possibility of further development of SNN
reinforcement learning.","Ling Zhang, Jian Cao, Yuan Zhang, Bohan Zhou, Shuo Feng",2021,http://arxiv.org/abs/2108.10078v1
A Survey on Offline Model-Based Reinforcement Learning,"Model-based approaches are becoming increasingly popular in the field of
offline reinforcement learning, with high potential in real-world applications
due to the model's capability of thoroughly utilizing the large historical
datasets available with supervised learning techniques. This paper presents a
literature review of recent work in offline model-based reinforcement learning,
a field that utilizes model-based approaches in offline reinforcement learning.
The survey provides a brief overview of the concepts and recent developments in
both offline reinforcement learning and model-based reinforcement learning, and
discuss the intersection of the two fields. We then presents key relevant
papers in the field of offline model-based reinforcement learning and discuss
their methods, particularly their approaches in solving the issue of
distributional shift, the main problem faced by all current offline model-based
reinforcement learning methods. We further discuss key challenges faced by the
field, and suggest possible directions for future work.",Haoyang He,2023,http://arxiv.org/abs/2305.03360v1
"Integration of Imitation Learning using GAIL and Reinforcement Learning
  using Task-achievement Rewards via Probabilistic Graphical Model","Integration of reinforcement learning and imitation learning is an important
problem that has been studied for a long time in the field of intelligent
robotics. Reinforcement learning optimizes policies to maximize the cumulative
reward, whereas imitation learning attempts to extract general knowledge about
the trajectories demonstrated by experts, i.e., demonstrators. Because each of
them has their own drawbacks, methods combining them and compensating for each
set of drawbacks have been explored thus far. However, many of the methods are
heuristic and do not have a solid theoretical basis. In this paper, we present
a new theory for integrating reinforcement and imitation learning by extending
the probabilistic generative model framework for reinforcement learning, {\it
plan by inference}. We develop a new probabilistic graphical model for
reinforcement learning with multiple types of rewards and a probabilistic
graphical model for Markov decision processes with multiple optimality
emissions (pMDP-MO). Furthermore, we demonstrate that the integrated learning
method of reinforcement learning and imitation learning can be formulated as a
probabilistic inference of policies on pMDP-MO by considering the output of the
discriminator in generative adversarial imitation learning as an additional
optimal emission observation. We adapt the generative adversarial imitation
learning and task-achievement reward to our proposed framework, achieving
significantly better performance than agents trained with reinforcement
learning or imitation learning alone. Experiments demonstrate that our
framework successfully integrates imitation and reinforcement learning even
when the number of demonstrators is only a few.","Akira Kinose, Tadahiro Taniguchi",2019,http://arxiv.org/abs/1907.02140v2
A Brief Survey of Deep Reinforcement Learning,"Deep reinforcement learning is poised to revolutionise the field of AI and
represents a step towards building autonomous systems with a higher level
understanding of the visual world. Currently, deep learning is enabling
reinforcement learning to scale to problems that were previously intractable,
such as learning to play video games directly from pixels. Deep reinforcement
learning algorithms are also applied to robotics, allowing control policies for
robots to be learned directly from camera inputs in the real world. In this
survey, we begin with an introduction to the general field of reinforcement
learning, then progress to the main streams of value-based and policy-based
methods. Our survey will cover central algorithms in deep reinforcement
learning, including the deep $Q$-network, trust region policy optimisation, and
asynchronous advantage actor-critic. In parallel, we highlight the unique
advantages of deep neural networks, focusing on visual understanding via
reinforcement learning. To conclude, we describe several current areas of
research within the field.","Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath",2017,http://arxiv.org/abs/1708.05866v2
Generalization and Regularization in DQN,"Deep reinforcement learning algorithms have shown an impressive ability to
learn complex control policies in high-dimensional tasks. However, despite the
ever-increasing performance on popular benchmarks, policies learned by deep
reinforcement learning algorithms can struggle to generalize when evaluated in
remarkably similar environments. In this paper we propose a protocol to
evaluate generalization in reinforcement learning through different modes of
Atari 2600 games. With that protocol we assess the generalization capabilities
of DQN, one of the most traditional deep reinforcement learning algorithms, and
we provide evidence suggesting that DQN overspecializes to the training
environment. We then comprehensively evaluate the impact of dropout and
$\ell_2$ regularization, as well as the impact of reusing learned
representations to improve the generalization capabilities of DQN. Despite
regularization being largely underutilized in deep reinforcement learning, we
show that it can, in fact, help DQN learn more general features. These features
can be reused and fine-tuned on similar tasks, considerably improving DQN's
sample efficiency.","Jesse Farebrother, Marlos C. Machado, Michael Bowling",2018,http://arxiv.org/abs/1810.00123v3
Rating-based Reinforcement Learning,"This paper develops a novel rating-based reinforcement learning approach that
uses human ratings to obtain human guidance in reinforcement learning.
Different from the existing preference-based and ranking-based reinforcement
learning paradigms, based on human relative preferences over sample pairs, the
proposed rating-based reinforcement learning approach is based on human
evaluation of individual trajectories without relative comparisons between
sample pairs. The rating-based reinforcement learning approach builds on a new
prediction model for human ratings and a novel multi-class loss function. We
conduct several experimental studies based on synthetic ratings and real human
ratings to evaluate the effectiveness and benefits of the new rating-based
reinforcement learning approach.","Devin White, Mingkang Wu, Ellen Novoseller, Vernon J. Lawhern, Nicholas Waytowich, Yongcan Cao",2023,http://arxiv.org/abs/2307.16348v2
Placement Optimization with Deep Reinforcement Learning,"Placement Optimization is an important problem in systems and chip design,
which consists of mapping the nodes of a graph onto a limited set of resources
to optimize for an objective, subject to constraints. In this paper, we start
by motivating reinforcement learning as a solution to the placement problem. We
then give an overview of what deep reinforcement learning is. We next formulate
the placement problem as a reinforcement learning problem and show how this
problem can be solved with policy gradient optimization. Finally, we describe
lessons we have learned from training deep reinforcement learning policies
across a variety of placement optimization problems.","Anna Goldie, Azalia Mirhoseini",2020,http://arxiv.org/abs/2003.08445v1
Equivariant Reinforcement Learning for Quadrotor UAV,"This paper presents an equivariant reinforcement learning framework for
quadrotor unmanned aerial vehicles. Successful training of reinforcement
learning often requires numerous interactions with the environments, which
hinders its applicability especially when the available computational resources
are limited, or when there is no reliable simulation model. We identified an
equivariance property of the quadrotor dynamics such that the dimension of the
state required in the training is reduced by one, thereby improving the
sampling efficiency of reinforcement learning substantially. This is
illustrated by numerical examples with popular reinforcement learning
techniques of TD3 and SAC.","Beomyeol Yu, Taeyoung Lee",2022,http://arxiv.org/abs/2206.01233v2
Explaining Reinforcement Learning with Shapley Values,"For reinforcement learning systems to be widely adopted, their users must
understand and trust them. We present a theoretical analysis of explaining
reinforcement learning using Shapley values, following a principled approach
from game theory for identifying the contribution of individual players to the
outcome of a cooperative game. We call this general framework Shapley Values
for Explaining Reinforcement Learning (SVERL). Our analysis exposes the
limitations of earlier uses of Shapley values in reinforcement learning. We
then develop an approach that uses Shapley values to explain agent performance.
In a variety of domains, SVERL produces meaningful explanations that match and
supplement human intuition.","Daniel Beechey, Thomas M. S. Smith, Özgür Şimşek",2023,http://arxiv.org/abs/2306.05810v1
Diverse Policies Converge in Reward-free Markov Decision Processe,"Reinforcement learning has achieved great success in many decision-making
tasks, and traditional reinforcement learning algorithms are mainly designed
for obtaining a single optimal solution. However, recent works show the
importance of developing diverse policies, which makes it an emerging research
topic. Despite the variety of diversity reinforcement learning algorithms that
have emerged, none of them theoretically answer the question of how the
algorithm converges and how efficient the algorithm is. In this paper, we
provide a unified diversity reinforcement learning framework and investigate
the convergence of training diverse policies. Under such a framework, we also
propose a provably efficient diversity reinforcement learning algorithm.
Finally, we verify the effectiveness of our method through numerical
experiments.","Fanqi Lin, Shiyu Huang, Weiwei Tu",2023,http://arxiv.org/abs/2308.11924v1
"PGN: A perturbation generation network against deep reinforcement
  learning","Deep reinforcement learning has advanced greatly and applied in many areas.
In this paper, we explore the vulnerability of deep reinforcement learning by
proposing a novel generative model for creating effective adversarial examples
to attack the agent. Our proposed model can achieve both targeted attacks and
untargeted attacks. Considering the specificity of deep reinforcement learning,
we propose the action consistency ratio as a measure of stealthiness, and a new
measurement index of effectiveness and stealthiness. Experiment results show
that our method can ensure the effectiveness and stealthiness of attack
compared with other algorithms. Moreover, our methods are considerably faster
and thus can achieve rapid and efficient verification of the vulnerability of
deep reinforcement learning.","Xiangjuan Li, Feifan Li, Yang Li, Quan Pan",2023,http://arxiv.org/abs/2312.12904v1
Modern Deep Reinforcement Learning Algorithms,"Recent advances in Reinforcement Learning, grounded on combining classical
theoretical results with Deep Learning paradigm, led to breakthroughs in many
artificial intelligence tasks and gave birth to Deep Reinforcement Learning
(DRL) as a field of research. In this work latest DRL algorithms are reviewed
with a focus on their theoretical justification, practical limitations and
observed empirical properties.","Sergey Ivanov, Alexander D'yakonov",2019,http://arxiv.org/abs/1906.10025v2
"Applications of Deep Reinforcement Learning in Communications and
  Networking: A Survey","This paper presents a comprehensive literature review on applications of deep
reinforcement learning in communications and networking. Modern networks, e.g.,
Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become
more decentralized and autonomous. In such networks, network entities need to
make decisions locally to maximize the network performance under uncertainty of
network environment. Reinforcement learning has been efficiently used to enable
the network entities to obtain the optimal policy including, e.g., decisions or
actions, given their states when the state and action spaces are small.
However, in complex and large-scale networks, the state and action spaces are
usually large, and the reinforcement learning may not be able to find the
optimal policy in reasonable time. Therefore, deep reinforcement learning, a
combination of reinforcement learning with deep learning, has been developed to
overcome the shortcomings. In this survey, we first give a tutorial of deep
reinforcement learning from fundamental concepts to advanced models. Then, we
review deep reinforcement learning approaches proposed to address emerging
issues in communications and networking. The issues include dynamic network
access, data rate control, wireless caching, data offloading, network security,
and connectivity preservation which are all important to next generation
networks such as 5G and beyond. Furthermore, we present applications of deep
reinforcement learning for traffic routing, resource sharing, and data
collection. Finally, we highlight important challenges, open issues, and future
research directions of applying deep reinforcement learning.","Nguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang, Ying-Chang Liang, Dong In Kim",2018,http://arxiv.org/abs/1810.07862v1
Data Valuation for Offline Reinforcement Learning,"The success of deep reinforcement learning (DRL) hinges on the availability
of training data, which is typically obtained via a large number of environment
interactions. In many real-world scenarios, costs and risks are associated with
gathering these data. The field of offline reinforcement learning addresses
these issues through outsourcing the collection of data to a domain expert or a
carefully monitored program and subsequently searching for a batch-constrained
optimal policy. With the emergence of data markets, an alternative to
constructing a dataset in-house is to purchase external data. However, while
state-of-the-art offline reinforcement learning approaches have shown a lot of
promise, they currently rely on carefully constructed datasets that are well
aligned with the intended target domains. This raises questions regarding the
transferability and robustness of an offline reinforcement learning agent
trained on externally acquired data. In this paper, we empirically evaluate the
ability of the current state-of-the-art offline reinforcement learning
approaches to coping with the source-target domain mismatch within two MuJoCo
environments, finding that current state-of-the-art offline reinforcement
learning algorithms underperform in the target domain. To address this, we
propose data valuation for offline reinforcement learning (DVORL), which allows
us to identify relevant and high-quality transitions, improving the performance
and transferability of policies learned by offline reinforcement learning
algorithms. The results show that our method outperforms offline reinforcement
learning baselines on two MuJoCo environments.","Amir Abolfazli, Gregory Palmer, Daniel Kudenko",2022,http://arxiv.org/abs/2205.09550v1
"A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a
  Platform","With the development of industry, drones are appearing in various field. In
recent years, deep reinforcement learning has made impressive gains in games,
and we are committed to applying deep reinforcement learning algorithms to the
field of robotics, moving reinforcement learning algorithms from game scenarios
to real-world application scenarios. We are inspired by the LunarLander of
OpenAI Gym, we decided to make a bold attempt in the field of reinforcement
learning to control drones. At present, there is still a lack of work applying
reinforcement learning algorithms to robot control, the physical simulation
platform related to robot control is only suitable for the verification of
classical algorithms, and is not suitable for accessing reinforcement learning
algorithms for the training. In this paper, we will face this problem, bridging
the gap between physical simulation platforms and intelligent agent, connecting
intelligent agents to a physical simulation platform, allowing agents to learn
and complete drone flight tasks in a simulator that approximates the real
world. We proposed a reinforcement learning framework based on Gazebo that is a
kind of physical simulation platform (ROS-RL), and used three continuous action
space reinforcement learning algorithms in the framework to dealing with the
problem of autonomous landing of drones. Experiments show the effectiveness of
the algorithm, the task of autonomous landing of drones based on reinforcement
learning achieved full success.","Z. Jiang, G. Song",2022,http://arxiv.org/abs/2209.02954v1
"Tackling Error Propagation through Reinforcement Learning: A Case of
  Greedy Dependency Parsing","Error propagation is a common problem in NLP. Reinforcement learning explores
erroneous states during training and can therefore be more robust when mistakes
are made early in a process. In this paper, we apply reinforcement learning to
greedy dependency parsing which is known to suffer from error propagation.
Reinforcement learning improves accuracy of both labeled and unlabeled
dependencies of the Stanford Neural Dependency Parser, a high performance
greedy parser, while maintaining its efficiency. We investigate the portion of
errors which are the result of error propagation and confirm that reinforcement
learning reduces the occurrence of error propagation.","Minh Le, Antske Fokkens",2017,http://arxiv.org/abs/1702.06794v1
"Active Reinforcement Learning -- A Roadmap Towards Curious Classifier
  Systems for Self-Adaptation","Intelligent systems have the ability to improve their behaviour over time
taking observations, experiences or explicit feedback into account. Traditional
approaches separate the learning problem and make isolated use of techniques
from different field of machine learning such as reinforcement learning, active
learning, anomaly detection or transfer learning, for instance. In this
context, the fundamental reinforcement learning approaches come with several
drawbacks that hinder their application to real-world systems: trial-and-error,
purely reactive behaviour or isolated problem handling. The idea of this
article is to present a concept for alleviating these drawbacks by setting up a
research agenda towards what we call ""active reinforcement learning"" in
intelligent systems.","Simon Reichhuber, Sven Tomforde",2022,http://arxiv.org/abs/2201.03947v1
Exploration by Distributional Reinforcement Learning,"We propose a framework based on distributional reinforcement learning and
recent attempts to combine Bayesian parameter updates with deep reinforcement
learning. We show that our proposed framework conceptually unifies multiple
previous methods in exploration. We also derive a practical algorithm that
achieves efficient exploration on challenging control tasks.","Yunhao Tang, Shipra Agrawal",2018,http://arxiv.org/abs/1805.01907v2
Implicit Policy for Reinforcement Learning,"We introduce Implicit Policy, a general class of expressive policies that can
flexibly represent complex action distributions in reinforcement learning, with
efficient algorithms to compute entropy regularized policy gradients. We
empirically show that, despite its simplicity in implementation, entropy
regularization combined with a rich policy class can attain desirable
properties displayed under maximum entropy reinforcement learning framework,
such as robustness and multi-modality.","Yunhao Tang, Shipra Agrawal",2018,http://arxiv.org/abs/1806.06798v2
Derivative-Free Reinforcement Learning: A Review,"Reinforcement learning is about learning agent models that make the best
sequential decisions in unknown environments. In an unknown environment, the
agent needs to explore the environment while exploiting the collected
information, which usually forms a sophisticated problem to solve.
Derivative-free optimization, meanwhile, is capable of solving sophisticated
problems. It commonly uses a sampling-and-updating framework to iteratively
improve the solution, where exploration and exploitation are also needed to be
well balanced. Therefore, derivative-free optimization deals with a similar
core issue as reinforcement learning, and has been introduced in reinforcement
learning approaches, under the names of learning classifier systems and
neuroevolution/evolutionary reinforcement learning. Although such methods have
been developed for decades, recently, derivative-free reinforcement learning
exhibits attracting increasing attention. However, recent survey on this topic
is still lacking. In this article, we summarize methods of derivative-free
reinforcement learning to date, and organize the methods in aspects including
parameter updating, model selection, exploration, and parallel/distributed
methods. Moreover, we discuss some current limitations and possible future
directions, hoping that this article could bring more attentions to this topic
and serve as a catalyst for developing novel and efficient approaches.","Hong Qian, Yang Yu",2021,http://arxiv.org/abs/2102.05710v1
Multi-Task Federated Reinforcement Learning with Adversaries,"Reinforcement learning algorithms, just like any other Machine learning
algorithm pose a serious threat from adversaries. The adversaries can
manipulate the learning algorithm resulting in non-optimal policies. In this
paper, we analyze the Multi-task Federated Reinforcement Learning algorithms,
where multiple collaborative agents in various environments are trying to
maximize the sum of discounted return, in the presence of adversarial agents.
We argue that the common attack methods are not guaranteed to carry out a
successful attack on Multi-task Federated Reinforcement Learning and propose an
adaptive attack method with better attack performance. Furthermore, we modify
the conventional federated reinforcement learning algorithm to address the
issue of adversaries that works equally well with and without the adversaries.
Experimentation on different small to mid-size reinforcement learning problems
show that the proposed attack method outperforms other general attack methods
and the proposed modification to federated reinforcement learning algorithm was
able to achieve near-optimal policies in the presence of adversarial agents.","Aqeel Anwar, Arijit Raychowdhury",2021,http://arxiv.org/abs/2103.06473v1
"rSoccer: A Framework for Studying Reinforcement Learning in Small and
  Very Small Size Robot Soccer","Reinforcement learning is an active research area with a vast number of
applications in robotics, and the RoboCup competition is an interesting
environment for studying and evaluating reinforcement learning methods. A known
difficulty in applying reinforcement learning to robotics is the high number of
experience samples required, being the use of simulated environments for
training the agents followed by transfer learning to real-world (sim-to-real) a
viable path. This article introduces an open-source simulator for the IEEE Very
Small Size Soccer and the Small Size League optimized for reinforcement
learning experiments. We also propose a framework for creating OpenAI Gym
environments with a set of benchmarks tasks for evaluating single-agent and
multi-agent robot soccer skills. We then demonstrate the learning capabilities
of two state-of-the-art reinforcement learning methods as well as their
limitations in certain scenarios introduced in this framework. We believe this
will make it easier for more teams to compete in these categories using
end-to-end reinforcement learning approaches and further develop this research
area.","Felipe B. Martins, Mateus G. Machado, Hansenclever F. Bassani, Pedro H. M. Braga, Edna S. Barros",2021,http://arxiv.org/abs/2106.12895v1
"Tracking the Race Between Deep Reinforcement Learning and Imitation
  Learning -- Extended Version","Learning-based approaches for solving large sequential decision making
problems have become popular in recent years. The resulting agents perform
differently and their characteristics depend on those of the underlying
learning approach. Here, we consider a benchmark planning problem from the
reinforcement learning domain, the Racetrack, to investigate the properties of
agents derived from different deep (reinforcement) learning approaches. We
compare the performance of deep supervised learning, in particular imitation
learning, to reinforcement learning for the Racetrack model. We find that
imitation learning yields agents that follow more risky paths. In contrast, the
decisions of deep reinforcement learning are more foresighted, i.e., avoid
states in which fatal decisions are more likely. Our evaluations show that for
this sequential decision making problem, deep reinforcement learning performs
best in many aspects even though for imitation learning optimal decisions are
considered.","Timo P. Gros, Daniel Höller, Jörg Hoffmann, Verena Wolf",2020,http://arxiv.org/abs/2008.00766v1
Reinforcement Learning and Video Games,"Reinforcement learning has exceeded human-level performance in game playing
AI with deep learning methods according to the experiments from DeepMind on Go
and Atari games. Deep learning solves high dimension input problems which stop
the development of reinforcement for many years. This study uses both two
techniques to create several agents with different algorithms that successfully
learn to play T-rex Runner. Deep Q network algorithm and three types of
improvements are implemented to train the agent. The results from some of them
are far from satisfactory but others are better than human experts. Batch
normalization is a method to solve internal covariate shift problems in deep
neural network. The positive influence of this on reinforcement learning has
also been proved in this study.",Yue Zheng,2019,http://arxiv.org/abs/1909.04751v1
Distributional Reinforcement Learning with Ensembles,"It is well known that ensemble methods often provide enhanced performance in
reinforcement learning. In this paper, we explore this concept further by using
group-aided training within the distributional reinforcement learning paradigm.
Specifically, we propose an extension to categorical reinforcement learning,
where distributional learning targets are implicitly based on the total
information gathered by an ensemble. We empirically show that this may lead to
much more robust initial learning, a stronger individual performance level, and
good efficiency on a per-sample basis.","Björn Lindenberg, Jonas Nordqvist, Karl-Olof Lindahl",2020,http://arxiv.org/abs/2003.10903v2
"Poisoning Deep Reinforcement Learning Agents with In-Distribution
  Triggers","In this paper, we propose a new data poisoning attack and apply it to deep
reinforcement learning agents. Our attack centers on what we call
in-distribution triggers, which are triggers native to the data distributions
the model will be trained on and deployed in. We outline a simple procedure for
embedding these, and other, triggers in deep reinforcement learning agents
following a multi-task learning paradigm, and demonstrate in three common
reinforcement learning environments. We believe that this work has important
implications for the security of deep learning models.","Chace Ashcraft, Kiran Karra",2021,http://arxiv.org/abs/2106.07798v1
A Survey of Exploration Methods in Reinforcement Learning,"Exploration is an essential component of reinforcement learning algorithms,
where agents need to learn how to predict and control unknown and often
stochastic environments. Reinforcement learning agents depend crucially on
exploration to obtain informative data for the learning process as the lack of
enough information could hinder effective learning. In this article, we provide
a survey of modern exploration methods in (Sequential) reinforcement learning,
as well as a taxonomy of exploration methods.","Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, Doina Precup",2021,http://arxiv.org/abs/2109.00157v2
"Reinforcement Learning and Control as Probabilistic Inference: Tutorial
  and Review","The framework of reinforcement learning or optimal control provides a
mathematical formalization of intelligent decision making that is powerful and
broadly applicable. While the general form of the reinforcement learning
problem enables effective reasoning about uncertainty, the connection between
reinforcement learning and inference in probabilistic models is not immediately
obvious. However, such a connection has considerable value when it comes to
algorithm design: formalizing a problem as probabilistic inference in principle
allows us to bring to bear a wide array of approximate inference tools, extend
the model in flexible and powerful ways, and reason about compositionality and
partial observability. In this article, we will discuss how a generalization of
the reinforcement learning or optimal control problem, which is sometimes
termed maximum entropy reinforcement learning, is equivalent to exact
probabilistic inference in the case of deterministic dynamics, and variational
inference in the case of stochastic dynamics. We will present a detailed
derivation of this framework, overview prior work that has drawn on this and
related ideas to propose new reinforcement learning and control algorithms, and
describe perspectives on future research.",Sergey Levine,2018,http://arxiv.org/abs/1805.00909v3
"CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in
  Coq","Reinforcement learning algorithms solve sequential decision-making problems
in probabilistic environments by optimizing for long-term reward. The desire to
use reinforcement learning in safety-critical settings inspires a recent line
of work on formally constrained reinforcement learning; however, these methods
place the implementation of the learning algorithm in their Trusted Computing
Base. The crucial correctness property of these implementations is a guarantee
that the learning algorithm converges to an optimal policy. This paper begins
the work of closing this gap by developing a Coq formalization of two canonical
reinforcement learning algorithms: value and policy iteration for finite state
Markov decision processes. The central results are a formalization of Bellman's
optimality principle and its proof, which uses a contraction property of
Bellman optimality operator to establish that a sequence converges in the
infinite horizon limit. The CertRL development exemplifies how the Giry monad
and mechanized metric coinduction streamline optimality proofs for
reinforcement learning algorithms. The CertRL library provides a general
framework for proving properties about Markov decision processes and
reinforcement learning algorithms, paving the way for further work on
formalization of reinforcement learning algorithms.","Koundinya Vajjha, Avraham Shinnar, Vasily Pestun, Barry Trager, Nathan Fulton",2020,http://arxiv.org/abs/2009.11403v2
"Photonic reinforcement learning based on optoelectronic reservoir
  computing","Reinforcement learning has been intensively investigated and developed in
artificial intelligence in the absence of training data, such as autonomous
driving vehicles, robot control, internet advertising, and elastic optical
networks. However, the computational cost of reinforcement learning with deep
neural networks is extremely high and reducing the learning cost is a
challenging issue. We propose a photonic on-line implementation of
reinforcement learning using optoelectronic delay-based reservoir computing,
both experimentally and numerically. In the proposed scheme, we accelerate
reinforcement learning at a rate of several megahertz because there is no
required learning process for the internal connection weights in reservoir
computing. We perform two benchmark tasks, CartPole-v0 and MountanCar-v0 tasks,
to evaluate the proposed scheme. Our results represent the first hardware
implementation of reinforcement learning based on photonic reservoir computing
and pave the way for fast and efficient reinforcement learning as a novel
photonic accelerator.","Kazutaka Kanno, Atsushi Uchida",2022,http://arxiv.org/abs/2202.12896v1
Teacher-student curriculum learning for reinforcement learning,"Reinforcement learning (rl) is a popular paradigm for sequential decision
making problems. The past decade's advances in rl have led to breakthroughs in
many challenging domains such as video games, board games, robotics, and chip
design. The sample inefficiency of deep reinforcement learning methods is a
significant obstacle when applying rl to real-world problems. Transfer learning
has been applied to reinforcement learning such that the knowledge gained in
one task can be applied when training in a new task. Curriculum learning is
concerned with sequencing tasks or data samples such that knowledge can be
transferred between those tasks to learn a target task that would otherwise be
too difficult to solve. Designing a curriculum that improves sample efficiency
is a complex problem. In this thesis, we propose a teacher-student curriculum
learning setting where we simultaneously train a teacher that selects tasks for
the student while the student learns how to solve the selected task. Our method
is independent of human domain knowledge and manual curriculum design. We
evaluated our methods on two reinforcement learning benchmarks: grid world and
the challenging Google Football environment. With our method, we can improve
the sample efficiency and generality of the student compared to tabula-rasa
reinforcement learning.",Yanick Schraner,2022,http://arxiv.org/abs/2210.17368v1
"Offline Robot Reinforcement Learning with Uncertainty-Guided Human
  Expert Sampling","Recent advances in batch (offline) reinforcement learning have shown
promising results in learning from available offline data and proved offline
reinforcement learning to be an essential toolkit in learning control policies
in a model-free setting. An offline reinforcement learning algorithm applied to
a dataset collected by a suboptimal non-learning-based algorithm can result in
a policy that outperforms the behavior agent used to collect the data. Such a
scenario is frequent in robotics, where existing automation is collecting
operational data. Although offline learning techniques can learn from data
generated by a sub-optimal behavior agent, there is still an opportunity to
improve the sample complexity of existing offline reinforcement learning
algorithms by strategically introducing human demonstration data into the
training process. To this end, we propose a novel approach that uses
uncertainty estimation to trigger the injection of human demonstration data and
guide policy training towards optimal behavior while reducing overall sample
complexity. Our experiments show that this approach is more sample efficient
when compared to a naive way of combining expert data with data collected from
a sub-optimal agent. We augmented an existing offline reinforcement learning
algorithm Conservative Q-Learning with our approach and performed experiments
on data collected from MuJoCo and OffWorld Gym learning environments.","Ashish Kumar, Ilya Kuzovkin",2022,http://arxiv.org/abs/2212.08232v1
"Deep reinforcement learning to detect brain lesions on MRI: a
  proof-of-concept application of reinforcement learning to medical images","Purpose: AI in radiology is hindered chiefly by: 1) Requiring large annotated
data sets. 2) Non-generalizability that limits deployment to new scanners /
institutions. And 3) Inadequate explainability and interpretability. We believe
that reinforcement learning can address all three shortcomings, with robust and
intuitive algorithms trainable on small datasets. To the best of our knowledge,
reinforcement learning has not been directly applied to computer vision tasks
for radiological images. In this proof-of-principle work, we train a deep
reinforcement learning network to predict brain tumor location.
  Materials and Methods: Using the BraTS brain tumor imaging database, we
trained a deep Q network on 70 post-contrast T1-weighted 2D image slices. We
did so in concert with image exploration, with rewards and punishments designed
to localize lesions. To compare with supervised deep learning, we trained a
keypoint detection convolutional neural network on the same 70 images. We
applied both approaches to a separate 30 image testing set.
  Results: Reinforcement learning predictions consistently improved during
training, whereas those of supervised deep learning quickly diverged.
Reinforcement learning predicted testing set lesion locations with 85%
accuracy, compared to roughly 7% accuracy for the supervised deep network.
  Conclusion: Reinforcement learning predicted lesions with high accuracy,
which is unprecedented for such a small training set. We believe that
reinforcement learning can propel radiology AI well past the inherent
limitations of supervised deep learning, with more clinician-driven research
and finally toward true clinical applicability.","Joseph Stember, Hrithwik Shalu",2020,http://arxiv.org/abs/2008.02708v1
Quantile Reinforcement Learning,"In reinforcement learning, the standard criterion to evaluate policies in a
state is the expectation of (discounted) sum of rewards. However, this
criterion may not always be suitable, we consider an alternative criterion
based on the notion of quantiles. In the case of episodic reinforcement
learning problems, we propose an algorithm based on stochastic approximation
with two timescales. We evaluate our proposition on a simple model of the TV
show, Who wants to be a millionaire.","Hugo Gilbert, Paul Weng",2016,http://arxiv.org/abs/1611.00862v1
Regret Bounds for Risk-Sensitive Reinforcement Learning,"In safety-critical applications of reinforcement learning such as healthcare
and robotics, it is often desirable to optimize risk-sensitive objectives that
account for tail outcomes rather than expected reward. We prove the first
regret bounds for reinforcement learning under a general class of
risk-sensitive objectives including the popular CVaR objective. Our theory is
based on a novel characterization of the CVaR objective as well as a novel
optimistic MDP construction.","O. Bastani, Y. J. Ma, E. Shen, W. Xu",2022,http://arxiv.org/abs/2210.05650v1
Reinforcement Learning the Chromatic Symmetric Function,"We propose a conjectural counting formula for the coefficients of the
chromatic symmetric function of unit interval graphs using reinforcement
learning. The formula counts specific disjoint cycle-tuples in the graphs,
referred to as Eschers, which satisfy certain concatenation conditions. These
conditions are identified by a reinforcement learning model and are independent
of the particular unit interval graph, resulting a universal counting
expression.","Gergely Bérczi, Jonas Klüver",2024,http://arxiv.org/abs/2410.19189v1
A Survey of In-Context Reinforcement Learning,"Reinforcement learning (RL) agents typically optimize their policies by
performing expensive backward passes to update their network parameters.
However, some agents can solve new tasks without updating any parameters by
simply conditioning on additional context such as their action-observation
histories. This paper surveys work on such behavior, known as in-context
reinforcement learning.","Amir Moeini, Jiuqi Wang, Jacob Beck, Ethan Blaser, Shimon Whiteson, Rohan Chandra, Shangtong Zhang",2025,http://arxiv.org/abs/2502.07978v1
A Threshold-based Scheme for Reinforcement Learning in Neural Networks,"A generic and scalable Reinforcement Learning scheme for Artificial Neural
Networks is presented, providing a general purpose learning machine. By
reference to a node threshold three features are described 1) A mechanism for
Primary Reinforcement, capable of solving linearly inseparable problems 2) The
learning scheme is extended to include a mechanism for Conditioned
Reinforcement, capable of forming long term strategy 3) The learning scheme is
modified to use a threshold-based deep learning algorithm, providing a robust
and biologically inspired alternative to backpropagation. The model may be used
for supervised as well as unsupervised training regimes.",Thomas H. Ward,2016,http://arxiv.org/abs/1609.03348v4
Deep Reinforcement Learning Boosted by External Knowledge,"Recent improvements in deep reinforcement learning have allowed to solve
problems in many 2D domains such as Atari games. However, in complex 3D
environments, numerous learning episodes are required which may be too time
consuming or even impossible especially in real-world scenarios. We present a
new architecture to combine external knowledge and deep reinforcement learning
using only visual input. A key concept of our system is augmenting image input
by adding environment feature information and combining two sources of
decision. We evaluate the performances of our method in a 3D
partially-observable environment from the Microsoft Malmo platform.
Experimental evaluation exhibits higher performance and faster learning
compared to a single reinforcement learning model.","Nicolas Bougie, Ryutaro Ichise",2017,http://arxiv.org/abs/1712.04101v1
"A Review of Reinforcement Learning for Autonomous Building Energy
  Management","The area of building energy management has received a significant amount of
interest in recent years. This area is concerned with combining advancements in
sensor technologies, communications and advanced control algorithms to optimize
energy utilization. Reinforcement learning is one of the most prominent machine
learning algorithms used for control problems and has had many successful
applications in the area of building energy management. This research gives a
comprehensive review of the literature relating to the application of
reinforcement learning to developing autonomous building energy management
systems. The main direction for future research and challenges in reinforcement
learning are also outlined.","Karl Mason, Santiago Grijalva",2019,http://arxiv.org/abs/1903.05196v2
"Sample Efficient Reinforcement Learning through Learning from
  Demonstrations in Minecraft","Sample inefficiency of deep reinforcement learning methods is a major
obstacle for their use in real-world applications. In this work, we show how
human demonstrations can improve final performance of agents on the Minecraft
minigame ObtainDiamond with only 8M frames of environment interaction. We
propose a training procedure where policy networks are first trained on human
data and later fine-tuned by reinforcement learning. Using a policy
exploitation mechanism, experience replay and an additional loss against
catastrophic forgetting, our best agent was able to achieve a mean score of 48.
Our proposed solution placed 3rd in the NeurIPS MineRL Competition for
Sample-Efficient Reinforcement Learning.","Christian Scheller, Yanick Schraner, Manfred Vogel",2020,http://arxiv.org/abs/2003.06066v1
Meta-Reinforcement Learning Using Model Parameters,"In meta-reinforcement learning, an agent is trained in multiple different
environments and attempts to learn a meta-policy that can efficiently adapt to
a new environment. This paper presents RAMP, a Reinforcement learning Agent
using Model Parameters that utilizes the idea that a neural network trained to
predict environment dynamics encapsulates the environment information. RAMP is
constructed in two phases: in the first phase, a multi-environment
parameterized dynamic model is learned. In the second phase, the model
parameters of the dynamic model are used as context for the multi-environment
policy of the model-free reinforcement learning agent.","Gabriel Hartmann, Amos Azaria",2022,http://arxiv.org/abs/2210.15515v1
"A proof of convergence of inverse reinforcement learning for
  multi-objective optimization","We show the convergence of Wasserstein inverse reinforcement learning for
multi-objective optimizations with the projective subgradient method by
formulating an inverse problem of the multi-objective optimization problem. In
addition, we prove convergence of inverse reinforcement learning (maximum
entropy inverse reinforcement learning, guided cost learning) with gradient
descent and the projective subgradient method.","Akira Kitaoka, Riki Eto",2023,http://arxiv.org/abs/2305.06137v3
"Review of Metrics to Measure the Stability, Robustness and Resilience of
  Reinforcement Learning","Reinforcement learning has received significant interest in recent years, due
primarily to the successes of deep reinforcement learning at solving many
challenging tasks such as playing Chess, Go and online computer games. However,
with the increasing focus on reinforcement learning, applications outside of
gaming and simulated environments require understanding the robustness,
stability, and resilience of reinforcement learning methods. To this end, we
conducted a comprehensive literature review to characterize the available
literature on these three behaviors as they pertain to reinforcement learning.
We classify the quantitative and theoretical approaches used to indicate or
measure robustness, stability, and resilience behaviors. In addition, we
identified the action or event to which the quantitative approaches were
attempting to be stable, robust, or resilient. Finally, we provide a decision
tree useful for selecting metrics to quantify the behaviors. We believe that
this is the first comprehensive review of stability, robustness and resilience
specifically geared towards reinforcement learning.",Laura L. Pullum,2022,http://arxiv.org/abs/2203.12048v1
"Deep Reinforcement Learning in Surgical Robotics: Enhancing the
  Automation Level","Surgical robotics is a rapidly evolving field that is transforming the
landscape of surgeries. Surgical robots have been shown to enhance precision,
minimize invasiveness, and alleviate surgeon fatigue. One promising area of
research in surgical robotics is the use of reinforcement learning to enhance
the automation level. Reinforcement learning is a type of machine learning that
involves training an agent to make decisions based on rewards and punishments.
This literature review aims to comprehensively analyze existing research on
reinforcement learning in surgical robotics. The review identified various
applications of reinforcement learning in surgical robotics, including
pre-operative, intra-body, and percutaneous procedures, listed the typical
studies, and compared their methodologies and results. The findings show that
reinforcement learning has great potential to improve the autonomy of surgical
robots. Reinforcement learning can teach robots to perform complex surgical
tasks, such as suturing and tissue manipulation. It can also improve the
accuracy and precision of surgical robots, making them more effective at
performing surgeries.","Cheng Qian, Hongliang Ren",2023,http://arxiv.org/abs/2309.00773v1
Topological Foundations of Reinforcement Learning,"The goal of this work is to serve as a foundation for deep studies of the
topology of state, action, and policy spaces in reinforcement learning. By
studying these spaces from a mathematical perspective, we expect to gain more
insight into how to build better algorithms to solve decision problems.
Therefore, we focus on presenting the connection between the Banach fixed point
theorem and the convergence of reinforcement learning algorithms, and we
illustrate how the insights gained from this can practically help in designing
more efficient algorithms. Before doing so, however, we first introduce
relevant concepts such as metric spaces, normed spaces and Banach spaces for
better understanding, before expressing the entire reinforcement learning
problem in terms of Markov decision processes. This allows us to properly
introduce the Banach contraction principle in a language suitable for
reinforcement learning, and to write the Bellman equations in terms of
operators on Banach spaces to show why reinforcement learning algorithms
converge. Finally, we show how the insights gained from the mathematical study
of convergence are helpful in reasoning about the best ways to make
reinforcement learning algorithms more efficient.",David Krame Kadurha,2024,http://arxiv.org/abs/2410.03706v1
"A Conceptual Framework for Externally-influenced Agents: An Assisted
  Reinforcement Learning Review","A long-term goal of reinforcement learning agents is to be able to perform
tasks in complex real-world scenarios. The use of external information is one
way of scaling agents to more complex problems. However, there is a general
lack of collaboration or interoperability between different approaches using
external information. In this work, while reviewing externally-influenced
methods, we propose a conceptual framework and taxonomy for assisted
reinforcement learning, aimed at fostering collaboration by classifying and
comparing various methods that use external information in the learning
process. The proposed taxonomy details the relationship between the external
information source and the learner agent, highlighting the process of
information decomposition, structure, retention, and how it can be used to
influence agent learning. As well as reviewing state-of-the-art methods, we
identify current streams of reinforcement learning that use external
information in order to improve the agent's performance and its decision-making
process. These include heuristic reinforcement learning, interactive
reinforcement learning, learning from demonstration, transfer learning, and
learning from multiple sources, among others. These streams of reinforcement
learning operate with the shared objective of scaffolding the learner agent.
Lastly, we discuss further possibilities for future work in the field of
assisted reinforcement learning systems.","Adam Bignold, Francisco Cruz, Matthew E. Taylor, Tim Brys, Richard Dazeley, Peter Vamplew, Cameron Foale",2020,http://arxiv.org/abs/2007.01544v2
A View on Deep Reinforcement Learning in System Optimization,"Many real-world systems problems require reasoning about the long term
consequences of actions taken to configure and manage the system. These
problems with delayed and often sequentially aggregated reward, are often
inherently reinforcement learning problems and present the opportunity to
leverage the recent substantial advances in deep reinforcement learning.
However, in some cases, it is not clear why deep reinforcement learning is a
good fit for the problem. Sometimes, it does not perform better than the
state-of-the-art solutions. And in other cases, random search or greedy
algorithms could outperform deep reinforcement learning. In this paper, we
review, discuss, and evaluate the recent trends of using deep reinforcement
learning in system optimization. We propose a set of essential metrics to guide
future works in evaluating the efficacy of using deep reinforcement learning in
system optimization. Our evaluation includes challenges, the types of problems,
their formulation in the deep reinforcement learning setting, embedding, the
model used, efficiency, and robustness. We conclude with a discussion on open
challenges and potential directions for pushing further the integration of
reinforcement learning in system optimization.","Ameer Haj-Ali, Nesreen K. Ahmed, Ted Willke, Joseph Gonzalez, Krste Asanovic, Ion Stoica",2019,http://arxiv.org/abs/1908.01275v3
"Prioritized Experience-based Reinforcement Learning with Human Guidance
  for Autonomous Driving","Reinforcement learning (RL) requires skillful definition and remarkable
computational efforts to solve optimization and control problems, which could
impair its prospect. Introducing human guidance into reinforcement learning is
a promising way to improve learning performance. In this paper, a comprehensive
human guidance-based reinforcement learning framework is established. A novel
prioritized experience replay mechanism that adapts to human guidance in the
reinforcement learning process is proposed to boost the efficiency and
performance of the reinforcement learning algorithm. To relieve the heavy
workload on human participants, a behavior model is established based on an
incremental online learning method to mimic human actions. We design two
challenging autonomous driving tasks for evaluating the proposed algorithm.
Experiments are conducted to access the training and testing performance and
learning mechanism of the proposed algorithm. Comparative results against the
state-of-the-art methods suggest the advantages of our algorithm in terms of
learning efficiency, performance, and robustness.","Jingda Wu, Zhiyu Huang, Wenhui Huang, Chen Lv",2021,http://arxiv.org/abs/2109.12516v2
"Learning Control Barrier Functions and their application in
  Reinforcement Learning: A Survey","Reinforcement learning is a powerful technique for developing new robot
behaviors. However, typical lack of safety guarantees constitutes a hurdle for
its practical application on real robots. To address this issue, safe
reinforcement learning aims to incorporate safety considerations, enabling
faster transfer to real robots and facilitating lifelong learning. One
promising approach within safe reinforcement learning is the use of control
barrier functions. These functions provide a framework to ensure that the
system remains in a safe state during the learning process. However,
synthesizing control barrier functions is not straightforward and often
requires ample domain knowledge. This challenge motivates the exploration of
data-driven methods for automatically defining control barrier functions, which
is highly appealing. We conduct a comprehensive review of the existing
literature on safe reinforcement learning using control barrier functions.
Additionally, we investigate various techniques for automatically learning the
Control Barrier Functions, aiming to enhance the safety and efficacy of
Reinforcement Learning in practical robot applications.","Maeva Guerrier, Hassan Fouad, Giovanni Beltrame",2024,http://arxiv.org/abs/2404.16879v1
"Continual Reinforcement Learning for HVAC Systems Control: Integrating
  Hypernetworks and Transfer Learning","Buildings with Heating, Ventilation, and Air Conditioning (HVAC) systems play
a crucial role in ensuring indoor comfort and efficiency. While traditionally
governed by physics-based models, the emergence of big data has enabled
data-driven methods like Deep Reinforcement Learning (DRL). However,
Reinforcement Learning (RL)-based techniques often suffer from sample
inefficiency and limited generalization, especially across varying HVAC
systems. We introduce a model-based reinforcement learning framework that uses
a Hypernetwork to continuously learn environment dynamics across tasks with
different action spaces. This enables efficient synthetic rollout generation
and improved sample usage. Our approach demonstrates strong backward transfer
in a continual learning setting after training on a second task, minimal
fine-tuning on the first task allows rapid convergence within just 5 episodes
and thus outperforming Model Free Reinforcement Learning (MFRL) and effectively
mitigating catastrophic forgetting. These findings have significant
implications for reducing energy consumption and operational costs in building
management, thus supporting global sustainability goals.
  Keywords: Deep Reinforcement Learning, HVAC Systems Control, Hypernetworks,
Transfer and Continual Learning, Catastrophic Forgetting","Gautham Udayakumar Bekal, Ahmed Ghareeb, Ashish Pujari",2025,http://arxiv.org/abs/2503.19212v1
Contrastive Abstraction for Reinforcement Learning,"Learning agents with reinforcement learning is difficult when dealing with
long trajectories that involve a large number of states. To address these
learning problems effectively, the number of states can be reduced by abstract
representations that cluster states. In principle, deep reinforcement learning
can find abstract states, but end-to-end learning is unstable. We propose
contrastive abstraction learning to find abstract states, where we assume that
successive states in a trajectory belong to the same abstract state. Such
abstract states may be basic locations, achieved subgoals, inventory, or health
conditions. Contrastive abstraction learning first constructs clusters of state
representations by contrastive learning and then applies modern Hopfield
networks to determine the abstract states. The first phase of contrastive
abstraction learning is self-supervised learning, where contrastive learning
forces states with sequential proximity to have similar representations. The
second phase uses modern Hopfield networks to map similar state representations
to the same fixed point, i.e.\ to an abstract state. The level of abstraction
can be adjusted by determining the number of fixed points of the modern
Hopfield network. Furthermore, \textit{contrastive abstraction learning} does
not require rewards and facilitates efficient reinforcement learning for a wide
range of downstream tasks. Our experiments demonstrate the effectiveness of
contrastive abstraction learning for reinforcement learning.","Vihang Patil, Markus Hofmarcher, Elisabeth Rumetshofer, Sepp Hochreiter",2024,http://arxiv.org/abs/2410.00704v1
"A Survey of Reinforcement Learning Techniques: Strategies, Recent
  Development, and Future Directions","Reinforcement learning is one of the core components in designing an
artificial intelligent system emphasizing real-time response. Reinforcement
learning influences the system to take actions within an arbitrary environment
either having previous knowledge about the environment model or not. In this
paper, we present a comprehensive study on Reinforcement Learning focusing on
various dimensions including challenges, the recent development of different
state-of-the-art techniques, and future directions. The fundamental objective
of this paper is to provide a framework for the presentation of available
methods of reinforcement learning that is informative enough and simple to
follow for the new researchers and academics in this domain considering the
latest concerns. First, we illustrated the core techniques of reinforcement
learning in an easily understandable and comparable way. Finally, we analyzed
and depicted the recent developments in reinforcement learning approaches. My
analysis pointed out that most of the models focused on tuning policy values
rather than tuning other things in a particular state of reasoning.",Amit Kumar Mondal,2020,http://arxiv.org/abs/2001.06921v2
Robust Reinforcement Learning on Graphs for Logistics optimization,"Logistics optimization nowadays is becoming one of the hottest areas in the
AI community. In the past year, significant advancements in the domain were
achieved by representing the problem in a form of graph. Another promising area
of research was to apply reinforcement learning algorithms to the above task.
In our work, we made advantage of using both approaches and apply reinforcement
learning on a graph. To do that, we have analyzed the most recent results in
both fields and selected SOTA algorithms both from graph neural networks and
reinforcement learning. Then, we combined selected models on the problem of
AMOD systems optimization for the transportation network of New York city. Our
team compared three algorithms - GAT, Pro-CNN and PTDNet - to bring to the fore
the important nodes on a graph representation. Finally, we achieved SOTA
results on AMOD systems optimization problem employing PTDNet with GNN and
training them in reinforcement fashion.
  Keywords: Graph Neural Network (GNN), Logistics optimization, Reinforcement
Learning","Zangir Iklassov, Dmitrii Medvedev",2022,http://arxiv.org/abs/2205.12888v1
Model-assisted Reinforcement Learning of a Quadrotor,"In recent times, reinforcement learning has produced baffling results when it
comes to performing control tasks with highly non-linear systems. The
impressive results always outweigh the potential vulnerabilities or
uncertainties associated with the agents when deployed in the real-world. While
the performance is remarkable compared to the classical control algorithms, the
reinforcement learning-based methods suffer from two flaws, robustness and
interpretability, which are vital for contemporary real-world applications. The
paper attempts to alleviate such problems with reinforcement learning and
proposes the concept of model-assisted reinforcement learning to induce a
notion of conservativeness in the agents. The control task considered for the
experiment involves navigating a CrazyFlie quadrotor. The paper also describes
a way of reformulating the task to have the flexibility of tuning the level of
conservativeness via multi-objective reinforcement learning. The results
include a comparison of the vanilla reinforcement learning approaches and the
proposed approach. The metrics are evaluated by systematically injecting
disturbances to classify the inherent robustness and conservativeness of the
agents. More concrete arguments are made by computing and comparing the
backward reachability tubes of the RL policies by solving the
Hamilton-Jacobi-Bellman partial differential equation (HJ PDE).",Arshad Javeed,2023,http://arxiv.org/abs/2311.06914v1
Reinforcement Learning and Machine ethics:a systematic review,"Machine ethics is the field that studies how ethical behaviour can be
accomplished by autonomous systems. While there exist some systematic reviews
aiming to consolidate the state of the art in machine ethics prior to 2020,
these tend to not include work that uses reinforcement learning agents as
entities whose ethical behaviour is to be achieved. The reason for this is that
only in the last years we have witnessed an increase in machine ethics studies
within reinforcement learning. We present here a systematic review of
reinforcement learning for machine ethics and machine ethics within
reinforcement learning. Additionally, we highlight trends in terms of ethics
specifications, components and frameworks of reinforcement learning, and
environments used to result in ethical behaviour. Our systematic review aims to
consolidate the work in machine ethics and reinforcement learning thus
completing the gap in the state of the art machine ethics landscape","Ajay Vishwanath, Louise A. Dennis, Marija Slavkovik",2024,http://arxiv.org/abs/2407.02425v1
Reinforcement Learning for IoT Security: A Comprehensive Survey,"The number of connected smart devices has been increasing exponentially for
different Internet-of-Things (IoT) applications. Security has been a long run
challenge in the IoT systems which has many attack vectors, security flaws and
vulnerabilities. Securing billions of B connected devices in IoT is a must task
to realize the full potential of IoT applications. Recently, researchers have
proposed many security solutions for IoT. Machine learning has been proposed as
one of the emerging solutions for IoT security and Reinforcement learning is
gaining more popularity for securing IoT systems. Reinforcement learning,
unlike other machine learning techniques, can learn the environment by having
minimum information about the parameters to be learned. It solves the
optimization problem by interacting with the environment adapting the
parameters on the fly. In this paper, we present an comprehensive survey of
different types of cyber-attacks against different IoT systems and then we
present reinforcement learning and deep reinforcement learning based security
solutions to combat those different types of attacks in different IoT systems.
Furthermore, we present the Reinforcement learning for securing CPS systems
(i.e., IoT with feedback and control) such as smart grid and smart
transportation system. The recent important attacks and countermeasures using
reinforcement learning B in IoT are also summarized in the form of tables. With
this paper, readers can have a more thorough understanding of IoT security
attacks and countermeasures using Reinforcement Learning, as well as research
trends in this area.","Aashma Uprety, Danda B. Rawat",2021,http://arxiv.org/abs/2102.07247v1
"A Survey on Physics Informed Reinforcement Learning: Review and Open
  Problems","The inclusion of physical information in machine learning frameworks has
revolutionized many application areas. This involves enhancing the learning
process by incorporating physical constraints and adhering to physical laws. In
this work we explore their utility for reinforcement learning applications. We
present a thorough review of the literature on incorporating physics
information, as known as physics priors, in reinforcement learning approaches,
commonly referred to as physics-informed reinforcement learning (PIRL). We
introduce a novel taxonomy with the reinforcement learning pipeline as the
backbone to classify existing works, compare and contrast them, and derive
crucial insights. Existing works are analyzed with regard to the
representation/ form of the governing physics modeled for integration, their
specific contribution to the typical reinforcement learning architecture, and
their connection to the underlying reinforcement learning pipeline stages. We
also identify core learning architectures and physics incorporation biases
(i.e., observational, inductive and learning) of existing PIRL approaches and
use them to further categorize the works for better understanding and
adaptation. By providing a comprehensive perspective on the implementation of
the physics-informed capability, the taxonomy presents a cohesive approach to
PIRL. It identifies the areas where this approach has been applied, as well as
the gaps and opportunities that exist. Additionally, the taxonomy sheds light
on unresolved issues and challenges, which can guide future research. This
nascent field holds great potential for enhancing reinforcement learning
algorithms by increasing their physical plausibility, precision, data
efficiency, and applicability in real-world scenarios.","Chayan Banerjee, Kien Nguyen, Clinton Fookes, Maziar Raissi",2023,http://arxiv.org/abs/2309.01909v1
"Non-Deterministic Policy Improvement Stabilizes Approximated
  Reinforcement Learning","This paper investigates a type of instability that is linked to the greedy
policy improvement in approximated reinforcement learning. We show empirically
that non-deterministic policy improvement can stabilize methods like LSPI by
controlling the improvements' stochasticity. Additionally we show that a
suitable representation of the value function also stabilizes the solution to
some degree. The presented approach is simple and should also be easily
transferable to more sophisticated algorithms like deep reinforcement learning.","Wendelin Böhmer, Rong Guo, Klaus Obermayer",2016,http://arxiv.org/abs/1612.07548v1
"Fine-grained acceleration control for autonomous intersection management
  using deep reinforcement learning","Recent advances in combining deep learning and Reinforcement Learning have
shown a promising path for designing new control agents that can learn optimal
policies for challenging control tasks. These new methods address the main
limitations of conventional Reinforcement Learning methods such as customized
feature engineering and small action/state space dimension requirements. In
this paper, we leverage one of the state-of-the-art Reinforcement Learning
methods, known as Trust Region Policy Optimization, to tackle intersection
management for autonomous vehicles. We show that using this method, we can
perform fine-grained acceleration control of autonomous vehicles in a grid
street plan to achieve a global design objective.","Hamid Mirzaei, Tony Givargis",2017,http://arxiv.org/abs/1705.10432v1
Anderson Acceleration for Reinforcement Learning,"Anderson acceleration is an old and simple method for accelerating the
computation of a fixed point. However, as far as we know and quite
surprisingly, it has never been applied to dynamic programming or reinforcement
learning. In this paper, we explain briefly what Anderson acceleration is and
how it can be applied to value iteration, this being supported by preliminary
experiments showing a significant speed up of convergence, that we critically
discuss. We also discuss how this idea could be applied more generally to
(deep) reinforcement learning.","Matthieu Geist, Bruno Scherrer",2018,http://arxiv.org/abs/1809.09501v1
Efficient Model-Free Reinforcement Learning Using Gaussian Process,"Efficient Reinforcement Learning usually takes advantage of demonstration or
good exploration strategy. By applying posterior sampling in model-free RL
under the hypothesis of GP, we propose Gaussian Process Posterior Sampling
Reinforcement Learning(GPPSTD) algorithm in continuous state space, giving
theoretical justifications and empirical results. We also provide theoretical
and empirical results that various demonstration could lower expected
uncertainty and benefit posterior sampling exploration. In this way, we
combined the demonstration and exploration process together to achieve a more
efficient reinforcement learning.","Ying Fan, Letian Chen, Yizhou Wang",2018,http://arxiv.org/abs/1812.04359v1
Optimizing Market Making using Multi-Agent Reinforcement Learning,"In this paper, reinforcement learning is applied to the problem of optimizing
market making. A multi-agent reinforcement learning framework is used to
optimally place limit orders that lead to successful trades. The framework
consists of two agents. The macro-agent optimizes on making the decision to
buy, sell, or hold an asset. The micro-agent optimizes on placing limit orders
within the limit order book. For the context of this paper, the proposed
framework is applied and studied on the Bitcoin cryptocurrency market. The goal
of this paper is to show that reinforcement learning is a viable strategy that
can be applied to complex problems (with complex environments) such as market
making.",Yagna Patel,2018,http://arxiv.org/abs/1812.10252v1
Is Epicurus the father of Reinforcement Learning?,"The Epicurean Philosophy is commonly thought as simplistic and hedonistic.
Here I discuss how this is a misconception and explore its link to
Reinforcement Learning. Based on the letters of Epicurus, I construct an
objective function for hedonism which turns out to be equivalent of the
Reinforcement Learning objective function when omitting the discount factor. I
then discuss how Plato and Aristotle 's views that can be also loosely linked
to Reinforcement Learning, as well as their weaknesses in relationship to it.
Finally, I emphasise the close affinity of the Epicurean views and the Bellman
equation.",Eleni Vasilaki,2017,http://arxiv.org/abs/1710.04582v1
"Optimization for Reinforcement Learning: From Single Agent to
  Cooperative Agents","This article reviews recent advances in multi-agent reinforcement learning
algorithms for large-scale control systems and communication networks, which
learn to communicate and cooperate. We provide an overview of this emerging
field, with an emphasis on the decentralized setting under different
coordination protocols. We highlight the evolution of reinforcement learning
algorithms from single-agent to multi-agent systems, from a distributed
optimization perspective, and conclude with future directions and challenges,
in the hope to catalyze the growing synergy among distributed optimization,
signal processing, and reinforcement learning communities.","Donghwan Lee, Niao He, Parameswaran Kamalaruban, Volkan Cevher",2019,http://arxiv.org/abs/1912.00498v1
"Curiosity-driven Exploration in Sparse-reward Multi-agent Reinforcement
  Learning","Sparsity of rewards while applying a deep reinforcement learning method
negatively affects its sample-efficiency. A viable solution to deal with the
sparsity of rewards is to learn via intrinsic motivation which advocates for
adding an intrinsic reward to the reward function to encourage the agent to
explore the environment and expand the sample space. Though intrinsic
motivation methods are widely used to improve data-efficient learning in the
reinforcement learning model, they also suffer from the so-called detachment
problem. In this article, we discuss the limitations of intrinsic curiosity
module in sparse-reward multi-agent reinforcement learning and propose a method
called I-Go-Explore that combines the intrinsic curiosity module with the
Go-Explore framework to alleviate the detachment problem.","Jiong Li, Pratik Gajane",2023,http://arxiv.org/abs/2302.10825v1
Deep reinforcement learning for process design: Review and perspective,"The transformation towards renewable energy and feedstock supply in the
chemical industry requires new conceptual process design approaches. Recently,
breakthroughs in artificial intelligence offer opportunities to accelerate this
transition. Specifically, deep reinforcement learning, a subclass of machine
learning, has shown the potential to solve complex decision-making problems and
aid sustainable process design. We survey state-of-the-art research in
reinforcement learning for process design through three major elements: (i)
information representation, (ii) agent architecture, and (iii) environment and
reward. Moreover, we discuss perspectives on underlying challenges and
promising future works to unfold the full potential of reinforcement learning
for process design in chemical engineering.","Qinghe Gao, Artur M. Schweidtmann",2023,http://arxiv.org/abs/2308.07822v1
Scalable Reinforcement Learning-based Neural Architecture Search,"In this publication, we assess the ability of a novel Reinforcement
Learning-based solution to the problem of Neural Architecture Search, where a
Reinforcement Learning (RL) agent learns to search for good architectures,
rather than to return a single optimal architecture. We consider both the
NAS-Bench-101 and NAS- Bench-301 settings, and compare against various known
strong baselines, such as local search and random search. We conclude that our
Reinforcement Learning agent displays strong scalability with regards to the
size of the search space, but limited robustness to hyperparameter changes.","Amber Cassimon, Siegfried Mercelis, Kevin Mets",2024,http://arxiv.org/abs/2410.01431v1
A Definition of Continual Reinforcement Learning,"In a standard view of the reinforcement learning problem, an agent's goal is
to efficiently identify a policy that maximizes long-term reward. However, this
perspective is based on a restricted view of learning as finding a solution,
rather than treating learning as endless adaptation. In contrast, continual
reinforcement learning refers to the setting in which the best agents never
stop learning. Despite the importance of continual reinforcement learning, the
community lacks a simple definition of the problem that highlights its
commitments and makes its primary concepts precise and clear. To this end, this
paper is dedicated to carefully defining the continual reinforcement learning
problem. We formalize the notion of agents that ""never stop learning"" through a
new mathematical language for analyzing and cataloging agents. Using this new
language, we define a continual learning agent as one that can be understood as
carrying out an implicit search process indefinitely, and continual
reinforcement learning as the setting in which the best agents are all
continual learning agents. We provide two motivating examples, illustrating
that traditional views of multi-task reinforcement learning and continual
supervised learning are special cases of our definition. Collectively, these
definitions and perspectives formalize many intuitive concepts at the heart of
learning, and open new research pathways surrounding continual learning agents.","David Abel, André Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, Satinder Singh",2023,http://arxiv.org/abs/2307.11046v2
"Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With
  Expert Demonstrations","Pretraining with expert demonstrations have been found useful in speeding up
the training process of deep reinforcement learning algorithms since less
online simulation data is required. Some people use supervised learning to
speed up the process of feature learning, others pretrain the policies by
imitating expert demonstrations. However, these methods are unstable and not
suitable for actor-critic reinforcement learning algorithms. Also, some
existing methods rely on the global optimum assumption, which is not true in
most scenarios. In this paper, we employ expert demonstrations in a
actor-critic reinforcement learning framework, and meanwhile ensure that the
performance is not affected by the fact that expert demonstrations are not
global optimal. We theoretically derive a method for computing policy gradients
and value estimators with only expert demonstrations. Our method is
theoretically plausible for actor-critic reinforcement learning algorithms that
pretrains both policy and value functions. We apply our method to two of the
typical actor-critic reinforcement learning algorithms, DDPG and ACER, and
demonstrate with experiments that our method not only outperforms the RL
algorithms without pretraining process, but also is more simulation efficient.","Xiaoqin Zhang, Huimin Ma",2018,http://arxiv.org/abs/1801.10459v2
"Using Deep Reinforcement Learning for the Continuous Control of Robotic
  Arms","Deep reinforcement learning enables algorithms to learn complex behavior,
deal with continuous action spaces and find good strategies in environments
with high dimensional state spaces. With deep reinforcement learning being an
active area of research and many concurrent inventions, we decided to focus on
a relatively simple robotic task to evaluate a set of ideas that might help to
solve recent reinforcement learning problems. We test a newly created
combination of two commonly used reinforcement learning methods, whether it is
able to learn more effectively than a baseline. We also compare different ideas
to preprocess information before it is fed to the reinforcement learning
algorithm. The goal of this strategy is to reduce training time and eventually
help the algorithm to converge. The concluding evaluation proves the general
applicability of the described concepts by testing them using a simulated
environment. These concepts might be reused for future experiments.",Winfried Lötzsch,2018,http://arxiv.org/abs/1810.06746v1
"Model-Reference Reinforcement Learning Control of Autonomous Surface
  Vehicles with Uncertainties","This paper presents a novel model-reference reinforcement learning control
method for uncertain autonomous surface vehicles. The proposed control combines
a conventional control method with deep reinforcement learning. With the
conventional control, we can ensure the learning-based control law provides
closed-loop stability for the overall system, and potentially increase the
sample efficiency of the deep reinforcement learning. With the reinforcement
learning, we can directly learn a control law to compensate for modeling
uncertainties. In the proposed control, a nominal system is employed for the
design of a baseline control law using a conventional control approach. The
nominal system also defines the desired performance for uncertain autonomous
vehicles to follow. In comparison with traditional deep reinforcement learning
methods, our proposed learning-based control can provide stability guarantees
and better sample efficiency. We demonstrate the performance of the new
algorithm via extensive simulation results.","Qingrui Zhang, Wei Pan, Vasso Reppa",2020,http://arxiv.org/abs/2003.13839v1
A Robotic Model of Hippocampal Reverse Replay for Reinforcement Learning,"Hippocampal reverse replay is thought to contribute to learning, and
particularly reinforcement learning, in animals. We present a computational
model of learning in the hippocampus that builds on a previous model of the
hippocampal-striatal network viewed as implementing a three-factor
reinforcement learning rule. To augment this model with hippocampal reverse
replay, a novel policy gradient learning rule is derived that associates place
cell activity with responses in cells representing actions. This new model is
evaluated using a simulated robot spatial navigation task inspired by the
Morris water maze. Results show that reverse replay can accelerate learning
from reinforcement, whilst improving stability and robustness over multiple
trials. As implied by the neurobiological data, our study implies that reverse
replay can make a significant positive contribution to reinforcement learning,
although learning that is less efficient and less stable is possible in its
absence. We conclude that reverse replay may enhance reinforcement learning in
the mammalian hippocampal-striatal system rather than provide its core
mechanism.","Matthew T. Whelan, Tony J. Prescott, Eleni Vasilaki",2021,http://arxiv.org/abs/2102.11914v1
Learning Robust Rewards with Adversarial Inverse Reinforcement Learning,"Reinforcement learning provides a powerful and general framework for decision
making and control, but its application in practice is often hindered by the
need for extensive feature and reward engineering. Deep reinforcement learning
methods can remove the need for explicit engineering of policy or value
features, but still require a manually specified reward function. Inverse
reinforcement learning holds the promise of automatic reward acquisition, but
has proven exceptionally difficult to apply to large, high-dimensional problems
with unknown dynamics. In this work, we propose adverserial inverse
reinforcement learning (AIRL), a practical and scalable inverse reinforcement
learning algorithm based on an adversarial reward learning formulation. We
demonstrate that AIRL is able to recover reward functions that are robust to
changes in dynamics, enabling us to learn policies even under significant
variation in the environment seen during training. Our experiments show that
AIRL greatly outperforms prior methods in these transfer settings.","Justin Fu, Katie Luo, Sergey Levine",2017,http://arxiv.org/abs/1710.11248v2
