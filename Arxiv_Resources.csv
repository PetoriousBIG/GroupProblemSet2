Title,Abstract,Authors,Year,URL
Lecture Notes: Optimization for Machine Learning,"Lecture notes on optimization for machine learning, derived from a course at
Princeton University and tutorials given in MLSS, Buenos Aires, as well as
Simons Foundation, Berkeley.",Elad Hazan,2019,http://arxiv.org/abs/1909.03550v1
An Optimal Control View of Adversarial Machine Learning,"I describe an optimal control view of adversarial machine learning, where the
dynamical system is the machine learner, the input are adversarial actions, and
the control costs are defined by the adversary's goals to do harm and be hard
to detect. This view encompasses many types of adversarial machine learning,
including test-item attacks, training-data poisoning, and adversarial reward
shaping. The view encourages adversarial machine learning researcher to utilize
advances in control theory and reinforcement learning.",Xiaojin Zhu,2018,http://arxiv.org/abs/1811.04422v1
"Minimax deviation strategies for machine learning and recognition with
  short learning samples","The article is devoted to the problem of small learning samples in machine
learning. The flaws of maximum likelihood learning and minimax learning are
looked into and the concept of minimax deviation learning is introduced that is
free of those flaws.","Michail Schlesinger, Evgeniy Vodolazskiy",2017,http://arxiv.org/abs/1707.04849v1
Machine Learning for Clinical Predictive Analytics,"In this chapter, we provide a brief overview of applying machine learning
techniques for clinical prediction tasks. We begin with a quick introduction to
the concepts of machine learning and outline some of the most common machine
learning algorithms. Next, we demonstrate how to apply the algorithms with
appropriate toolkits to conduct machine learning experiments for clinical
prediction tasks. The objectives of this chapter are to (1) understand the
basics of machine learning techniques and the reasons behind why they are
useful for solving clinical prediction problems, (2) understand the intuition
behind some machine learning models, including regression, decision trees, and
support vector machines, and (3) understand how to apply these models to
clinical prediction problems using publicly available datasets via case
studies.",Wei-Hung Weng,2019,http://arxiv.org/abs/1909.09246v1
"Towards Modular Machine Learning Solution Development: Benefits and
  Trade-offs","Machine learning technologies have demonstrated immense capabilities in
various domains. They play a key role in the success of modern businesses.
However, adoption of machine learning technologies has a lot of untouched
potential. Cost of developing custom machine learning solutions that solve
unique business problems is a major inhibitor to far-reaching adoption of
machine learning technologies. We recognize that the monolithic nature
prevalent in today's machine learning applications stands in the way of
efficient and cost effective customized machine learning solution development.
In this work we explore the benefits of modular machine learning solutions and
discuss how modular machine learning solutions can overcome some of the major
solution engineering limitations of monolithic machine learning solutions. We
analyze the trade-offs between modular and monolithic machine learning
solutions through three deep learning problems; one text based and the two
image based. Our experimental results show that modular machine learning
solutions have a promising potential to reap the solution engineering
advantages of modularity while gaining performance and data advantages in a way
the monolithic machine learning solutions do not permit.","Samiyuru Menik, Lakshmish Ramaswamy",2023,http://arxiv.org/abs/2301.09753v1
The Tribes of Machine Learning and the Realm of Computer Architecture,"Machine learning techniques have influenced the field of computer
architecture like many other fields. This paper studies how the fundamental
machine learning techniques can be applied towards computer architecture
problems. We also provide a detailed survey of computer architecture research
that employs different machine learning methods. Finally, we present some
future opportunities and the outstanding challenges that need to be overcome to
exploit full potential of machine learning for computer architecture.","Ayaz Akram, Jason Lowe-Power",2020,http://arxiv.org/abs/2012.04105v1
"A Machine Learning Tutorial for Operational Meteorology, Part I:
  Traditional Machine Learning","Recently, the use of machine learning in meteorology has increased greatly.
While many machine learning methods are not new, university classes on machine
learning are largely unavailable to meteorology students and are not required
to become a meteorologist. The lack of formal instruction has contributed to
perception that machine learning methods are 'black boxes' and thus end-users
are hesitant to apply the machine learning methods in their every day workflow.
To reduce the opaqueness of machine learning methods and lower hesitancy
towards machine learning in meteorology, this paper provides a survey of some
of the most common machine learning methods. A familiar meteorological example
is used to contextualize the machine learning methods while also discussing
machine learning topics using plain language. The following machine learning
methods are demonstrated: linear regression; logistic regression; decision
trees; random forest; gradient boosted decision trees; naive Bayes; and support
vector machines. Beyond discussing the different methods, the paper also
contains discussions on the general machine learning process as well as best
practices to enable readers to apply machine learning to their own datasets.
Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory
notebooks) used to make the examples in the paper is provided in an effort to
catalyse the use of machine learning in meteorology.","Randy J. Chase, David R. Harrison, Amanda Burke, Gary M. Lackmann, Amy McGovern",2022,http://arxiv.org/abs/2204.07492v2
Position Paper: Towards Transparent Machine Learning,"Transparent machine learning is introduced as an alternative form of machine
learning, where both the model and the learning system are represented in
source code form. The goal of this project is to enable direct human
understanding of machine learning models, giving us the ability to learn,
verify, and refine them as programs. If solved, this technology could represent
a best-case scenario for the safety and security of AI systems going forward.",Dustin Juliano,2019,http://arxiv.org/abs/1911.06612v1
Understanding Bias in Machine Learning,"Bias is known to be an impediment to fair decisions in many domains such as
human resources, the public sector, health care etc. Recently, hope has been
expressed that the use of machine learning methods for taking such decisions
would diminish or even resolve the problem. At the same time, machine learning
experts warn that machine learning models can be biased as well. In this
article, our goal is to explain the issue of bias in machine learning from a
technical perspective and to illustrate the impact that biased data can have on
a machine learning model. To reach such a goal, we develop interactive plots to
visualizing the bias learned from synthetic data.","Jindong Gu, Daniela Oelke",2019,http://arxiv.org/abs/1909.01866v1
"A Unified Analytical Framework for Trustable Machine Learning and
  Automation Running with Blockchain","Traditional machine learning algorithms use data from databases that are
mutable, and therefore the data cannot be fully trusted. Also, the machine
learning process is difficult to automate. This paper proposes building a
trustable machine learning system by using blockchain technology, which can
store data in a permanent and immutable way. In addition, smart contracts are
used to automate the machine learning process. This paper makes three
contributions. First, it establishes a link between machine learning technology
and blockchain technology. Previously, machine learning and blockchain have
been considered two independent technologies without an obvious link. Second,
it proposes a unified analytical framework for trustable machine learning by
using blockchain technology. This unified framework solves both the
trustability and automation issues in machine learning. Third, it enables a
computer to translate core machine learning implementation from a single thread
on a single machine to multiple threads on multiple machines running with
blockchain by using a unified approach. The paper uses association rule mining
as an example to demonstrate how trustable machine learning can be implemented
with blockchain, and it shows how this approach can be used to analyze opioid
prescriptions to help combat the opioid crisis.",Tao Wang,2019,http://arxiv.org/abs/1903.08801v1
"MLBench: How Good Are Machine Learning Clouds for Binary Classification
  Tasks on Structured Data?","We conduct an empirical study of machine learning functionalities provided by
major cloud service providers, which we call machine learning clouds. Machine
learning clouds hold the promise of hiding all the sophistication of running
large-scale machine learning: Instead of specifying how to run a machine
learning task, users only specify what machine learning task to run and the
cloud figures out the rest. Raising the level of abstraction, however, rarely
comes free - a performance penalty is possible. How good, then, are current
machine learning clouds on real-world machine learning workloads?
  We study this question with a focus on binary classication problems. We
present mlbench, a novel benchmark constructed by harvesting datasets from
Kaggle competitions. We then compare the performance of the top winning code
available from Kaggle with that of running machine learning clouds from both
Azure and Amazon on mlbench. Our comparative study reveals the strength and
weakness of existing machine learning clouds and points out potential future
directions for improvement.","Yu Liu, Hantian Zhang, Luyuan Zeng, Wentao Wu, Ce Zhang",2017,http://arxiv.org/abs/1707.09562v3
Data Pricing in Machine Learning Pipelines,"Machine learning is disruptive. At the same time, machine learning can only
succeed by collaboration among many parties in multiple steps naturally as
pipelines in an eco-system, such as collecting data for possible machine
learning applications, collaboratively training models by multiple parties and
delivering machine learning services to end users. Data is critical and
penetrating in the whole machine learning pipelines. As machine learning
pipelines involve many parties and, in order to be successful, have to form a
constructive and dynamic eco-system, marketplaces and data pricing are
fundamental in connecting and facilitating those many parties. In this article,
we survey the principles and the latest research development of data pricing in
machine learning pipelines. We start with a brief review of data marketplaces
and pricing desiderata. Then, we focus on pricing in three important steps in
machine learning pipelines. To understand pricing in the step of training data
collection, we review pricing raw data sets and data labels. We also
investigate pricing in the step of collaborative training of machine learning
models, and overview pricing machine learning models for end users in the step
of machine learning deployment. We also discuss a series of possible future
directions.","Zicun Cong, Xuan Luo, Pei Jian, Feida Zhu, Yong Zhang",2021,http://arxiv.org/abs/2108.07915v1
Techniques for Automated Machine Learning,"Automated machine learning (AutoML) aims to find optimal machine learning
solutions automatically given a machine learning problem. It could release the
burden of data scientists from the multifarious manual tuning process and
enable the access of domain experts to the off-the-shelf machine learning
solutions without extensive experience. In this paper, we review the current
developments of AutoML in terms of three categories, automated feature
engineering (AutoFE), automated model and hyperparameter learning (AutoMHL),
and automated deep learning (AutoDL). State-of-the-art techniques adopted in
the three categories are presented, including Bayesian optimization,
reinforcement learning, evolutionary algorithm, and gradient-based approaches.
We summarize popular AutoML frameworks and conclude with current open
challenges of AutoML.","Yi-Wei Chen, Qingquan Song, Xia Hu",2019,http://arxiv.org/abs/1907.08908v1
"The Landscape of Modern Machine Learning: A Review of Machine,
  Distributed and Federated Learning","With the advance of the powerful heterogeneous, parallel and distributed
computing systems and ever increasing immense amount of data, machine learning
has become an indispensable part of cutting-edge technology, scientific
research and consumer products. In this study, we present a review of modern
machine and deep learning. We provide a high-level overview for the latest
advanced machine learning algorithms, applications, and frameworks. Our
discussion encompasses parallel distributed learning, deep learning as well as
federated learning. As a result, our work serves as an introductory text to the
vast field of modern machine learning.","Omer Subasi, Oceane Bel, Joseph Manzano, Kevin Barker",2023,http://arxiv.org/abs/2312.03120v1
"Parallelization of Machine Learning Algorithms Respectively on Single
  Machine and Spark","With the rapid development of big data technologies, how to dig out useful
information from massive data becomes an essential problem. However, using
machine learning algorithms to analyze large data may be time-consuming and
inefficient on the traditional single machine. To solve these problems, this
paper has made some research on the parallelization of several classic machine
learning algorithms respectively on the single machine and the big data
platform Spark. We compare the runtime and efficiency of traditional machine
learning algorithms with parallelized machine learning algorithms respectively
on the single machine and Spark platform. The research results have shown
significant improvement in runtime and efficiency of parallelized machine
learning algorithms.",Jiajun Shen,2022,http://arxiv.org/abs/2206.07090v2
AutoCompete: A Framework for Machine Learning Competition,"In this paper, we propose AutoCompete, a highly automated machine learning
framework for tackling machine learning competitions. This framework has been
learned by us, validated and improved over a period of more than two years by
participating in online machine learning competitions. It aims at minimizing
human interference required to build a first useful predictive model and to
assess the practical difficulty of a given machine learning challenge. The
proposed system helps in identifying data types, choosing a machine learn- ing
model, tuning hyper-parameters, avoiding over-fitting and optimization for a
provided evaluation metric. We also observe that the proposed system produces
better (or comparable) results with less runtime as compared to other
approaches.","Abhishek Thakur, Artus Krohn-Grimberghe",2015,http://arxiv.org/abs/1507.02188v1
"Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in
  Social Good Applications","This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning
in Social Good Applications, which was held on June 24, 2016 in New York.",Kush R. Varshney,2016,http://arxiv.org/abs/1607.02450v2
Mathematical Perspective of Machine Learning,"We take a closer look at some theoretical challenges of Machine Learning as a
function approximation, gradient descent as the default optimization algorithm,
limitations of fixed length and width networks and a different approach to RNNs
from a mathematical perspective.",Yarema Boryshchak,2020,http://arxiv.org/abs/2007.01503v1
Private Machine Learning via Randomised Response,"We introduce a general learning framework for private machine learning based
on randomised response. Our assumption is that all actors are potentially
adversarial and as such we trust only to release a single noisy version of an
individual's datapoint. We discuss a general approach that forms a consistent
way to estimate the true underlying machine learning model and demonstrate this
in the case of logistic regression.",David Barber,2020,http://arxiv.org/abs/2001.04942v2
A Survey of Optimization Methods from a Machine Learning Perspective,"Machine learning develops rapidly, which has made many theoretical
breakthroughs and is widely applied in various fields. Optimization, as an
important part of machine learning, has attracted much attention of
researchers. With the exponential growth of data amount and the increase of
model complexity, optimization methods in machine learning face more and more
challenges. A lot of work on solving optimization problems or improving
optimization methods in machine learning has been proposed successively. The
systematic retrospect and summary of the optimization methods from the
perspective of machine learning are of great significance, which can offer
guidance for both developments of optimization and machine learning research.
In this paper, we first describe the optimization problems in machine learning.
Then, we introduce the principles and progresses of commonly used optimization
methods. Next, we summarize the applications and developments of optimization
methods in some popular machine learning fields. Finally, we explore and give
some challenges and open problems for the optimization in machine learning.","Shiliang Sun, Zehui Cao, Han Zhu, Jing Zhao",2019,http://arxiv.org/abs/1906.06821v2
Ten-year Survival Prediction for Breast Cancer Patients,"This report assesses different machine learning approaches to 10-year
survival prediction of breast cancer patients.","Changmao Li, Han He, Yunze Hao, Caleb Ziems",2019,http://arxiv.org/abs/1911.00776v1
When Machine Learning Meets Privacy: A Survey and Outlook,"The newly emerged machine learning (e.g. deep learning) methods have become a
strong driving force to revolutionize a wide range of industries, such as smart
healthcare, financial technology, and surveillance systems. Meanwhile, privacy
has emerged as a big concern in this machine learning-based artificial
intelligence era. It is important to note that the problem of privacy
preservation in the context of machine learning is quite different from that in
traditional data privacy protection, as machine learning can act as both friend
and foe. Currently, the work on the preservation of privacy and machine
learning (ML) is still in an infancy stage, as most existing solutions only
focus on privacy problems during the machine learning process. Therefore, a
comprehensive study on the privacy preservation problems and machine learning
is required. This paper surveys the state of the art in privacy issues and
solutions for machine learning. The survey covers three categories of
interactions between privacy and machine learning: (i) private machine
learning, (ii) machine learning aided privacy protection, and (iii) machine
learning-based privacy attack and corresponding protection schemes. The current
research progress in each category is reviewed and the key challenges are
identified. Finally, based on our in-depth analysis of the area of privacy and
machine learning, we point out future research directions in this field.","Bo Liu, Ming Ding, Sina Shaham, Wenny Rahayu, Farhad Farokhi, Zihuai Lin",2020,http://arxiv.org/abs/2011.11819v1
Augmented Q Imitation Learning (AQIL),"The study of unsupervised learning can be generally divided into two
categories: imitation learning and reinforcement learning. In imitation
learning the machine learns by mimicking the behavior of an expert system
whereas in reinforcement learning the machine learns via direct environment
feedback. Traditional deep reinforcement learning takes a significant time
before the machine starts to converge to an optimal policy. This paper proposes
Augmented Q-Imitation-Learning, a method by which deep reinforcement learning
convergence can be accelerated by applying Q-imitation-learning as the initial
training process in traditional Deep Q-learning.","Xiao Lei Zhang, Anish Agarwal",2020,http://arxiv.org/abs/2004.00993v2
Probabilistic Machine Learning for Healthcare,"Machine learning can be used to make sense of healthcare data. Probabilistic
machine learning models help provide a complete picture of observed data in
healthcare. In this review, we examine how probabilistic machine learning can
advance healthcare. We consider challenges in the predictive model building
pipeline where probabilistic models can be beneficial including calibration and
missing data. Beyond predictive models, we also investigate the utility of
probabilistic machine learning models in phenotyping, in generative models for
clinical use cases, and in reinforcement learning.","Irene Y. Chen, Shalmali Joshi, Marzyeh Ghassemi, Rajesh Ranganath",2020,http://arxiv.org/abs/2009.11087v1
Evaluation Challenges for Geospatial ML,"As geospatial machine learning models and maps derived from their predictions
are increasingly used for downstream analyses in science and policy, it is
imperative to evaluate their accuracy and applicability. Geospatial machine
learning has key distinctions from other learning paradigms, and as such, the
correct way to measure performance of spatial machine learning outputs has been
a topic of debate. In this paper, I delineate unique challenges of model
evaluation for geospatial machine learning with global or remotely sensed
datasets, culminating in concrete takeaways to improve evaluations of
geospatial model performance.",Esther Rolf,2023,http://arxiv.org/abs/2303.18087v1
"A comprehensive review of Quantum Machine Learning: from NISQ to Fault
  Tolerance","Quantum machine learning, which involves running machine learning algorithms
on quantum devices, has garnered significant attention in both academic and
business circles. In this paper, we offer a comprehensive and unbiased review
of the various concepts that have emerged in the field of quantum machine
learning. This includes techniques used in Noisy Intermediate-Scale Quantum
(NISQ) technologies and approaches for algorithms compatible with
fault-tolerant quantum computing hardware. Our review covers fundamental
concepts, algorithms, and the statistical learning theory pertinent to quantum
machine learning.","Yunfei Wang, Junyu Liu",2024,http://arxiv.org/abs/2401.11351v2
"Towards CRISP-ML(Q): A Machine Learning Process Model with Quality
  Assurance Methodology","Machine learning is an established and frequently used technique in industry
and academia but a standard process model to improve success and efficiency of
machine learning applications is still missing. Project organizations and
machine learning practitioners have a need for guidance throughout the life
cycle of a machine learning application to meet business expectations. We
therefore propose a process model for the development of machine learning
applications, that covers six phases from defining the scope to maintaining the
deployed machine learning application. The first phase combines business and
data understanding as data availability oftentimes affects the feasibility of
the project. The sixth phase covers state-of-the-art approaches for monitoring
and maintenance of a machine learning applications, as the risk of model
degradation in a changing environment is eminent. With each task of the
process, we propose quality assurance methodology that is suitable to adress
challenges in machine learning development that we identify in form of risks.
The methodology is drawn from practical experience and scientific literature
and has proven to be general and stable. The process model expands on CRISP-DM,
a data mining process model that enjoys strong industry support but lacks to
address machine learning specific tasks. Our work proposes an industry and
application neutral process model tailored for machine learning applications
with focus on technical tasks for quality assurance.","Stefan Studer, Thanh Binh Bui, Christian Drescher, Alexander Hanuschkin, Ludwig Winkler, Steven Peters, Klaus-Robert Mueller",2020,http://arxiv.org/abs/2003.05155v2
"Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of
  learning relational order via reinforcement learning procedure?","In this article, we extend the conventional framework of
convolutional-Restricted-Boltzmann-Machine to learn highly abstract features
among abitrary number of time related input maps by constructing a layer of
multiplicative units, which capture the relations among inputs. In many cases,
more than two maps are strongly related, so it is wise to make multiplicative
unit learn relations among more input maps, in other words, to find the optimal
relational-order of each unit. In order to enable our machine to learn
relational order, we developed a reinforcement-learning method whose optimality
is proven to train the network.",Zizhuang Wang,2017,http://arxiv.org/abs/1706.08001v1
Spatial Transfer Learning with Simple MLP,"First step to investigate the potential of transfer learning applied to the
field of spatial statistics",Hongjian Yang,2024,http://arxiv.org/abs/2405.03720v1
Distributed Multi-Task Learning with Shared Representation,"We study the problem of distributed multi-task learning with shared
representation, where each machine aims to learn a separate, but related, task
in an unknown shared low-dimensional subspaces, i.e. when the predictor matrix
has low rank. We consider a setting where each task is handled by a different
machine, with samples for the task available locally on the machine, and study
communication-efficient methods for exploiting the shared structure.","Jialei Wang, Mladen Kolar, Nathan Srebro",2016,http://arxiv.org/abs/1603.02185v1
Components of Machine Learning: Binding Bits and FLOPS,"Many machine learning problems and methods are combinations of three
components: data, hypothesis space and loss function. Different machine
learning methods are obtained as combinations of different choices for the
representation of data, hypothesis space and loss function. After reviewing the
mathematical structure of these three components, we discuss intrinsic
trade-offs between statistical and computational properties of machine learning
methods.",Alexander Jung,2019,http://arxiv.org/abs/1910.12387v2
Impact of Legal Requirements on Explainability in Machine Learning,"The requirements on explainability imposed by European laws and their
implications for machine learning (ML) models are not always clear. In that
perspective, our research analyzes explanation obligations imposed for private
and public decision-making, and how they can be implemented by machine learning
techniques.","Adrien Bibal, Michael Lognoul, Alexandre de Streel, Benoît Frénay",2020,http://arxiv.org/abs/2007.05479v1
Machine Learning Potential Repository,"This paper introduces a machine learning potential repository that includes
Pareto optimal machine learning potentials. It also shows the systematic
development of accurate and fast machine learning potentials for a wide range
of elemental systems. As a result, many Pareto optimal machine learning
potentials are available in the repository from a website. Therefore, the
repository will help many scientists to perform accurate and fast atomistic
simulations.",Atsuto Seko,2020,http://arxiv.org/abs/2007.14206v1
Quantum memristors for neuromorphic quantum machine learning,"Quantum machine learning may permit to realize more efficient machine
learning calculations with near-term quantum devices. Among the diverse quantum
machine learning paradigms which are currently being considered, quantum
memristors are promising as a way of combining, in the same quantum hardware, a
unitary evolution with the nonlinearity provided by the measurement and
feedforward. Thus, an efficient way of deploying neuromorphic quantum computing
for quantum machine learning may be enabled.",Lucas Lamata,2024,http://arxiv.org/abs/2412.18979v1
metric-learn: Metric Learning Algorithms in Python,"metric-learn is an open source Python package implementing supervised and
weakly-supervised distance metric learning algorithms. As part of
scikit-learn-contrib, it provides a unified interface compatible with
scikit-learn which allows to easily perform cross-validation, model selection,
and pipelining with other machine learning estimators. metric-learn is
thoroughly tested and available on PyPi under the MIT licence.","William de Vazelhes, CJ Carey, Yuan Tang, Nathalie Vauquier, Aurélien Bellet",2019,http://arxiv.org/abs/1908.04710v3
Theoretical Models of Learning to Learn,"A Machine can only learn if it is biased in some way. Typically the bias is
supplied by hand, for example through the choice of an appropriate set of
features. However, if the learning machine is embedded within an {\em
environment} of related tasks, then it can {\em learn} its own bias by learning
sufficiently many tasks from the environment. In this paper two models of bias
learning (or equivalently, learning to learn) are introduced and the main
theoretical results presented. The first model is a PAC-type model based on
empirical process theory, while the second is a hierarchical Bayes model.",Jonathan Baxter,2020,http://arxiv.org/abs/2002.12364v1
On-the-Fly Learning in a Perpetual Learning Machine,"Despite the promise of brain-inspired machine learning, deep neural networks
(DNN) have frustratingly failed to bridge the deceptively large gap between
learning and memory. Here, we introduce a Perpetual Learning Machine; a new
type of DNN that is capable of brain-like dynamic 'on the fly' learning because
it exists in a self-supervised state of Perpetual Stochastic Gradient Descent.
Thus, we provide the means to unify learning and memory within a machine
learning framework. We also explore the elegant duality of abstraction and
synthesis: the Yin and Yang of deep learning.",Andrew J. R. Simpson,2015,http://arxiv.org/abs/1509.00913v3
"An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality
  in Machine Learning","We propose a clustering-based iterative algorithm to solve certain
optimization problems in machine learning, where we start the algorithm by
aggregating the original data, solving the problem on aggregated data, and then
in subsequent steps gradually disaggregate the aggregated data. We apply the
algorithm to common machine learning problems such as the least absolute
deviation regression problem, support vector machines, and semi-supervised
support vector machines. We derive model-specific data aggregation and
disaggregation procedures. We also show optimality, convergence, and the
optimality gap of the approximated solution in each iteration. A computational
study is provided.","Young Woong Park, Diego Klabjan",2016,http://arxiv.org/abs/1607.01400v1
Human-in-the-loop Machine Learning: A Macro-Micro Perspective,"Though technical advance of artificial intelligence and machine learning has
enabled many promising intelligent systems, many computing tasks are still not
able to be fully accomplished by machine intelligence. Motivated by the
complementary nature of human and machine intelligence, an emerging trend is to
involve humans in the loop of machine learning and decision-making. In this
paper, we provide a macro-micro review of human-in-the-loop machine learning.
We first describe major machine learning challenges which can be addressed by
human intervention in the loop. Then we examine closely the latest research and
findings of introducing humans into each step of the lifecycle of machine
learning. Finally, we analyze current research gaps and point out future
research directions.","Jiangtao Wang, Bin Guo, Liming Chen",2022,http://arxiv.org/abs/2202.10564v1
Can Machines Learn the True Probabilities?,"When there exists uncertainty, AI machines are designed to make decisions so
as to reach the best expected outcomes. Expectations are based on true facts
about the objective environment the machines interact with, and those facts can
be encoded into AI models in the form of true objective probability functions.
Accordingly, AI models involve probabilistic machine learning in which the
probabilities should be objectively interpreted. We prove under some basic
assumptions when machines can learn the true objective probabilities, if any,
and when machines cannot learn them.",Jinsook Kim,2024,http://arxiv.org/abs/2407.05526v1
Scientific Machine Learning Benchmarks,"The breakthrough in Deep Learning neural networks has transformed the use of
AI and machine learning technologies for the analysis of very large
experimental datasets. These datasets are typically generated by large-scale
experimental facilities at national laboratories. In the context of science,
scientific machine learning focuses on training machines to identify patterns,
trends, and anomalies to extract meaningful scientific insights from such
datasets. With a new generation of experimental facilities, the rate of data
generation and the scale of data volumes will increasingly require the use of
more automated data analysis. At present, identifying the most appropriate
machine learning algorithm for the analysis of any given scientific dataset is
still a challenge for scientists. This is due to many different machine
learning frameworks, computer architectures, and machine learning models.
Historically, for modelling and simulation on HPC systems such problems have
been addressed through benchmarking computer applications, algorithms, and
architectures. Extending such a benchmarking approach and identifying metrics
for the application of machine learning methods to scientific datasets is a new
challenge for both scientists and computer scientists. In this paper, we
describe our approach to the development of scientific machine learning
benchmarks and review other approaches to benchmarking scientific machine
learning.","Jeyan Thiyagalingam, Mallikarjun Shankar, Geoffrey Fox, Tony Hey",2021,http://arxiv.org/abs/2110.12773v1
Some Insights into Lifelong Reinforcement Learning Systems,"A lifelong reinforcement learning system is a learning system that has the
ability to learn through trail-and-error interaction with the environment over
its lifetime. In this paper, I give some arguments to show that the traditional
reinforcement learning paradigm fails to model this type of learning system.
Some insights into lifelong reinforcement learning are provided, along with a
simplistic prototype lifelong reinforcement learning system.",Changjian Li,2020,http://arxiv.org/abs/2001.09608v1
Bayesian Optimization for Machine Learning : A Practical Guidebook,"The engineering of machine learning systems is still a nascent field; relying
on a seemingly daunting collection of quickly evolving tools and best
practices. It is our hope that this guidebook will serve as a useful resource
for machine learning practitioners looking to take advantage of Bayesian
optimization techniques. We outline four example machine learning problems that
can be solved using open source machine learning libraries, and highlight the
benefits of using Bayesian optimization in the context of these common machine
learning applications.","Ian Dewancker, Michael McCourt, Scott Clark",2016,http://arxiv.org/abs/1612.04858v1
Towards A Rigorous Science of Interpretable Machine Learning,"As machine learning systems become ubiquitous, there has been a surge of
interest in interpretable machine learning: systems that provide explanation
for their outputs. These explanations are often used to qualitatively assess
other criteria such as safety or non-discrimination. However, despite the
interest in interpretability, there is very little consensus on what
interpretable machine learning is and how it should be measured. In this
position paper, we first define interpretability and describe when
interpretability is needed (and when it is not). Next, we suggest a taxonomy
for rigorous evaluation and expose open questions towards a more rigorous
science of interpretable machine learning.","Finale Doshi-Velez, Been Kim",2017,http://arxiv.org/abs/1702.08608v2
Infrastructure for Usable Machine Learning: The Stanford DAWN Project,"Despite incredible recent advances in machine learning, building machine
learning applications remains prohibitively time-consuming and expensive for
all but the best-trained, best-funded engineering organizations. This expense
comes not from a need for new and improved statistical models but instead from
a lack of systems and tools for supporting end-to-end machine learning
application development, from data preparation and labeling to
productionization and monitoring. In this document, we outline opportunities
for infrastructure supporting usable, end-to-end machine learning applications
in the context of the nascent DAWN (Data Analytics for What's Next) project at
Stanford.","Peter Bailis, Kunle Olukotun, Christopher Re, Matei Zaharia",2017,http://arxiv.org/abs/1705.07538v2
Techniques for Interpretable Machine Learning,"Interpretable machine learning tackles the important problem that humans
cannot understand the behaviors of complex machine learning models and how
these models arrive at a particular decision. Although many approaches have
been proposed, a comprehensive understanding of the achievements and challenges
is still lacking. We provide a survey covering existing techniques to increase
the interpretability of machine learning models. We also discuss crucial issues
that the community should consider in future work such as designing
user-friendly explanations and developing comprehensive evaluation metrics to
further push forward the area of interpretable machine learning.","Mengnan Du, Ninghao Liu, Xia Hu",2018,http://arxiv.org/abs/1808.00033v3
Solving machine learning optimization problems using quantum computers,"Classical optimization algorithms in machine learning often take a long time
to compute when applied to a multi-dimensional problem and require a huge
amount of CPU and GPU resource. Quantum parallelism has a potential to speed up
machine learning algorithms. We describe a generic mathematical model to
leverage quantum parallelism to speed-up machine learning algorithms. We also
apply quantum machine learning and quantum parallelism applied to a
$3$-dimensional image that vary with time.","Venkat R. Dasari, Mee Seong Im, Lubjana Beshaj",2019,http://arxiv.org/abs/1911.08587v1
Opening the black box of deep learning,"The great success of deep learning shows that its technology contains
profound truth, and understanding its internal mechanism not only has important
implications for the development of its technology and effective application in
various fields, but also provides meaningful insights into the understanding of
human brain mechanism. At present, most of the theoretical research on deep
learning is based on mathematics. This dissertation proposes that the neural
network of deep learning is a physical system, examines deep learning from
three different perspectives: microscopic, macroscopic, and physical world
views, answers multiple theoretical puzzles in deep learning by using physics
principles. For example, from the perspective of quantum mechanics and
statistical physics, this dissertation presents the calculation methods for
convolution calculation, pooling, normalization, and Restricted Boltzmann
Machine, as well as the selection of cost functions, explains why deep learning
must be deep, what characteristics are learned in deep learning, why
Convolutional Neural Networks do not have to be trained layer by layer, and the
limitations of deep learning, etc., and proposes the theoretical direction and
basis for the further development of deep learning now and in the future. The
brilliance of physics flashes in deep learning, we try to establish the deep
learning technology based on the scientific theory of physics.","Dian Lei, Xiaoxiao Chen, Jianfei Zhao",2018,http://arxiv.org/abs/1805.08355v1
Concept-Oriented Deep Learning,"Concepts are the foundation of human deep learning, understanding, and
knowledge integration and transfer. We propose concept-oriented deep learning
(CODL) which extends (machine) deep learning with concept representations and
conceptual understanding capability. CODL addresses some of the major
limitations of deep learning: interpretability, transferability, contextual
adaptation, and requirement for lots of labeled training data. We discuss the
major aspects of CODL including concept graph, concept representations, concept
exemplars, and concept representation learning systems supporting incremental
and continual learning.",Daniel T Chang,2018,http://arxiv.org/abs/1806.01756v1
"Deep learning research landscape & roadmap in a nutshell: past, present
  and future -- Towards deep cortical learning","The past, present and future of deep learning is presented in this work.
Given this landscape & roadmap, we predict that deep cortical learning will be
the convergence of deep learning & cortical learning which builds an artificial
cortical column ultimately.",Aras R. Dargazany,2019,http://arxiv.org/abs/1908.02130v1
A First Look at Deep Learning Apps on Smartphones,"We are in the dawn of deep learning explosion for smartphones. To bridge the
gap between research and practice, we present the first empirical study on
16,500 the most popular Android apps, demystifying how smartphone apps exploit
deep learning in the wild. To this end, we build a new static tool that
dissects apps and analyzes their deep learning functions. Our study answers
threefold questions: what are the early adopter apps of deep learning, what do
they use deep learning for, and how do their deep learning models look like.
Our study has strong implications for app developers, smartphone vendors, and
deep learning R\&D. On one hand, our findings paint a promising picture of deep
learning for smartphones, showing the prosperity of mobile deep learning
frameworks as well as the prosperity of apps building their cores atop deep
learning. On the other hand, our findings urge optimizations on deep learning
models deployed on smartphones, the protection of these models, and validation
of research ideas on these models.","Mengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, Xuanzhe Liu",2018,http://arxiv.org/abs/1812.05448v4
"Geometrization of deep networks for the interpretability of deep
  learning systems","How to understand deep learning systems remains an open problem. In this
paper we propose that the answer may lie in the geometrization of deep
networks. Geometrization is a bridge to connect physics, geometry, deep network
and quantum computation and this may result in a new scheme to reveal the rule
of the physical world. By comparing the geometry of image matching and deep
networks, we show that geometrization of deep networks can be used to
understand existing deep learning systems and it may also help to solve the
interpretability problem of deep learning systems.","Xiao Dong, Ling Zhou",2019,http://arxiv.org/abs/1901.02354v2
Why & When Deep Learning Works: Looking Inside Deep Learnings,"The Intel Collaborative Research Institute for Computational Intelligence
(ICRI-CI) has been heavily supporting Machine Learning and Deep Learning
research from its foundation in 2012. We have asked six leading ICRI-CI Deep
Learning researchers to address the challenge of ""Why & When Deep Learning
works"", with the goal of looking inside Deep Learning, providing insights on
how deep networks function, and uncovering key observations on their
expressiveness, limitations, and potential. The output of this challenge
resulted in five papers that address different facets of deep learning. These
different facets include a high-level understating of why and when deep
networks work (and do not work), the impact of geometry on the expressiveness
of deep networks, and making deep networks interpretable.",Ronny Ronen,2017,http://arxiv.org/abs/1705.03921v1
Learning Task-aware Robust Deep Learning Systems,"Many works demonstrate that deep learning system is vulnerable to adversarial
attack. A deep learning system consists of two parts: the deep learning task
and the deep model. Nowadays, most existing works investigate the impact of the
deep model on robustness of deep learning systems, ignoring the impact of the
learning task. In this paper, we adopt the binary and interval label encoding
strategy to redefine the classification task and design corresponding loss to
improve robustness of the deep learning system. Our method can be viewed as
improving the robustness of deep learning systems from both the learning task
and deep model. Experimental results demonstrate that our learning task-aware
method is much more robust than traditional classification while retaining the
accuracy.","Keji Han, Yun Li, Xianzhong Long, Yao Ge",2020,http://arxiv.org/abs/2010.05125v2
Deep Learning in Software Engineering,"Recent years, deep learning is increasingly prevalent in the field of
Software Engineering (SE). However, many open issues still remain to be
investigated. How do researchers integrate deep learning into SE problems?
Which SE phases are facilitated by deep learning? Do practitioners benefit from
deep learning? The answers help practitioners and researchers develop practical
deep learning models for SE tasks. To answer these questions, we conduct a
bibliography analysis on 98 research papers in SE that use deep learning
techniques. We find that 41 SE tasks in all SE phases have been facilitated by
deep learning integrated solutions. In which, 84.7% papers only use standard
deep learning models and their variants to solve SE problems. The
practicability becomes a concern in utilizing deep learning techniques. How to
improve the effectiveness, efficiency, understandability, and testability of
deep learning based solutions may attract more SE researchers in the future.","Xiaochen Li, He Jiang, Zhilei Ren, Ge Li, Jingxuan Zhang",2018,http://arxiv.org/abs/1805.04825v1
Moving Deep Learning into Web Browser: How Far Can We Go?,"Recently, several JavaScript-based deep learning frameworks have emerged,
making it possible to perform deep learning tasks directly in browsers.
However, little is known on what and how well we can do with these frameworks
for deep learning in browsers. To bridge the knowledge gap, in this paper, we
conduct the first empirical study of deep learning in browsers. We survey 7
most popular JavaScript-based deep learning frameworks, investigating to what
extent deep learning tasks have been supported in browsers so far. Then we
measure the performance of different frameworks when running different deep
learning tasks. Finally, we dig out the performance gap between deep learning
in browsers and on native platforms by comparing the performance of
TensorFlow.js and TensorFlow in Python. Our findings could help application
developers, deep-learning framework vendors and browser vendors to improve the
efficiency of deep learning in browsers.","Yun Ma, Dongwei Xiang, Shuyu Zheng, Deyu Tian, Xuanzhe Liu",2019,http://arxiv.org/abs/1901.09388v2
Greedy Deep Dictionary Learning,"In this work we propose a new deep learning tool called deep dictionary
learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at
a time. This requires solving a simple (shallow) dictionary learning problem,
the solution to this is well known. We apply the proposed technique on some
benchmark deep learning datasets. We compare our results with other deep
learning tools like stacked autoencoder and deep belief network; and state of
the art supervised dictionary learning tools like discriminative KSVD and label
consistent KSVD. Our method yields better results than all.","Snigdha Tariyal, Angshul Majumdar, Richa Singh, Mayank Vatsa",2016,http://arxiv.org/abs/1602.00203v1
"Quantum Neural Networks: Concepts, Applications, and Challenges","Quantum deep learning is a research field for the use of quantum computing
techniques for training deep neural networks. The research topics and
directions of deep learning and quantum computing have been separated for long
time, however by discovering that quantum circuits can act like artificial
neural networks, quantum deep learning research is widely adopted. This paper
explains the backgrounds and basic principles of quantum deep learning and also
introduces major achievements. After that, this paper discusses the challenges
of quantum deep learning research in multiple perspectives. Lastly, this paper
presents various future research directions and application fields of quantum
deep learning.","Yunseok Kwak, Won Joon Yun, Soyi Jung, Joongheon Kim",2021,http://arxiv.org/abs/2108.01468v1
"NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders
  of Deep Giants","Tiny deep learning has attracted increasing attention driven by the
substantial demand for deploying deep learning on numerous intelligent
Internet-of-Things devices. However, it is still challenging to unleash tiny
deep learning's full potential on both large-scale datasets and downstream
tasks due to the under-fitting issues caused by the limited model capacity of
tiny neural networks (TNNs). To this end, we propose a framework called
NetBooster to empower tiny deep learning by augmenting the architectures of
TNNs via an expansion-then-contraction strategy. Extensive experiments show
that NetBooster consistently outperforms state-of-the-art tiny deep learning
solutions.","Zhongzhi Yu, Yonggan Fu, Jiayi Yuan, Haoran You, Yingyan Lin",2023,http://arxiv.org/abs/2306.13586v1
Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey,"Deep reinforcement learning augments the reinforcement learning framework and
utilizes the powerful representation of deep neural networks. Recent works have
demonstrated the remarkable successes of deep reinforcement learning in various
domains including finance, medicine, healthcare, video games, robotics, and
computer vision. In this work, we provide a detailed review of recent and
state-of-the-art research advances of deep reinforcement learning in computer
vision. We start with comprehending the theories of deep learning,
reinforcement learning, and deep reinforcement learning. We then propose a
categorization of deep reinforcement learning methodologies and discuss their
advantages and limitations. In particular, we divide deep reinforcement
learning into seven main categories according to their applications in computer
vision, i.e. (i)landmark localization (ii) object detection; (iii) object
tracking; (iv) registration on both 2D image and 3D image volumetric data (v)
image segmentation; (vi) videos analysis; and (vii) other applications. Each of
these categories is further analyzed with reinforcement learning techniques,
network design, and performance. Moreover, we provide a comprehensive analysis
of the existing publicly available datasets and examine source code
availability. Finally, we present some open issues and discuss future research
directions on deep reinforcement learning in computer vision","Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki, Khoa Luu, Marios Savvides",2021,http://arxiv.org/abs/2108.11510v1
"Probabilistic Deep Learning with Probabilistic Neural Networks and Deep
  Probabilistic Models","Probabilistic deep learning is deep learning that accounts for uncertainty,
both model uncertainty and data uncertainty. It is based on the use of
probabilistic models and deep neural networks. We distinguish two approaches to
probabilistic deep learning: probabilistic neural networks and deep
probabilistic models. The former employs deep neural networks that utilize
probabilistic layers which can represent and process uncertainty; the latter
uses probabilistic models that incorporate deep neural network components which
capture complex non-linear stochastic relationships between the random
variables. We discuss some major examples of each approach including Bayesian
neural networks and mixture density networks (for probabilistic neural
networks), and variational autoencoders, deep Gaussian processes and deep mixed
effects models (for deep probabilistic models). TensorFlow Probability is a
library for probabilistic modeling and inference which can be used for both
approaches of probabilistic deep learning. We include its code examples for
illustration.",Daniel T. Chang,2021,http://arxiv.org/abs/2106.00120v3
"Towards energy-efficient Deep Learning: An overview of energy-efficient
  approaches along the Deep Learning Lifecycle","Deep Learning has enabled many advances in machine learning applications in
the last few years. However, since current Deep Learning algorithms require
much energy for computations, there are growing concerns about the associated
environmental costs. Energy-efficient Deep Learning has received much attention
from researchers and has already made much progress in the last couple of
years. This paper aims to gather information about these advances from the
literature and show how and at which points along the lifecycle of Deep
Learning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation)
it is possible to reduce energy consumption.","Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon",2023,http://arxiv.org/abs/2303.01980v1
A Unified Framework of Deep Neural Networks by Capsules,"With the growth of deep learning, how to describe deep neural networks
unifiedly is becoming an important issue. We first formalize neural networks
mathematically with their directed graph representations, and prove a
generation theorem about the induced networks of connected directed acyclic
graphs. Then, we set up a unified framework for deep learning with capsule
networks. This capsule framework could simplify the description of existing
deep neural networks, and provide a theoretical basis of graphic designing and
programming techniques for deep learning models, thus would be of great
significance to the advancement of deep learning.","Yujian Li, Chuanhui Shan",2018,http://arxiv.org/abs/1805.03551v2
Integrating Learning and Reasoning with Deep Logic Models,"Deep learning is very effective at jointly learning feature representations
and classification models, especially when dealing with high dimensional input
patterns. Probabilistic logic reasoning, on the other hand, is capable to take
consistent and robust decisions in complex environments. The integration of
deep learning and logic reasoning is still an open-research problem and it is
considered to be the key for the development of real intelligent agents. This
paper presents Deep Logic Models, which are deep graphical models integrating
deep learning and logic reasoning both for learning and inference. Deep Logic
Models create an end-to-end differentiable architecture, where deep learners
are embedded into a network implementing a continuous relaxation of the logic
knowledge. The learning process allows to jointly learn the weights of the deep
learners and the meta-parameters controlling the high-level reasoning. The
experimental results show that the proposed methodology overtakes the
limitations of the other approaches that have been proposed to bridge deep
learning and reasoning.","Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Marco Gori",2019,http://arxiv.org/abs/1901.04195v1
Deep Learning in the Field of Biometric Template Protection: An Overview,"Today, deep learning represents the most popular and successful form of
machine learning. Deep learning has revolutionised the field of pattern
recognition, including biometric recognition. Biometric systems utilising deep
learning have been shown to achieve auspicious recognition accuracy, surpassing
human performance. Apart from said breakthrough advances in terms of biometric
performance, the use of deep learning was reported to impact different
covariates of biometrics such as algorithmic fairness, vulnerability to
attacks, or template protection. Technologies of biometric template protection
are designed to enable a secure and privacy-preserving deployment of
biometrics. In the recent past, deep learning techniques have been frequently
applied in biometric template protection systems for various purposes. This
work provides an overview of how advances in deep learning take influence on
the field of biometric template protection. The interrelation between improved
biometric performance rates and security in biometric template protection is
elaborated. Further, the use of deep learning for obtaining feature
representations that are suitable for biometric template protection is
discussed. Novel methods that apply deep learning to achieve various goals of
biometric template protection are surveyed along with deep learning-based
attacks.","Christian Rathgeb, Jascha Kolberg, Andreas Uhl, Christoph Busch",2023,http://arxiv.org/abs/2303.02715v1
A Survey Analyzing Generalization in Deep Reinforcement Learning,"Reinforcement learning research obtained significant success and attention
with the utilization of deep neural networks to solve problems in high
dimensional state or action spaces. While deep reinforcement learning policies
are currently being deployed in many different fields from medical applications
to large language models, there are still ongoing questions the field is trying
to answer on the generalization capabilities of deep reinforcement learning
policies. In this paper, we will formalize and analyze generalization in deep
reinforcement learning. We will explain the fundamental reasons why deep
reinforcement learning policies encounter overfitting problems that limit their
generalization capabilities. Furthermore, we will categorize and explain the
manifold solution approaches to increase generalization, and overcome
overfitting in deep reinforcement learning policies. From exploration to
adversarial analysis and from regularization to robustness our paper provides
an analysis on a wide range of subfields within deep reinforcement learning
with a broad scope and in-depth view. We believe our study can provide a
compact guideline for the current advancements in deep reinforcement learning,
and help to construct robust deep neural policies with higher generalization
skills.",Ezgi Korkmaz,2024,http://arxiv.org/abs/2401.02349v2
What Really is Deep Learning Doing?,"Deep learning has achieved a great success in many areas, from computer
vision to natural language processing, to game playing, and much more. Yet,
what deep learning is really doing is still an open question. There are a lot
of works in this direction. For example, [5] tried to explain deep learning by
group renormalization, and [6] tried to explain deep learning from the view of
functional approximation. In order to address this very crucial question, here
we see deep learning from perspective of mechanical learning and learning
machine (see [1], [2]). From this particular angle, we can see deep learning
much better and answer with confidence: What deep learning is really doing? why
it works well, how it works, and how much data is necessary for learning. We
also will discuss advantages and disadvantages of deep learning at the end of
this work.",Chuyu Xiong,2017,http://arxiv.org/abs/1711.03577v1
Transferability in Deep Learning: A Survey,"The success of deep learning algorithms generally depends on large-scale
data, while humans appear to have inherent ability of knowledge transfer, by
recognizing and applying relevant knowledge from previous learning experiences
when encountering and solving unseen tasks. Such an ability to acquire and
reuse knowledge is known as transferability in deep learning. It has formed the
long-term quest towards making deep learning as data-efficient as human
learning, and has been motivating fruitful design of more powerful deep
learning algorithms. We present this survey to connect different isolated areas
in deep learning with their relation to transferability, and to provide a
unified and complete view to investigating transferability through the whole
lifecycle of deep learning. The survey elaborates the fundamental goals and
challenges in parallel with the core principles and methods, covering recent
cornerstones in deep architectures, pre-training, task adaptation and domain
adaptation. This highlights unanswered questions on the appropriate objectives
for learning transferable knowledge and for adapting the knowledge to new tasks
and domains, avoiding catastrophic forgetting and negative transfer. Finally,
we implement a benchmark and an open-source library, enabling a fair evaluation
of deep learning methods in terms of transferability.","Junguang Jiang, Yang Shu, Jianmin Wang, Mingsheng Long",2022,http://arxiv.org/abs/2201.05867v1
"Feature versus Raw Sequence: Deep Learning Comparative Study on
  Predicting Pre-miRNA","Should we input known genome sequence features or input sequence itself in
deep learning framework? As deep learning more popular in various applications,
researchers often come to question whether to generate features or use raw
sequences for deep learning. To answer this question, we study the prediction
accuracy of precursor miRNA prediction of feature-based deep belief network and
sequence-based convolution neural network. Tested on a variant of six-layer
convolution neural net and three-layer deep belief network, we find the raw
sequence input based convolution neural network model performs similar or
slightly better than feature based deep belief networks with best accuracy
values of 0.995 and 0.990, respectively. Both the models outperform existing
benchmarks models. The results shows us that if provided large enough data,
well devised raw sequence based deep learning models can replace feature based
deep learning models. However, construction of well behaved deep learning model
can be very challenging. In cased features can be easily extracted,
feature-based deep learning models may be a better alternative.","Jaya Thomas, Sonia Thomas, Lee Sael",2017,http://arxiv.org/abs/1710.06798v1
"Distributed Deep Reinforcement Learning: A Survey and A Multi-Player
  Multi-Agent Learning Toolbox","With the breakthrough of AlphaGo, deep reinforcement learning becomes a
recognized technique for solving sequential decision-making problems. Despite
its reputation, data inefficiency caused by its trial and error learning
mechanism makes deep reinforcement learning hard to be practical in a wide
range of areas. Plenty of methods have been developed for sample efficient deep
reinforcement learning, such as environment modeling, experience transfer, and
distributed modifications, amongst which, distributed deep reinforcement
learning has shown its potential in various applications, such as
human-computer gaming, and intelligent transportation. In this paper, we
conclude the state of this exciting field, by comparing the classical
distributed deep reinforcement learning methods, and studying important
components to achieve efficient distributed learning, covering single player
single agent distributed deep reinforcement learning to the most complex
multiple players multiple agents distributed deep reinforcement learning.
Furthermore, we review recently released toolboxes that help to realize
distributed deep reinforcement learning without many modifications of their
non-distributed versions. By analyzing their strengths and weaknesses, a
multi-player multi-agent distributed deep reinforcement learning toolbox is
developed and released, which is further validated on Wargame, a complex
environment, showing usability of the proposed toolbox for multiple players and
multiple agents distributed deep reinforcement learning under complex games.
Finally, we try to point out challenges and future trends, hoping this brief
review can provide a guide or a spark for researchers who are interested in
distributed deep reinforcement learning.","Qiyue Yin, Tongtong Yu, Shengqi Shen, Jun Yang, Meijing Zhao, Kaiqi Huang, Bin Liang, Liang Wang",2022,http://arxiv.org/abs/2212.00253v1
Are Efficient Deep Representations Learnable?,"Many theories of deep learning have shown that a deep network can require
dramatically fewer resources to represent a given function compared to a
shallow network. But a question remains: can these efficient representations be
learned using current deep learning techniques? In this work, we test whether
standard deep learning methods can in fact find the efficient representations
posited by several theories of deep representation. Specifically, we train deep
neural networks to learn two simple functions with known efficient solutions:
the parity function and the fast Fourier transform. We find that using
gradient-based optimization, a deep network does not learn the parity function,
unless initialized very close to a hand-coded exact solution. We also find that
a deep linear neural network does not learn the fast Fourier transform, even in
the best-case scenario of infinite training data, unless the weights are
initialized very close to the exact hand-coded solution. Our results suggest
that not every element of the class of compositional functions can be learned
efficiently by a deep network, and further restrictions are necessary to
understand what functions are both efficiently representable and learnable.","Maxwell Nye, Andrew Saxe",2018,http://arxiv.org/abs/1807.06399v1
Deep Learning: A Critical Appraisal,"Although deep learning has historical roots going back decades, neither the
term ""deep learning"" nor the approach was popular just over five years ago,
when the field was reignited by papers such as Krizhevsky, Sutskever and
Hinton's now classic (2012) deep network model of Imagenet. What has the field
discovered in the five subsequent years? Against a background of considerable
progress in areas such as speech recognition, image recognition, and game
playing, and considerable enthusiasm in the popular press, I present ten
concerns for deep learning, and suggest that deep learning must be supplemented
by other techniques if we are to reach artificial general intelligence.",Gary Marcus,2018,http://arxiv.org/abs/1801.00631v1
Deep Learning for Sentiment Analysis : A Survey,"Deep learning has emerged as a powerful machine learning technique that
learns multiple layers of representations or features of the data and produces
state-of-the-art prediction results. Along with the success of deep learning in
many other application domains, deep learning is also popularly used in
sentiment analysis in recent years. This paper first gives an overview of deep
learning and then provides a comprehensive survey of its current applications
in sentiment analysis.","Lei Zhang, Shuai Wang, Bing Liu",2018,http://arxiv.org/abs/1801.07883v2
Deep Learning for Visual Navigation of Underwater Robots,"This paper aims to briefly survey deep learning methods for visual navigation
of underwater robotics. The scope of this paper includes the visual perception
of underwater robotics with deep learning methods, the available visual
underwater datasets, imitation learning, and reinforcement learning methods for
navigation. Additionally, relevant works will be categorized under the
imitation learning or deep learning paradigm for underwater robots for clarity
of the training methodologies in the current landscape. Literature that uses
deep learning algorithms to process non-visual data for underwater navigation
will not be considered, except as contrasting examples.",M. Sunbeam,2023,http://arxiv.org/abs/2310.19495v1
When deep learning meets security,"Deep learning is an emerging research field that has proven its effectiveness
towards deploying more efficient intelligent systems. Security, on the other
hand, is one of the most essential issues in modern communication systems.
Recently many papers have shown that using deep learning models can achieve
promising results when applied to the security domain. In this work, we provide
an overview for the recent studies that apply deep learning techniques to the
field of security.",Majd Latah,2018,http://arxiv.org/abs/1807.04739v1
Deep Causal Learning for Robotic Intelligence,"This invited review discusses causal learning in the context of robotic
intelligence. The paper introduced the psychological findings on causal
learning in human cognition, then it introduced the traditional statistical
solutions on causal discovery and causal inference. The paper reviewed recent
deep causal learning algorithms with a focus on their architectures and the
benefits of using deep nets and discussed the gap between deep causal learning
and the needs of robotic intelligence.",Yangming Li,2022,http://arxiv.org/abs/2212.12597v1
"Deep learning in radiology: an overview of the concepts and a survey of
  the state of the art","Deep learning is a branch of artificial intelligence where networks of simple
interconnected units are used to extract patterns from data in order to solve
complex problems. Deep learning algorithms have shown groundbreaking
performance in a variety of sophisticated tasks, especially those related to
images. They have often matched or exceeded human performance. Since the
medical field of radiology mostly relies on extracting useful information from
images, it is a very natural application area for deep learning, and research
in this area has rapidly grown in recent years. In this article, we review the
clinical reality of radiology and discuss the opportunities for application of
deep learning algorithms. We also introduce basic concepts of deep learning
including convolutional neural networks. Then, we present a survey of the
research in deep learning applied to radiology. We organize the studies by the
types of specific tasks that they attempt to solve and review the broad range
of utilized deep learning algorithms. Finally, we briefly discuss opportunities
and challenges for incorporating deep learning in the radiology practice of the
future.","Maciej A. Mazurowski, Mateusz Buda, Ashirbani Saha, Mustafa R. Bashir",2018,http://arxiv.org/abs/1802.08717v1
A Survey on Deep Learning Methods for Robot Vision,"Deep learning has allowed a paradigm shift in pattern recognition, from using
hand-crafted features together with statistical classifiers to using
general-purpose learning procedures for learning data-driven representations,
features, and classifiers together. The application of this new paradigm has
been particularly successful in computer vision, in which the development of
deep learning methods for vision applications has become a hot research topic.
Given that deep learning has already attracted the attention of the robot
vision community, the main purpose of this survey is to address the use of deep
learning in robot vision. To achieve this, a comprehensive overview of deep
learning and its usage in computer vision is given, that includes a description
of the most frequently used neural models and their main application areas.
Then, the standard methodology and tools used for designing deep-learning based
vision systems are presented. Afterwards, a review of the principal work using
deep learning in robot vision is presented, as well as current and future
trends related to the use of deep learning in robotics. This survey is intended
to be a guide for the developers of robot vision systems.","Javier Ruiz-del-Solar, Patricio Loncomilla, Naiomi Soto",2018,http://arxiv.org/abs/1803.10862v1
A Selective Overview of Deep Learning,"Deep learning has arguably achieved tremendous success in recent years. In
simple words, deep learning uses the composition of many nonlinear functions to
model the complex dependency between input features and labels. While neural
networks have a long history, recent advances have greatly improved their
performance in computer vision, natural language processing, etc. From the
statistical and scientific perspective, it is natural to ask: What is deep
learning? What are the new characteristics of deep learning, compared with
classical methods? What are the theoretical foundations of deep learning? To
answer these questions, we introduce common neural network models (e.g.,
convolutional neural nets, recurrent neural nets, generative adversarial nets)
and training techniques (e.g., stochastic gradient descent, dropout, batch
normalization) from a statistical point of view. Along the way, we highlight
new characteristics of deep learning (including depth and over-parametrization)
and explain their practical and theoretical benefits. We also sample recent
results on theories of deep learning, many of which are only suggestive. While
a complete understanding of deep learning remains elusive, we hope that our
perspectives and discussions serve as a stimulus for new statistical research.","Jianqing Fan, Cong Ma, Yiqiao Zhong",2019,http://arxiv.org/abs/1904.05526v2
Interpretations of Deep Learning by Forests and Haar Wavelets,"This paper presents a basic property of region dividing of ReLU (rectified
linear unit) deep learning when new layers are successively added, by which two
new perspectives of interpreting deep learning are given. The first is related
to decision trees and forests; we construct a deep learning structure
equivalent to a forest in classification abilities, which means that certain
kinds of ReLU deep learning can be considered as forests. The second
perspective is that Haar wavelet represented functions can be approximated by
ReLU deep learning with arbitrary precision; and then a general conclusion of
function approximation abilities of ReLU deep learning is given. Finally,
generalize some of the conclusions of ReLU deep learning to the case of
sigmoid-unit deep learning.",Changcun Huang,2019,http://arxiv.org/abs/1906.06706v7
A Brief Survey of Deep Reinforcement Learning,"Deep reinforcement learning is poised to revolutionise the field of AI and
represents a step towards building autonomous systems with a higher level
understanding of the visual world. Currently, deep learning is enabling
reinforcement learning to scale to problems that were previously intractable,
such as learning to play video games directly from pixels. Deep reinforcement
learning algorithms are also applied to robotics, allowing control policies for
robots to be learned directly from camera inputs in the real world. In this
survey, we begin with an introduction to the general field of reinforcement
learning, then progress to the main streams of value-based and policy-based
methods. Our survey will cover central algorithms in deep reinforcement
learning, including the deep $Q$-network, trust region policy optimisation, and
asynchronous advantage actor-critic. In parallel, we highlight the unique
advantages of deep neural networks, focusing on visual understanding via
reinforcement learning. To conclude, we describe several current areas of
research within the field.","Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath",2017,http://arxiv.org/abs/1708.05866v2
Topological Deep Learning: A Review of an Emerging Paradigm,"Topological data analysis (TDA) provides insight into data shape. The
summaries obtained by these methods are principled global descriptions of
multi-dimensional data whilst exhibiting stable properties such as robustness
to deformation and noise. Such properties are desirable in deep learning
pipelines but they are typically obtained using non-TDA strategies. This is
partly caused by the difficulty of combining TDA constructs (e.g. barcode and
persistence diagrams) with current deep learning algorithms. Fortunately, we
are now witnessing a growth of deep learning applications embracing
topologically-guided components. In this survey, we review the nascent field of
topological deep learning by first revisiting the core concepts of TDA. We then
explore how the use of TDA techniques has evolved over time to support deep
learning frameworks, and how they can be integrated into different aspects of
deep learning. Furthermore, we touch on TDA usage for analyzing existing deep
models; deep topological analytics. Finally, we discuss the challenges and
future prospects of topological deep learning.","Ali Zia, Abdelwahed Khamis, James Nichols, Zeeshan Hayder, Vivien Rolland, Lars Petersson",2023,http://arxiv.org/abs/2302.03836v1
Generalization and Expressivity for Deep Nets,"Along with the rapid development of deep learning in practice, the
theoretical explanations for its success become urgent. Generalization and
expressivity are two widely used measurements to quantify theoretical behaviors
of deep learning. The expressivity focuses on finding functions expressible by
deep nets but cannot be approximated by shallow nets with the similar number of
neurons. It usually implies the large capacity. The generalization aims at
deriving fast learning rate for deep nets. It usually requires small capacity
to reduce the variance. Different from previous studies on deep learning,
pursuing either expressivity or generalization, we take both factors into
account to explore the theoretical advantages of deep nets. For this purpose,
we construct a deep net with two hidden layers possessing excellent
expressivity in terms of localized and sparse approximation. Then, utilizing
the well known covering number to measure the capacity, we find that deep nets
possess excellent expressive power (measured by localized and sparse
approximation) without enlarging the capacity of shallow nets. As a
consequence, we derive near optimal learning rates for implementing empirical
risk minimization (ERM) on the constructed deep nets. These results
theoretically exhibit the advantage of deep nets from learning theory
viewpoints.",Shao-Bo Lin,2018,http://arxiv.org/abs/1803.03772v2
Deep Incremental Boosting,"This paper introduces Deep Incremental Boosting, a new technique derived from
AdaBoost, specifically adapted to work with Deep Learning methods, that reduces
the required training time and improves generalisation. We draw inspiration
from Transfer of Learning approaches to reduce the start-up time to training
each incremental Ensemble member. We show a set of experiments that outlines
some preliminary results on some common Deep Learning datasets and discuss the
potential improvements Deep Incremental Boosting brings to traditional Ensemble
methods in Deep Learning.","Alan Mosca, George D Magoulas",2017,http://arxiv.org/abs/1708.03704v1
Combining Deep Learning with Good Old-Fashioned Machine Learning,"We present a comprehensive, stacking-based framework for combining deep
learning with good old-fashioned machine learning, called Deep GOld. Our
framework involves ensemble selection from 51 retrained pretrained deep
networks as first-level models, and 10 machine-learning algorithms as
second-level models. Enabled by today's state-of-the-art software tools and
hardware platforms, Deep GOld delivers consistent improvement when tested on
four image-classification datasets: Fashion MNIST, CIFAR10, CIFAR100, and Tiny
ImageNet. Of 120 experiments, in all but 10 Deep GOld improved the original
networks' performance.",Moshe Sipper,2022,http://arxiv.org/abs/2207.03757v2
"Deep frequency principle towards understanding why deeper learning is
  faster","Understanding the effect of depth in deep learning is a critical problem. In
this work, we utilize the Fourier analysis to empirically provide a promising
mechanism to understand why feedforward deeper learning is faster. To this end,
we separate a deep neural network, trained by normal stochastic gradient
descent, into two parts during analysis, i.e., a pre-condition component and a
learning component, in which the output of the pre-condition one is the input
of the learning one. We use a filtering method to characterize the frequency
distribution of a high-dimensional function. Based on experiments of deep
networks and real dataset, we propose a deep frequency principle, that is, the
effective target function for a deeper hidden layer biases towards lower
frequency during the training. Therefore, the learning component effectively
learns a lower frequency function if the pre-condition component has more
layers. Due to the well-studied frequency principle, i.e., deep neural networks
learn lower frequency functions faster, the deep frequency principle provides a
reasonable explanation to why deeper learning is faster. We believe these
empirical studies would be valuable for future theoretical studies of the
effect of depth in deep learning.","Zhi-Qin John Xu, Hanxu Zhou",2020,http://arxiv.org/abs/2007.14313v2
Deep Bayesian Active Learning with Image Data,"Even though active learning forms an important pillar of machine learning,
deep learning tools are not prevalent within it. Deep learning poses several
difficulties when used in an active learning setting. First, active learning
(AL) methods generally rely on being able to learn and update models from small
amounts of data. Recent advances in deep learning, on the other hand, are
notorious for their dependence on large amounts of data. Second, many AL
acquisition functions rely on model uncertainty, yet deep learning methods
rarely represent such model uncertainty. In this paper we combine recent
advances in Bayesian deep learning into the active learning framework in a
practical way. We develop an active learning framework for high dimensional
data, a task which has been extremely challenging so far, with very sparse
existing literature. Taking advantage of specialised models such as Bayesian
convolutional neural networks, we demonstrate our active learning techniques
with image data, obtaining a significant improvement on existing active
learning approaches. We demonstrate this on both the MNIST dataset, as well as
for skin cancer diagnosis from lesion images (ISIC2016 task).","Yarin Gal, Riashat Islam, Zoubin Ghahramani",2017,http://arxiv.org/abs/1703.02910v1
"Deep reinforcement learning for optical systems: A case study of
  mode-locked lasers","We demonstrate that deep reinforcement learning (deep RL) provides a highly
effective strategy for the control and self-tuning of optical systems. Deep RL
integrates the two leading machine learning architectures of deep neural
networks and reinforcement learning to produce robust and stable learning for
control. Deep RL is ideally suited for optical systems as the tuning and
control relies on interactions with its environment with a goal-oriented
objective to achieve optimal immediate or delayed rewards. This allows the
optical system to recognize bi-stable structures and navigate, via trajectory
planning, to optimally performing solutions, the first such algorithm
demonstrated to do so in optical systems. We specifically demonstrate the deep
RL architecture on a mode-locked laser, where robust self-tuning and control
can be established through access of the deep RL agent to its waveplates and
polarizers. We further integrate transfer learning to help the deep RL agent
rapidly learn new parameter regimes and generalize its control authority.
Additionally, the deep RL learning can be easily integrated with other control
paradigms to provide a broad framework to control any optical system.","Chang Sun, Eurika Kaiser, Steven L. Brunton, J. Nathan Kutz",2020,http://arxiv.org/abs/2006.05579v1
"Error Bounds for a Matrix-Vector Product Approximation with Deep ReLU
  Neural Networks","Among the several paradigms of artificial intelligence (AI) or machine
learning (ML), a remarkably successful paradigm is deep learning. Deep
learning's phenomenal success has been hoped to be interpreted via fundamental
research on the theory of deep learning. Accordingly, applied research on deep
learning has spurred the theory of deep learning-oriented depth and breadth of
developments. Inspired by such developments, we pose these fundamental
questions: can we accurately approximate an arbitrary matrix-vector product
using deep rectified linear unit (ReLU) feedforward neural networks (FNNs)? If
so, can we bound the resulting approximation error? In light of these
questions, we derive error bounds in Lebesgue and Sobolev norms that comprise
our developed deep approximation theory. Guided by this theory, we have
successfully trained deep ReLU FNNs whose test results justify our developed
theory. The developed theory is also applicable for guiding and easing the
training of teacher deep ReLU FNNs in view of the emerging teacher-student AI
or ML paradigms that are essential for solving several AI or ML problems in
wireless communications and signal processing; network science and graph signal
processing; and network neuroscience and brain physics.",Tilahun M. Getu,2021,http://arxiv.org/abs/2111.12963v1
Introduction to deep learning,"Deep Learning (DL) has made a major impact on data science in the last
decade. This chapter introduces the basic concepts of this field. It includes
both the basic structures used to design deep neural networks and a brief
survey of some of its popular use cases.","Lihi Shiloh-Perl, Raja Giryes",2020,http://arxiv.org/abs/2003.03253v1
Deep Learning: From Basics to Building Deep Neural Networks with Python,"This book is intended for beginners who have no familiarity with deep
learning. Our only expectation from readers is that they already have the basic
programming skills in Python.",Milad Vazan,2022,http://arxiv.org/abs/2205.01069v1
"A Nesterov's Accelerated quasi-Newton method for Global Routing using
  Deep Reinforcement Learning","Deep Q-learning method is one of the most popularly used deep reinforcement
learning algorithms which uses deep neural networks to approximate the
estimation of the action-value function. Training of the deep Q-network (DQN)
is usually restricted to first order gradient based methods. This paper
attempts to accelerate the training of deep Q-networks by introducing a second
order Nesterov's accelerated quasi-Newton method. We evaluate the performance
of the proposed method on deep reinforcement learning using double DQNs for
global routing. The results show that the proposed method can obtain better
routing solutions compared to the DQNs trained with first order Adam and
RMSprop methods.","S. Indrapriyadarsini, Shahrzad Mahboubi, Hiroshi Ninomiya, Takeshi Kamio, Hideki Asai",2020,http://arxiv.org/abs/2010.09465v1
Augmented Q Imitation Learning (AQIL),"The study of unsupervised learning can be generally divided into two
categories: imitation learning and reinforcement learning. In imitation
learning the machine learns by mimicking the behavior of an expert system
whereas in reinforcement learning the machine learns via direct environment
feedback. Traditional deep reinforcement learning takes a significant time
before the machine starts to converge to an optimal policy. This paper proposes
Augmented Q-Imitation-Learning, a method by which deep reinforcement learning
convergence can be accelerated by applying Q-imitation-learning as the initial
training process in traditional Deep Q-learning.","Xiao Lei Zhang, Anish Agarwal",2020,http://arxiv.org/abs/2004.00993v2
Deep Reinforcement Learning for Conversational AI,"Deep reinforcement learning is revolutionizing the artificial intelligence
field. Currently, it serves as a good starting point for constructing
intelligent autonomous systems which offer a better knowledge of the visual
world. It is possible to scale deep reinforcement learning with the use of deep
learning and do amazing tasks such as use of pixels in playing video games. In
this paper, key concepts of deep reinforcement learning including reward
function, differences between reinforcement learning and supervised learning
and models for implementation of reinforcement are discussed. Key challenges
related to the implementation of reinforcement learning in conversational AI
domain are identified as well as discussed in detail. Various conversational
models which are based on deep reinforcement learning (as well as deep
learning) are also discussed. In summary, this paper discusses key aspects of
deep reinforcement learning which are crucial for designing an efficient
conversational AI.","Mahipal Jadeja, Neelanshi Varia, Agam Shah",2017,http://arxiv.org/abs/1709.05067v1
An Overview of Deep Semi-Supervised Learning,"Deep neural networks demonstrated their ability to provide remarkable
performances on a wide range of supervised learning tasks (e.g., image
classification) when trained on extensive collections of labeled data (e.g.,
ImageNet). However, creating such large datasets requires a considerable amount
of resources, time, and effort. Such resources may not be available in many
practical cases, limiting the adoption and the application of many deep
learning methods. In a search for more data-efficient deep learning methods to
overcome the need for large annotated datasets, there is a rising research
interest in semi-supervised learning and its applications to deep neural
networks to reduce the amount of labeled data required, by either developing
novel methods or adopting existing semi-supervised learning frameworks for a
deep learning setting. In this paper, we provide a comprehensive overview of
deep semi-supervised learning, starting with an introduction to the field,
followed by a summarization of the dominant semi-supervised approaches in deep
learning.","Yassine Ouali, Céline Hudelot, Myriam Tami",2020,http://arxiv.org/abs/2006.05278v2
Node-By-Node Greedy Deep Learning for Interpretable Features,"Multilayer networks have seen a resurgence under the umbrella of deep
learning. Current deep learning algorithms train the layers of the network
sequentially, improving algorithmic performance as well as providing some
regularization. We present a new training algorithm for deep networks which
trains \emph{each node in the network} sequentially. Our algorithm is orders of
magnitude faster, creates more interpretable internal representations at the
node level, while not sacrificing on the ultimate out-of-sample performance.","Ke Wu, Malik Magdon-Ismail",2016,http://arxiv.org/abs/1602.06183v1
Lecture Notes: Neural Network Architectures,"These lecture notes provide an overview of Neural Network architectures from
a mathematical point of view. Especially, Machine Learning with Neural Networks
is seen as an optimization problem. Covered are an introduction to Neural
Networks and the following architectures: Feedforward Neural Network,
Convolutional Neural Network, ResNet, and Recurrent Neural Network.",Evelyn Herberg,2023,http://arxiv.org/abs/2304.05133v2
"Neural Network Processing Neural Networks: An efficient way to learn
  higher order functions","Functions are rich in meaning and can be interpreted in a variety of ways.
Neural networks were proven to be capable of approximating a large class of
functions[1]. In this paper, we propose a new class of neural networks called
""Neural Network Processing Neural Networks"" (NNPNNs), which inputs neural
networks and numerical values, instead of just numerical values. Thus enabling
neural networks to represent and process rich structures.",Firat Tuna,2019,http://arxiv.org/abs/1911.05640v2
"Guaranteed Quantization Error Computation for Neural Network Model
  Compression","Neural network model compression techniques can address the computation issue
of deep neural networks on embedded devices in industrial systems. The
guaranteed output error computation problem for neural network compression with
quantization is addressed in this paper. A merged neural network is built from
a feedforward neural network and its quantized version to produce the exact
output difference between two neural networks. Then, optimization-based methods
and reachability analysis methods are applied to the merged neural network to
compute the guaranteed quantization error. Finally, a numerical example is
proposed to validate the applicability and effectiveness of the proposed
approach.","Wesley Cooke, Zihao Mo, Weiming Xiang",2023,http://arxiv.org/abs/2304.13812v1
Graph Structure of Neural Networks,"Neural networks are often represented as graphs of connections between
neurons. However, despite their wide use, there is currently little
understanding of the relationship between the graph structure of the neural
network and its predictive performance. Here we systematically investigate how
does the graph structure of neural networks affect their predictive
performance. To this end, we develop a novel graph-based representation of
neural networks called relational graph, where layers of neural network
computation correspond to rounds of message exchange along the graph structure.
Using this representation we show that: (1) a ""sweet spot"" of relational graphs
leads to neural networks with significantly improved predictive performance;
(2) neural network's performance is approximately a smooth function of the
clustering coefficient and average path length of its relational graph; (3) our
findings are consistent across many different tasks and datasets; (4) the sweet
spot can be identified efficiently; (5) top-performing neural networks have
graph structure surprisingly similar to those of real biological neural
networks. Our work opens new directions for the design of neural architectures
and the understanding on neural networks in general.","Jiaxuan You, Jure Leskovec, Kaiming He, Saining Xie",2020,http://arxiv.org/abs/2007.06559v2
"Hybrid Quantum-Classical Neural Networks for Downlink Beamforming
  Optimization","This paper investigates quantum machine learning to optimize the beamforming
in a multiuser multiple-input single-output downlink system. We aim to combine
the power of quantum neural networks and the success of classical deep neural
networks to enhance the learning performance. Specifically, we propose two
hybrid quantum-classical neural networks to maximize the sum rate of a downlink
system. The first one proposes a quantum neural network employing parameterized
quantum circuits that follows a classical convolutional neural network. The
classical neural network can be jointly trained with the quantum neural network
or pre-trained leading to a fine-tuning transfer learning method. The second
one designs a quantum convolutional neural network to better extract features
followed by a classical deep neural network. Our results demonstrate the
feasibility of the proposed hybrid neural networks, and reveal that the first
method can achieve similar sum rate performance compared to a benchmark
classical neural network with significantly less training parameters; while the
second method can achieve higher sum rate especially in presence of many users
still with less training parameters. The robustness of the proposed methods is
verified using both software simulators and hardware emulators considering
noisy intermediate-scale quantum devices.","Juping Zhang, Gan Zheng, Toshiaki Koike-Akino, Kai-Kit Wong, Fraser Burton",2024,http://arxiv.org/abs/2408.04747v1
Cortex Neural Network: learning with Neural Network groups,"Neural Network has been successfully applied to many real-world problems,
such as image recognition and machine translation. However, for the current
architecture of neural networks, it is hard to perform complex cognitive tasks,
for example, to process the image and audio inputs together. Cortex, as an
important architecture in the brain, is important for animals to perform the
complex cognitive task. We view the architecture of Cortex in the brain as a
missing part in the design of the current artificial neural network. In this
paper, we purpose Cortex Neural Network (CrtxNN). The Cortex Neural Network is
an upper architecture of neural networks which motivated from cerebral cortex
in the brain to handle different tasks in the same learning system. It is able
to identify different tasks and solve them with different methods. In our
implementation, the Cortex Neural Network is able to process different
cognitive tasks and perform reflection to get a higher accuracy. We provide a
series of experiments to examine the capability of the cortex architecture on
traditional neural networks. Our experiments proved its ability on the Cortex
Neural Network can reach accuracy by 98.32% on MNIST and 62% on CIFAR10 at the
same time, which can promisingly reduce the loss by 40%.",Liyao Gao,2018,http://arxiv.org/abs/1804.03313v1
Assessing Intelligence in Artificial Neural Networks,"The purpose of this work was to develop of metrics to assess network
architectures that balance neural network size and task performance. To this
end, the concept of neural efficiency is introduced to measure neural layer
utilization, and a second metric called artificial intelligence quotient (aIQ)
was created to balance neural network performance and neural network
efficiency. To study aIQ and neural efficiency, two simple neural networks were
trained on MNIST: a fully connected network (LeNet-300-100) and a convolutional
neural network (LeNet-5). The LeNet-5 network with the highest aIQ was 2.32%
less accurate but contained 30,912 times fewer parameters than the highest
accuracy network. Both batch normalization and dropout layers were found to
increase neural efficiency. Finally, high aIQ networks are shown to be
memorization and overtraining resistant, capable of learning proper digit
classification with an accuracy of 92.51% even when 75% of the class labels are
randomized. These results demonstrate the utility of aIQ and neural efficiency
as metrics for balancing network performance and size.","Nicholas J. Schaub, Nathan Hotaling",2020,http://arxiv.org/abs/2006.02909v1
Rational Neural Network Controllers,"Neural networks have shown great success in many machine learning related
tasks, due to their ability to act as general function approximators. Recent
work has demonstrated the effectiveness of neural networks in control systems
(known as neural feedback loops), most notably by using a neural network as a
controller. However, one of the big challenges of this approach is that neural
networks have been shown to be sensitive to adversarial attacks. This means
that, unless they are designed properly, they are not an ideal candidate for
controllers due to issues with robustness and uncertainty, which are pivotal
aspects of control systems. There has been initial work on robustness to both
analyse and design dynamical systems with neural network controllers. However,
one prominent issue with these methods is that they use existing neural network
architectures tailored for traditional machine learning tasks. These structures
may not be appropriate for neural network controllers and it is important to
consider alternative architectures. This paper considers rational neural
networks and presents novel rational activation functions, which can be used
effectively in robustness problems for neural feedback loops. Rational
activation functions are replaced by a general rational neural network
structure, which is convex in the neural network's parameters. A method is
proposed to recover a stabilising controller from a Sum of Squares feasibility
test. This approach is then applied to a refined rational neural network which
is more compatible with Sum of Squares programming. Numerical examples show
that this method can successfully recover stabilising rational neural network
controllers for neural feedback loops with non-linear plants with noise and
parametric uncertainty.","Matthew Newton, Antonis Papachristodoulou",2023,http://arxiv.org/abs/2307.06287v1
Asymptotic Theory of Expectile Neural Networks,"Neural networks are becoming an increasingly important tool in applications.
However, neural networks are not widely used in statistical genetics. In this
paper, we propose a new neural networks method called expectile neural
networks. When the size of parameter is too large, the standard maximum
likelihood procedures may not work. We use sieve method to constrain parameter
space. And we prove its consistency and normality under nonparametric
regression framework.","Jinghang Lin, Xiaoxi Shen, Qing Lu",2020,http://arxiv.org/abs/2011.01218v1
"Combining Recurrent and Convolutional Neural Networks for Relation
  Classification","This paper investigates two different neural architectures for the task of
relation classification: convolutional neural networks and recurrent neural
networks. For both models, we demonstrate the effect of different architectural
choices. We present a new context representation for convolutional neural
networks for relation classification (extended middle context). Furthermore, we
propose connectionist bi-directional recurrent neural networks and introduce
ranking loss for their optimization. Finally, we show that combining
convolutional and recurrent neural networks using a simple voting scheme is
accurate enough to improve results. Our neural models achieve state-of-the-art
results on the SemEval 2010 relation classification task.","Ngoc Thang Vu, Heike Adel, Pankaj Gupta, Hinrich Schütze",2016,http://arxiv.org/abs/1605.07333v1
"A Comprehensive Review of Spiking Neural Networks: Interpretation,
  Optimization, Efficiency, and Best Practices","Biological neural networks continue to inspire breakthroughs in neural
network performance. And yet, one key area of neural computation that has been
under-appreciated and under-investigated is biologically plausible,
energy-efficient spiking neural networks, whose potential is especially
attractive for low-power, mobile, or otherwise hardware-constrained settings.
We present a literature review of recent developments in the interpretation,
optimization, efficiency, and accuracy of spiking neural networks. Key
contributions include identification, discussion, and comparison of
cutting-edge methods in spiking neural network optimization, energy-efficiency,
and evaluation, starting from first principles so as to be accessible to new
practitioners.","Kai Malcolm, Josue Casco-Rodriguez",2023,http://arxiv.org/abs/2303.10780v2
"Design and development of opto-neural processors for simulation of
  neural networks trained in image detection for potential implementation in
  hybrid robotics","Neural networks have been employed for a wide range of processing
applications like image processing, motor control, object detection and many
others. Living neural networks offer advantages of lower power consumption,
faster processing, and biological realism. Optogenetics offers high spatial and
temporal control over biological neurons and presents potential in training
live neural networks. This work proposes a simulated living neural network
trained indirectly by backpropagating STDP based algorithms using precision
activation by optogenetics achieving accuracy comparable to traditional neural
network training algorithms.",Sanjana Shetty,2024,http://arxiv.org/abs/2401.10289v1
Convex Formulation of Overparameterized Deep Neural Networks,"Analysis of over-parameterized neural networks has drawn significant
attention in recentyears. It was shown that such systems behave like convex
systems under various restrictedsettings, such as for two-level neural
networks, and when learning is only restricted locally inthe so-called neural
tangent kernel space around specialized initializations. However, there areno
theoretical techniques that can analyze fully trained deep neural networks
encountered inpractice. This paper solves this fundamental problem by
investigating such overparameterizeddeep neural networks when fully trained. We
generalize a new technique called neural feature repopulation, originally
introduced in (Fang et al., 2019a) for two-level neural networks, to analyze
deep neural networks. It is shown that under suitable representations,
overparameterized deep neural networks are inherently convex, and when
optimized, the system can learn effective features suitable for the underlying
learning task under mild conditions. This new analysis is consistent with
empirical observations that deep neural networks are capable of learning
efficient feature representations. Therefore, the highly unexpected result of
this paper can satisfactorily explain the practical success of deep neural
networks. Empirical studies confirm that predictions of our theory are
consistent with results observed in practice.","Cong Fang, Yihong Gu, Weizhong Zhang, Tong Zhang",2019,http://arxiv.org/abs/1911.07626v1
"Approximate Bisimulation Relations for Neural Networks and Application
  to Assured Neural Network Compression","In this paper, we propose a concept of approximate bisimulation relation for
feedforward neural networks. In the framework of approximate bisimulation
relation, a novel neural network merging method is developed to compute the
approximate bisimulation error between two neural networks based on
reachability analysis of neural networks. The developed method is able to
quantitatively measure the distance between the outputs of two neural networks
with the same inputs. Then, we apply the approximate bisimulation relation
results to perform neural networks model reduction and compute the compression
precision, i.e., assured neural networks compression. At last, using the
assured neural network compression, we accelerate the verification processes of
ACAS Xu neural networks to illustrate the effectiveness and advantages of our
proposed approximate bisimulation approach.","Weiming Xiang, Zhongzhu Shao",2022,http://arxiv.org/abs/2202.01214v1
"Optimal rates of approximation by shallow ReLU$^k$ neural networks and
  applications to nonparametric regression","We study the approximation capacity of some variation spaces corresponding to
shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth
functions are contained in these spaces with finite variation norms. For
functions with less smoothness, the approximation rates in terms of the
variation norm are established. Using these results, we are able to prove the
optimal approximation rates in terms of the number of neurons for shallow
ReLU$^k$ neural networks. It is also shown how these results can be used to
derive approximation bounds for deep neural networks and convolutional neural
networks (CNNs). As applications, we study convergence rates for nonparametric
regression using three ReLU neural network models: shallow neural network,
over-parameterized neural network, and CNN. In particular, we show that shallow
neural networks can achieve the minimax optimal rates for learning H\""older
functions, which complements recent results for deep neural networks. It is
also proven that over-parameterized (deep or shallow) neural networks can
achieve nearly optimal rates for nonparametric regression.","Yunfei Yang, Ding-Xuan Zhou",2023,http://arxiv.org/abs/2304.01561v3
"Understanding Vector-Valued Neural Networks and Their Relationship with
  Real and Hypercomplex-Valued Neural Networks","Despite the many successful applications of deep learning models for
multidimensional signal and image processing, most traditional neural networks
process data represented by (multidimensional) arrays of real numbers. The
intercorrelation between feature channels is usually expected to be learned
from the training data, requiring numerous parameters and careful training. In
contrast, vector-valued neural networks are conceived to process arrays of
vectors and naturally consider the intercorrelation between feature channels.
Consequently, they usually have fewer parameters and often undergo more robust
training than traditional neural networks. This paper aims to present a broad
framework for vector-valued neural networks, referred to as V-nets. In this
context, hypercomplex-valued neural networks are regarded as vector-valued
models with additional algebraic properties. Furthermore, this paper explains
the relationship between vector-valued and traditional neural networks.
Precisely, a vector-valued neural network can be obtained by placing
restrictions on a real-valued model to consider the intercorrelation between
feature channels. Finally, we show how V-nets, including hypercomplex-valued
neural networks, can be implemented in current deep-learning libraries as
real-valued networks.",Marcos Eduardo Valle,2023,http://arxiv.org/abs/2309.07716v2
Nonlinear Systems Identification Using Deep Dynamic Neural Networks,"Neural networks are known to be effective function approximators. Recently,
deep neural networks have proven to be very effective in pattern recognition,
classification tasks and human-level control to model highly nonlinear
realworld systems. This paper investigates the effectiveness of deep neural
networks in the modeling of dynamical systems with complex behavior. Three deep
neural network structures are trained on sequential data, and we investigate
the effectiveness of these networks in modeling associated characteristics of
the underlying dynamical systems. We carry out similar evaluations on select
publicly available system identification datasets. We demonstrate that deep
neural networks are effective model estimators from input-output data","Olalekan Ogunmolu, Xuejun Gu, Steve Jiang, Nicholas Gans",2016,http://arxiv.org/abs/1610.01439v1
Geometric Decomposition of Feed Forward Neural Networks,"There have been several attempts to mathematically understand neural networks
and many more from biological and computational perspectives. The field has
exploded in the last decade, yet neural networks are still treated much like a
black box. In this work we describe a structure that is inherent to a feed
forward neural network. This will provide a framework for future work on neural
networks to improve training algorithms, compute the homology of the network,
and other applications. Our approach takes a more geometric point of view and
is unlike other attempts to mathematically understand neural networks that rely
on a functional perspective.",Sven Cattell,2016,http://arxiv.org/abs/1612.02522v1
Neural Networks Architecture Evaluation in a Quantum Computer,"In this work, we propose a quantum algorithm to evaluate neural networks
architectures named Quantum Neural Network Architecture Evaluation (QNNAE). The
proposed algorithm is based on a quantum associative memory and the learning
algorithm for artificial neural networks. Unlike conventional algorithms for
evaluating neural network architectures, QNNAE does not depend on
initialization of weights. The proposed algorithm has a binary output and
results in 0 with probability proportional to the performance of the network.
And its computational cost is equal to the computational cost to train a neural
network.","Adenilton José da Silva, Rodolfo Luan F. de Oliveira",2017,http://arxiv.org/abs/1711.04759v1
Building Compact and Robust Deep Neural Networks with Toeplitz Matrices,"Deep neural networks are state-of-the-art in a wide variety of tasks,
however, they exhibit important limitations which hinder their use and
deployment in real-world applications. When developing and training neural
networks, the accuracy should not be the only concern, neural networks must
also be cost-effective and reliable. Although accurate, large neural networks
often lack these properties. This thesis focuses on the problem of training
neural networks which are not only accurate but also compact, easy to train,
reliable and robust to adversarial examples. To tackle these problems, we
leverage the properties of structured matrices from the Toeplitz family to
build compact and secure neural networks.",Alexandre Araujo,2021,http://arxiv.org/abs/2109.00959v1
Application of Neural Network in Optimization of Chemical Process,"Artificial neural network (ANN) has been widely used due to its strong
nonlinear mapping ability, fault tolerance and self-learning ability. This
article summarizes the development history of artificial neural networks,
introduces three common neural network types, BP neural network, RBF neural
network and convolutional neural network, and focuses on the practical
application in chemical process optimization, especially the results achieved
in multi-objective control optimization and process parameter improvement.","Fei Liang, Taowen Zhang",2021,http://arxiv.org/abs/2110.04942v1
Compact Matrix Quantum Group Equivariant Neural Networks,"We derive the existence of a new type of neural network, called a compact
matrix quantum group equivariant neural network, that learns from data that has
an underlying quantum symmetry. We apply the Woronowicz formulation of
Tannaka-Krein duality to characterise the weight matrices that appear in these
neural networks for any easy compact matrix quantum group. We show that compact
matrix quantum group equivariant neural networks contain, as a subclass, all
compact matrix group equivariant neural networks. Moreover, we obtain
characterisations of the weight matrices for many compact matrix group
equivariant neural networks that have not previously appeared in the machine
learning literature.",Edward Pearce-Crump,2023,http://arxiv.org/abs/2311.06358v1
"Universal Approximation Theorem for Vector- and Hypercomplex-Valued
  Neural Networks","The universal approximation theorem states that a neural network with one
hidden layer can approximate continuous functions on compact sets with any
desired precision. This theorem supports using neural networks for various
applications, including regression and classification tasks. Furthermore, it is
valid for real-valued neural networks and some hypercomplex-valued neural
networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural
networks. However, hypercomplex-valued neural networks are a type of
vector-valued neural network defined on an algebra with additional algebraic or
geometric properties. This paper extends the universal approximation theorem
for a wide range of vector-valued neural networks, including
hypercomplex-valued models as particular instances. Precisely, we introduce the
concept of non-degenerate algebra and state the universal approximation theorem
for neural networks defined on such algebras.","Marcos Eduardo Valle, Wington L. Vital, Guilherme Vieira",2024,http://arxiv.org/abs/2401.02277v2
Detecting Neural Trojans Through Merkle Trees,"Deep neural networks are utilized in a growing number of industries. Much of
the current literature focuses on the applications of deep neural networks
without discussing the security of the network itself. One security issue
facing deep neural networks is neural trojans. Through a neural trojan, a
malicious actor may force the deep neural network to act in unintended ways.
Several potential defenses have been proposed, but they are computationally
expensive, complex, or unusable in commercial applications. We propose Merkle
trees as a novel way to detect and isolate neural trojans.",Joshua Strubel,2023,http://arxiv.org/abs/2306.05368v1
"Why Quantization Improves Generalization: NTK of Binary Weight Neural
  Networks","Quantized neural networks have drawn a lot of attention as they reduce the
space and computational complexity during the inference. Moreover, there has
been folklore that quantization acts as an implicit regularizer and thus can
improve the generalizability of neural networks, yet no existing work
formalizes this interesting folklore. In this paper, we take the binary weights
in a neural network as random variables under stochastic rounding, and study
the distribution propagation over different layers in the neural network. We
propose a quasi neural network to approximate the distribution propagation,
which is a neural network with continuous parameters and smooth activation
function. We derive the neural tangent kernel (NTK) for this quasi neural
network, and show that the eigenvalue of NTK decays at approximately
exponential rate, which is comparable to that of Gaussian kernel with
randomized scale. This in turn indicates that the Reproducing Kernel Hilbert
Space (RKHS) of a binary weight neural network covers a strict subset of
functions compared with the one with real value weights. We use experiments to
verify that the quasi neural network we proposed can well approximate binary
weight neural network. Furthermore, binary weight neural network gives a lower
generalization gap compared with real value weight neural network, which is
similar to the difference between Gaussian kernel and Laplace kernel.","Kaiqi Zhang, Ming Yin, Yu-Xiang Wang",2022,http://arxiv.org/abs/2206.05916v1
Bayesian Neural Networks: Essentials,"Bayesian neural networks utilize probabilistic layers that capture
uncertainty over weights and activations, and are trained using Bayesian
inference. Since these probabilistic layers are designed to be drop-in
replacement of their deterministic counter parts, Bayesian neural networks
provide a direct and natural way to extend conventional deep neural networks to
support probabilistic deep learning. However, it is nontrivial to understand,
design and train Bayesian neural networks due to their complexities. We discuss
the essentials of Bayesian neural networks including duality (deep neural
networks, probabilistic models), approximate Bayesian inference, Bayesian
priors, Bayesian posteriors, and deep variational learning. We use TensorFlow
Probability APIs and code examples for illustration. The main problem with
Bayesian neural networks is that the architecture of deep neural networks makes
it quite redundant, and costly, to account for uncertainty for a large number
of successive layers. Hybrid Bayesian neural networks, which use few
probabilistic layers judicially positioned in the networks, provide a practical
solution.",Daniel T. Chang,2021,http://arxiv.org/abs/2106.13594v1
Fourier Neural Networks for Function Approximation,"The success of Neural networks in providing miraculous results when applied
to a wide variety of tasks is astonishing. Insight in the working can be
obtained by studying the universal approximation property of neural networks.
It is proved extensively that neural networks are universal approximators.
Further it is proved that deep Neural networks are better approximators. It is
specifically proved that for a narrow neural network to approximate a function
which is otherwise implemented by a deep Neural network, the network take
exponentially large number of neurons. In this work, we have implemented
existing methodologies for a variety of synthetic functions and identified
their deficiencies. Further, we examined that Fourier neural network is able to
perform fairly good with only two layers in the neural network. A modified
Fourier Neural network which has sinusoidal activation and two hidden layer is
proposed and the results are tabulated.","R Subhash Chandra Bose, Kakarla Yaswanth",2021,http://arxiv.org/abs/2111.08438v1
"Genetic cellular neural networks for generating three-dimensional
  geometry","There are a number of ways to procedurally generate interesting
three-dimensional shapes, and a method where a cellular neural network is
combined with a mesh growth algorithm is presented here. The aim is to create a
shape from a genetic code in such a way that a crude search can find
interesting shapes. Identical neural networks are placed at each vertex of a
mesh which can communicate with neural networks on neighboring vertices. The
output of the neural networks determine how the mesh grows, allowing
interesting shapes to be produced emergently, mimicking some of the complexity
of biological organism development. Since the neural networks' parameters can
be freely mutated, the approach is amenable for use in a genetic algorithm.",Hugo Martay,2016,http://arxiv.org/abs/1603.08551v1
Survey of Dropout Methods for Deep Neural Networks,"Dropout methods are a family of stochastic techniques used in neural network
training or inference that have generated significant research interest and are
widely used in practice. They have been successfully applied in neural network
regularization, model compression, and in measuring the uncertainty of neural
network outputs. While original formulated for dense neural network layers,
recent advances have made dropout methods also applicable to convolutional and
recurrent neural network layers. This paper summarizes the history of dropout
methods, their various applications, and current areas of research interest.
Important proposed methods are described in additional detail.","Alex Labach, Hojjat Salehinejad, Shahrokh Valaee",2019,http://arxiv.org/abs/1904.13310v2
"General Regression Neural Networks, Radial Basis Function Neural
  Networks, Support Vector Machines, and Feedforward Neural Networks","The aim of this project is to develop a code to discover the optimal sigma
value that maximum the F1 score and the optimal sigma value that maximizes the
accuracy and to find out if they are the same. Four algorithms which can be
used to solve this problem are: Genetic Regression Neural Networks (GRNNs),
Radial Based Function (RBF) Neural Networks (RBFNNs), Support Vector Machines
(SVMs) and Feedforward Neural Network (FFNNs).","Alison Jenkins, Vinika Gupta, Mary Lenoir",2019,http://arxiv.org/abs/1911.07115v1
On neural network kernels and the storage capacity problem,"In this short note, we reify the connection between work on the storage
capacity problem in wide two-layer treelike neural networks and the
rapidly-growing body of literature on kernel limits of wide neural networks.
Concretely, we observe that the ""effective order parameter"" studied in the
statistical mechanics literature is exactly equivalent to the infinite-width
Neural Network Gaussian Process Kernel. This correspondence connects the
expressivity and trainability of wide two-layer neural networks.","Jacob A. Zavatone-Veth, Cengiz Pehlevan",2022,http://arxiv.org/abs/2201.04669v1
Deep Neural Networks - A Brief History,Introduction to deep neural networks and their history.,Krzysztof J. Cios,2017,http://arxiv.org/abs/1701.05549v1
GPU Acceleration of Sparse Neural Networks,"In this paper, we use graphics processing units(GPU) to accelerate sparse and
arbitrary structured neural networks. Sparse networks have nodes in the network
that are not fully connected with nodes in preceding and following layers, and
arbitrary structure neural networks have different number of nodes in each
layers. Sparse Neural networks with arbitrary structures are generally created
in the processes like neural network pruning and evolutionary machine learning
strategies. We show that we can gain significant speedup for full activation of
such neural networks using graphical processing units. We do a prepossessing
step to determine dependency groups for all the nodes in a network, and use
that information to guide the progression of activation in the neural network.
Then we compute activation for each nodes in its own separate thread in the
GPU, which allows for massive parallelization. We use CUDA framework to
implement our approach and compare the results of sequential and GPU
implementations. Our results show that the activation of sparse neural networks
lends very well to GPU acceleration and can help speed up machine learning
strategies which generate such networks or other processes that have similar
structure.","Aavaas Gajurel, Sushil J. Louis, Frederick C Harris",2020,http://arxiv.org/abs/2005.04347v1
Neural Network Pruning as Spectrum Preserving Process,"Neural networks have achieved remarkable performance in various application
domains. Nevertheless, a large number of weights in pre-trained deep neural
networks prohibit them from being deployed on smartphones and embedded systems.
It is highly desirable to obtain lightweight versions of neural networks for
inference in edge devices. Many cost-effective approaches were proposed to
prune dense and convolutional layers that are common in deep neural networks
and dominant in the parameter space. However, a unified theoretical foundation
for the problem mostly is missing. In this paper, we identify the close
connection between matrix spectrum learning and neural network training for
dense and convolutional layers and argue that weight pruning is essentially a
matrix sparsification process to preserve the spectrum. Based on the analysis,
we also propose a matrix sparsification algorithm tailored for neural network
pruning that yields better pruning result. We carefully design and conduct
experiments to support our arguments. Hence we provide a consolidated viewpoint
for neural network pruning and enhance the interpretability of deep neural
networks by identifying and preserving the critical neural weights.","Shibo Yao, Dantong Yu, Ioannis Koutis",2023,http://arxiv.org/abs/2307.08982v1
On Hiding Neural Networks Inside Neural Networks,"Modern neural networks often contain significantly more parameters than the
size of their training data. We show that this excess capacity provides an
opportunity for embedding secret machine learning models within a trained
neural network. Our novel framework hides the existence of a secret neural
network with arbitrary desired functionality within a carrier network. We prove
theoretically that the secret network's detection is computationally infeasible
and demonstrate empirically that the carrier network does not compromise the
secret network's disguise. Our paper introduces a previously unknown
steganographic technique that can be exploited by adversaries if left
unchecked.","Chuan Guo, Ruihan Wu, Kilian Q. Weinberger",2020,http://arxiv.org/abs/2002.10078v3
"Deep physical neural networks enabled by a backpropagation algorithm for
  arbitrary physical systems","Deep neural networks have become a pervasive tool in science and engineering.
However, modern deep neural networks' growing energy requirements now
increasingly limit their scaling and broader use. We propose a radical
alternative for implementing deep neural network models: Physical Neural
Networks. We introduce a hybrid physical-digital algorithm called Physics-Aware
Training to efficiently train sequences of controllable physical systems to act
as deep neural networks. This method automatically trains the functionality of
any sequence of real physical systems, directly, using backpropagation, the
same technique used for modern deep neural networks. To illustrate their
generality, we demonstrate physical neural networks with three diverse physical
systems-optical, mechanical, and electrical. Physical neural networks may
facilitate unconventional machine learning hardware that is orders of magnitude
faster and more energy efficient than conventional electronic processors.","Logan G. Wright, Tatsuhiro Onodera, Martin M. Stein, Tianyu Wang, Darren T. Schachter, Zoey Hu, Peter L. McMahon",2021,http://arxiv.org/abs/2104.13386v1
Consistency of Neural Networks with Regularization,"Neural networks have attracted a lot of attention due to its success in
applications such as natural language processing and computer vision. For large
scale data, due to the tremendous number of parameters in neural networks,
overfitting is an issue in training neural networks. To avoid overfitting, one
common approach is to penalize the parameters especially the weights in neural
networks. Although neural networks has demonstrated its advantages in many
applications, the theoretical foundation of penalized neural networks has not
been well-established. Our goal of this paper is to propose the general
framework of neural networks with regularization and prove its consistency.
Under certain conditions, the estimated neural network will converge to true
underlying function as the sample size increases. The method of sieves and the
theory on minimal neural networks are used to overcome the issue of
unidentifiability for the parameters. Two types of activation functions:
hyperbolic tangent function(Tanh) and rectified linear unit(ReLU) have been
taken into consideration. Simulations have been conducted to verify the
validation of theorem of consistency.","Xiaoxi Shen, Jinghang Lin",2022,http://arxiv.org/abs/2207.01538v1
"Understanding Weight Similarity of Neural Networks via Chain
  Normalization Rule and Hypothesis-Training-Testing","We present a weight similarity measure method that can quantify the weight
similarity of non-convex neural networks. To understand the weight similarity
of different trained models, we propose to extract the feature representation
from the weights of neural networks. We first normalize the weights of neural
networks by introducing a chain normalization rule, which is used for weight
representation learning and weight similarity measure. We extend the
traditional hypothesis-testing method to a hypothesis-training-testing
statistical inference method to validate the hypothesis on the weight
similarity of neural networks. With the chain normalization rule and the new
statistical inference, we study the weight similarity measure on Multi-Layer
Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural
Network (RNN), and find that the weights of an identical neural network
optimized with the Stochastic Gradient Descent (SGD) algorithm converge to a
similar local solution in a metric space. The weight similarity measure
provides more insight into the local solutions of neural networks. Experiments
on several datasets consistently validate the hypothesis of weight similarity
measure.","Guangcong Wang, Guangrun Wang, Wenqi Liang, Jianhuang Lai",2022,http://arxiv.org/abs/2208.04369v1
Graph Metanetworks for Processing Diverse Neural Architectures,"Neural networks efficiently encode learned information within their
parameters. Consequently, many tasks can be unified by treating neural networks
themselves as input data. When doing so, recent studies demonstrated the
importance of accounting for the symmetries and geometry of parameter spaces.
However, those works developed architectures tailored to specific networks such
as MLPs and CNNs without normalization layers, and generalizing such
architectures to other types of networks can be challenging. In this work, we
overcome these challenges by building new metanetworks - neural networks that
take weights from other neural networks as input. Put simply, we carefully
build graphs representing the input neural networks and process the graphs
using graph neural networks. Our approach, Graph Metanetworks (GMNs),
generalizes to neural architectures where competing methods struggle, such as
multi-head attention layers, normalization layers, convolutional layers, ResNet
blocks, and group-equivariant linear layers. We prove that GMNs are expressive
and equivariant to parameter permutation symmetries that leave the input neural
network functions unchanged. We validate the effectiveness of our method on
several metanetwork tasks over diverse neural network architectures.","Derek Lim, Haggai Maron, Marc T. Law, Jonathan Lorraine, James Lucas",2023,http://arxiv.org/abs/2312.04501v2
Deep Neural Networks for Pattern Recognition,"In the field of pattern recognition research, the method of using deep neural
networks based on improved computing hardware recently attracted attention
because of their superior accuracy compared to conventional methods. Deep
neural networks simulate the human visual system and achieve human equivalent
accuracy in image classification, object detection, and segmentation. This
chapter introduces the basic structure of deep neural networks that simulate
human neural networks. Then we identify the operational processes and
applications of conditional generative adversarial networks, which are being
actively researched based on the bottom-up and top-down mechanisms, the most
important functions of the human visual perception process. Finally, recent
developments in training strategies for effective learning of complex deep
neural networks are addressed.","Kyongsik Yun, Alexander Huyen, Thomas Lu",2018,http://arxiv.org/abs/1809.09645v1
"Evidence, Definitions and Algorithms regarding the Existence of
  Cohesive-Convergence Groups in Neural Network Optimization","Understanding the convergence process of neural networks is one of the most
complex and crucial issues in the field of machine learning. Despite the close
association of notable successes in this domain with the convergence of
artificial neural networks, this concept remains predominantly theoretical. In
reality, due to the non-convex nature of the optimization problems that
artificial neural networks tackle, very few trained networks actually achieve
convergence. To expand recent research efforts on artificial-neural-network
convergence, this paper will discuss a different approach based on observations
of cohesive-convergence groups emerging during the optimization process of an
artificial neural network.",Thien An L. Nguyen,2024,http://arxiv.org/abs/2403.05610v1
"Hybrid deep neural network based prediction method for unsteady flows
  with moving boundaries","A novel hybrid deep neural network architecture is designed to capture the
spatial-temporal features of unsteady flows around moving boundaries directly
from high-dimensional unsteady flow fields data. The hybrid deep neural network
is constituted by the convolutional neural network (CNN), improved
convolutional Long-Short Term Memory neural network (ConvLSTM) and
deconvolutional neural network (DeCNN). Flow fields at future time step can be
predicted through flow fields by previous time steps and boundary positions at
those steps by the novel hybrid deep neural network. Unsteady wake flows around
a forced oscillation cylinder with various amplitudes are calculated to
establish the datasets as training samples for training the hybrid deep neural
networks. The trained hybrid deep neural networks are then tested by predicting
the unsteady flow fields around a forced oscillation cylinder with new
amplitude. The effect of neural network structure parameters on prediction
accuracy was analyzed. The hybrid deep neural network, constituted by the best
parameter combination, is used to predict the flow fields in the future time.
The predicted flow fields are in good agreement with those calculated directly
by computational fluid dynamic solver, which means that this kind of deep
neural network can capture accurate spatial-temporal information from the
spatial-temporal series of unsteady flows around moving boundaries. The result
shows the potential capability of this kind novel hybrid deep neural network in
flow control for vibrating cylinder, where the fast calculation of
high-dimensional nonlinear unsteady flow around moving boundaries is needed.","Renkun Han, Zhong Zhang, Yixing Wang, Ziyang Liu, Yang Zhang, Gang Chen",2020,http://arxiv.org/abs/2006.00690v1
$e$PCA: High Dimensional Exponential Family PCA,"Many applications, such as photon-limited imaging and genomics, involve large
datasets with noisy entries from exponential family distributions. It is of
interest to estimate the covariance structure and principal components of the
noiseless distribution. Principal Component Analysis (PCA), the standard method
for this setting, can be inefficient when the noise is non-Gaussian.
  We develop $e$PCA (exponential family PCA), a new methodology for PCA on
exponential family distributions. $e$PCA can be used for dimensionality
reduction and denoising of large data matrices. $e$PCA involves the
eigendecomposition of a new covariance matrix estimator, constructed in a
simple and deterministic way using moment calculations, shrinkage, and random
matrix theory.
  We provide several theoretical justifications for our estimator, including
the finite-sample convergence rate, and the Marchenko-Pastur law in high
dimensions. $e$PCA compares favorably to PCA and various PCA alternatives for
exponential families, in simulations as well as in XFEL and SNP data analysis.
An open-source implementation is available.","Lydia T. Liu, Edgar Dobriban, Amit Singer",2016,http://arxiv.org/abs/1611.05550v2
Bucketed PCA Neural Networks with Neurons Mirroring Signals,"The bucketed PCA neural network (PCA-NN) with transforms is developed here in
an effort to benchmark deep neural networks (DNN's), for problems on supervised
classification. Most classical PCA models apply PCA to the entire training data
set to establish a reductive representation and then employ non-network tools
such as high-order polynomial classifiers. In contrast, the bucketed PCA-NN
applies PCA to individual buckets which are constructed in two consecutive
phases, as well as retains a genuine architecture of a neural network. This
facilitates a fair apple-to-apple comparison to DNN's, esp. to reveal that a
major chunk of accuracy achieved by many impressive DNN's could possibly be
explained by the bucketed PCA-NN (e.g., 96% out of 98% for the MNIST data set
as an example). Compared with most DNN's, the three building blocks of the
bucketed PCA-NN are easier to comprehend conceptually - PCA, transforms, and
bucketing for error correction. Furthermore, unlike the somewhat quasi-random
neurons ubiquitously observed in DNN's, the PCA neurons resemble or mirror the
input signals and are more straightforward to decipher as a result.",Jackie Shen,2021,http://arxiv.org/abs/2108.00605v1
Principal Component Analysis: A Generalized Gini Approach,"A principal component analysis based on the generalized Gini correlation
index is proposed (Gini PCA). The Gini PCA generalizes the standard PCA based
on the variance. It is shown, in the Gaussian case, that the standard PCA is
equivalent to the Gini PCA. It is also proven that the dimensionality reduction
based on the generalized Gini correlation matrix, that relies on city-block
distances, is robust to outliers. Monte Carlo simulations and an application on
cars data (with outliers) show the robustness of the Gini PCA and provide
different interpretations of the results compared with the variance PCA.","Charpentier, Arthur, Mussard, Stephane, Tea Ouraga",2019,http://arxiv.org/abs/1910.10133v1
Sparse PCA on fixed-rank matrices,"Sparse PCA is the optimization problem obtained from PCA by adding a sparsity
constraint on the principal components. Sparse PCA is NP-hard and hard to
approximate even in the single-component case. In this paper we settle the
computational complexity of sparse PCA with respect to the rank of the
covariance matrix. We show that, if the rank of the covariance matrix is a
fixed value, then there is an algorithm that solves sparse PCA to global
optimality, whose running time is polynomial in the number of features. We also
prove a similar result for the version of sparse PCA which requires the
principal components to have disjoint supports.",Alberto Del Pia,2022,http://arxiv.org/abs/2201.02487v1
"When Collaborative Filtering is not Collaborative: Unfairness of PCA for
  Recommendations","We study the fairness of dimensionality reduction methods for
recommendations. We focus on the established method of principal component
analysis (PCA), which identifies latent components and produces a low-rank
approximation via the leading components while discarding the trailing
components. Prior works have defined notions of ""fair PCA""; however, these
definitions do not answer the following question: what makes PCA unfair? We
identify two underlying mechanisms of PCA that induce unfairness at the item
level. The first negatively impacts less popular items, due to the fact that
less popular items rely on trailing latent components to recover their values.
The second negatively impacts the highly popular items, since the leading PCA
components specialize in individual popular items instead of capturing
similarities between items. To address these issues, we develop a
polynomial-time algorithm, Item-Weighted PCA, a modification of PCA that uses
item-specific weights in the objective. On a stylized class of matrices, we
prove that Item-Weighted PCA using a specific set of weights minimizes a
popularity-normalized error metric. Our evaluations on real-world datasets show
that Item-Weighted PCA not only improves overall recommendation quality by up
to $0.1$ item-level AUC-ROC but also improves on both popular and less popular
items.","David Liu, Jackie Baek, Tina Eliassi-Rad",2023,http://arxiv.org/abs/2310.09687v1
"Cancer-Net PCa-Data: An Open-Source Benchmark Dataset for Prostate
  Cancer Clinical Decision Support using Synthetic Correlated Diffusion Imaging
  Data","The recent introduction of synthetic correlated diffusion (CDI$^s$) imaging
has demonstrated significant potential in the realm of clinical decision
support for prostate cancer (PCa). CDI$^s$ is a new form of magnetic resonance
imaging (MRI) designed to characterize tissue characteristics through the joint
correlation of diffusion signal attenuation across different Brownian motion
sensitivities. Despite the performance improvement, the CDI$^s$ data for PCa
has not been previously made publicly available. In our commitment to advance
research efforts for PCa, we introduce Cancer-Net PCa-Data, an open-source
benchmark dataset of volumetric CDI$^s$ imaging data of PCa patients.
Cancer-Net PCa-Data consists of CDI$^s$ volumetric images from a patient cohort
of 200 patient cases, along with full annotations (gland masks, tumor masks,
and PCa diagnosis for each tumor). We also analyze the demographic and label
region diversity of Cancer-Net PCa-Data for potential biases. Cancer-Net
PCa-Data is the first-ever public dataset of CDI$^s$ imaging data for PCa, and
is a part of the global open-source initiative dedicated to advancement in
machine learning and imaging research to aid clinicians in the global fight
against cancer.","Hayden Gunraj, Chi-en Amy Tai, Alexander Wong",2023,http://arxiv.org/abs/2311.11647v1
On the efficiency-loss free ordering-robustness of product-PCA,"This article studies the robustness of the eigenvalue ordering, an important
issue when estimating the leading eigen-subspace by principal component
analysis (PCA). In Yata and Aoshima (2010), cross-data-matrix PCA (CDM-PCA) was
proposed and shown to have smaller bias than PCA in estimating eigenvalues.
While CDM-PCA has the potential to achieve better estimation of the leading
eigen-subspace than the usual PCA, its robustness is not well recognized. In
this article, we first develop a more stable variant of CDM-PCA, which we call
product-PCA (PPCA), that provides a more convenient formulation for theoretical
investigation. Secondly, we prove that, in the presence of outliers, PPCA is
more robust than PCA in maintaining the correct ordering of leading
eigenvalues. The robustness gain in PPCA comes from the random data partition,
and it does not rely on a data down-weighting scheme as most robust statistical
methods do. This enables us to establish the surprising finding that, when
there are no outliers, PPCA and PCA share the same asymptotic distribution.
That is, the robustness gain of PPCA in estimating the leading eigen-subspace
has no efficiency loss in comparison with PCA. Simulation studies and a face
data example are presented to show the merits of PPCA. In conclusion, PPCA has
a good potential to replace the role of the usual PCA in real applications
whether outliers are present or not.","Hung Hung, Su-Yun Huang",2023,http://arxiv.org/abs/2302.11124v3
A Compared Study Between Some Subspace Based Algorithms,"The technology of face recognition has made some progress in recent years.
After studying the PCA, 2DPCA, R1-PCA, L1-PCA, KPCA and KECA algorithms, in
this paper ECA (2DECA) is proposed by extracting features in PCA (2DPCA) based
on Renyi entropy contribution. And then we conduct a study on the 2DL1-PCA and
2DR1-PCA algorithms. On the basis of the experiments, this paper compares the
difference of the recognition accuracy and operational efficiency between the
above algorithms.","Xing Liu, Xiao-Jun Wu, Zhen Liu, He-Feng Yin",2019,http://arxiv.org/abs/1912.10657v1
Efficient fair PCA for fair representation learning,"We revisit the problem of fair principal component analysis (PCA), where the
goal is to learn the best low-rank linear approximation of the data that
obfuscates demographic information. We propose a conceptually simple approach
that allows for an analytic solution similar to standard PCA and can be
kernelized. Our methods have the same complexity as standard PCA, or kernel
PCA, and run much faster than existing methods for fair PCA based on
semidefinite programming or manifold optimization, while achieving similar
results.","Matthäus Kleindessner, Michele Donini, Chris Russell, Muhammad Bilal Zafar",2023,http://arxiv.org/abs/2302.13319v1
Torus Principal Component Analysis with an Application to RNA Structures,"There are several cutting edge applications needing PCA methods for data on
tori and we propose a novel torus-PCA method with important properties that can
be generally applied. There are two existing general methods: tangent space PCA
and geodesic PCA. However, unlike tangent space PCA, our torus-PCA honors the
cyclic topology of the data space whereas, unlike geodesic PCA, our torus-PCA
produces a variety of non-winding, non-dense descriptors. This is achieved by
deforming tori into spheres and then using a variant of the recently developed
principle nested spheres analysis. This PCA analysis involves a step of small
sphere fitting and we provide an improved test to avoid overfitting. However,
deforming tori into spheres creates singularities. We introduce a data-adaptive
pre-clustering technique to keep the singularities away from the data. For the
frequently encountered case that the residual variance around the PCA main
component is small, we use a post-mode hunting technique for more fine-grained
clustering. Thus in general, there are three successive interrelated key steps
of torus-PCA in practice: pre-clustering, deformation, and post-mode hunting.
We illustrate our method with two recently studied RNA structure (tori) data
sets: one is a small RNA data set which is established as the benchmark for PCA
and we validate our method through this data. Another is a large RNA data set
(containing the small RNA data set) for which we show that our method provides
interpretable principal components as well as giving further insight into its
structure.","Benjamin Eltzner, Stephan Huckemann, Kanti V. Mardia",2015,http://arxiv.org/abs/1511.04993v1
"Accelerating Wireless Federated Learning via Nesterov's Momentum and
  Distributed Principle Component Analysis","A wireless federated learning system is investigated by allowing a server and
workers to exchange uncoded information via orthogonal wireless channels. Since
the workers frequently upload local gradients to the server via
bandwidth-limited channels, the uplink transmission from the workers to the
server becomes a communication bottleneck. Therefore, a one-shot distributed
principle component analysis (PCA) is leveraged to reduce the dimension of
uploaded gradients such that the communication bottleneck is relieved. A
PCA-based wireless federated learning (PCA-WFL) algorithm and its accelerated
version (i.e., PCA-AWFL) are proposed based on the low-dimensional gradients
and the Nesterov's momentum. For the non-convex loss functions, a finite-time
analysis is performed to quantify the impacts of system hyper-parameters on the
convergence of the PCA-WFL and PCA-AWFL algorithms. The PCA-AWFL algorithm is
theoretically certified to converge faster than the PCA-WFL algorithm. Besides,
the convergence rates of PCA-WFL and PCA-AWFL algorithms quantitatively reveal
the linear speedup with respect to the number of workers over the vanilla
gradient descent algorithm. Numerical results are used to demonstrate the
improved convergence rates of the proposed PCA-WFL and PCA-AWFL algorithms over
the benchmarks.","Yanjie Dong, Luya Wang, Yuanfang Chi, Jia Wang, Haijun Zhang, Fei Richard Yu, Victor C. M. Leung, Xiping Hu",2023,http://arxiv.org/abs/2303.17885v1
Comparison of PCA with ICA from data distribution perspective,"We performed an empirical comparison of ICA and PCA algorithms by applying
them on two simulated noisy time series with varying distribution parameters
and level of noise. In general, ICA shows better results than PCA because it
takes into account higher moments of data distribution. On the other hand, PCA
remains quite sensitive to the level of correlations among signals.",Miron Ivanov,2017,http://arxiv.org/abs/1709.10222v1
"PCA of high dimensional random walks with comparison to neural network
  training","One technique to visualize the training of neural networks is to perform PCA
on the parameters over the course of training and to project to the subspace
spanned by the first few PCA components. In this paper we compare this
technique to the PCA of a high dimensional random walk. We compute the
eigenvalues and eigenvectors of the covariance of the trajectory and prove that
in the long trajectory and high dimensional limit most of the variance is in
the first few PCA components, and that the projection of the trajectory onto
any subspace spanned by PCA components is a Lissajous curve. We generalize
these results to a random walk with momentum and to an Ornstein-Uhlenbeck
processes (i.e., a random walk in a quadratic potential) and show that in high
dimensions the walk is not mean reverting, but will instead be trapped at a
fixed distance from the minimum. We finally compare the distribution of PCA
variances and the PCA projected training trajectories of a linear model trained
on CIFAR-10 and ResNet-50-v2 trained on Imagenet and find that the distribution
of PCA variances resembles a random walk with drift.","Joseph M. Antognini, Jascha Sohl-Dickstein",2018,http://arxiv.org/abs/1806.08805v1
Internal Partial Combinatory Algebras and their Slices,"A partial combinatory algebra (PCA) is a set equipped with a partial binary
operation that models a notion of computability. This paper studies a
generalization of PCAs, introduced by W. Stekelenburg, where a PCA is not a set
but an object in a given regular category. The corresponding class of
categories of assemblies is closed both under taking small products and under
slicing, which is to be contrasted with the situation for ordinary PCAs. We
describe these two constructions explicitly at the level of PCAs, allowing us
to compute a number of examples of products and slices of PCAs. Moreover, we
show how PCAs can be transported along regular functors, enabling us to compare
PCAs constructed over different base categories. Via a Grothendieck
construction, this leads to a (2-)category whose objects are PCAs and whose
arrows are generalized applicative morphisms. This category has small products,
which correspond to the small products of categories of assemblies, and it has
finite coproducts in a weak sense. Finally, we give a criterion when a functor
between categories of assemblies that is induced by an applicative morphism has
a right adjoint, by generalizing the notion of computational density introduced
by P. Hofstra and J. van Oosten.",Jetze Zoethout,2019,http://arxiv.org/abs/1910.09816v1
Toroidal PCA via density ridges,"Principal Component Analysis (PCA) is a well-known linear dimension-reduction
technique designed for Euclidean data. In a wide spectrum of applied fields,
however, it is common to observe multivariate circular data (also known as
toroidal data), rendering spurious the use of PCA on it due to the periodicity
of its support. This paper introduces Toroidal Ridge PCA (TR-PCA), a novel
construction of PCA for bivariate circular data that leverages the concept of
density ridges as a flexible first principal component analog. Two reference
bivariate circular distributions, the bivariate sine von Mises and the
bivariate wrapped Cauchy, are employed as the parametric distributional basis
of TR-PCA. Efficient algorithms are presented to compute density ridges for
these two distribution models. A complete PCA methodology adapted to toroidal
data (including scores, variance decomposition, and resolution of edge cases)
is introduced and implemented in the companion R package ridgetorus. The
usefulness of TR-PCA is showcased with a novel case study involving the
analysis of ocean currents on the coast of Santa Barbara.","Eduardo García-Portugués, Arturo Prieto-Tirado",2022,http://arxiv.org/abs/2212.10856v2
Streaming Probabilistic PCA for Missing Data with Heteroscedastic Noise,"Streaming principal component analysis (PCA) is an integral tool in
large-scale machine learning for rapidly estimating low-dimensional subspaces
of very high dimensional and high arrival-rate data with missing entries and
corrupting noise. However, modern trends increasingly combine data from a
variety of sources, meaning they may exhibit heterogeneous quality across
samples. Since standard streaming PCA algorithms do not account for non-uniform
noise, their subspace estimates can quickly degrade. On the other hand, the
recently proposed Heteroscedastic Probabilistic PCA Technique (HePPCAT)
addresses this heterogeneity, but it was not designed to handle missing entries
and streaming data, nor does it adapt to non-stationary behavior in time series
data. This paper proposes the Streaming HeteroscedASTic Algorithm for PCA
(SHASTA-PCA) to bridge this divide. SHASTA-PCA employs a stochastic alternating
expectation maximization approach that jointly learns the low-rank latent
factors and the unknown noise variances from streaming data that may have
missing entries and heteroscedastic noise, all while maintaining a low memory
and computational footprint. Numerical experiments validate the superior
subspace estimation of our method compared to state-of-the-art streaming PCA
algorithms in the heteroscedastic setting. Finally, we illustrate SHASTA-PCA
applied to highly-heterogeneous real data from astronomy.","Kyle Gilman, David Hong, Jeffrey A. Fessler, Laura Balzano",2023,http://arxiv.org/abs/2310.06277v1
TL-PCA: Transfer Learning of Principal Component Analysis,"Principal component analysis (PCA) can be significantly limited when there is
too few examples of the target data of interest. We propose a transfer learning
approach to PCA (TL-PCA) where knowledge from a related source task is used in
addition to the scarce data of a target task. Our TL-PCA has two versions, one
that uses a pretrained PCA solution of the source task, and another that uses
the source data. Our proposed approach extends the PCA optimization objective
with a penalty on the proximity of the target subspace and the source subspace
as given by the pretrained source model or the source data. This optimization
is solved by eigendecomposition for which the number of data-dependent
eigenvectors (i.e., principal directions of TL-PCA) is not limited to the
number of target data examples, which is a root cause that limits the standard
PCA performance. Accordingly, our results for image datasets show that the
representation of test data is improved by TL-PCA for dimensionality reduction
where the learned subspace dimension is lower or higher than the number of
target data examples.","Sharon Hendy, Yehuda Dar",2024,http://arxiv.org/abs/2410.10805v1
"Intelligent Reflecting Surface Aided Pilot Contamination Attack and Its
  Countermeasure","Pilot contamination attack (PCA) in a time division duplex wireless
communication system is considered, where an eavesdropper (Eve) attacks the
reverse pilot transmission phase in order to wiretap the data transmitted from
a transmitter, Alice, to a receiver, Bob. We propose a new PCA scheme for Eve,
wherein Eve does not emit any signal by itself but uses an intelligent
reflecting surface (IRS) to reflect the pilot sent by Bob to Alice. The
proposed new PCA scheme, referred to as IRS-PCA, increases the signal leakage
from Alice to the IRS during the data transmission phase, which is then
reflected by the IRS to Eve in order to improve the wiretapping capability of
Eve. The proposed IRS-PCA scheme disables many existing countermeasures on PCA
due to the fact that with IRS-PCA, Eve no longer needs to know the pilot
sequence of Bob, and therefore, poses severe threat to the security of the
legitimate wireless communication system. In view of this, the problems of 1)
IRS-PCA detection and 2) secure transmission under IRSPCA are considered in
this paper. For IRS-PCA detection, a generalized cumulative sum (GCUSUM)
detection procedure is proposed based on the framework of quickest detection,
aiming at detecting the occurrence of IRS-PCA as soon as possible once it
occurs. For secure transmission under IRS-PCA, a cooperative channel estimation
scheme is proposed to estimate the channel of the IRS, based on which
zero-forcing beamforming is designed to reduce signal leakage.","Ke-Wen Huang, Hui-Ming Wang",2020,http://arxiv.org/abs/2009.08512v1
"A co-kurtosis PCA based dimensionality reduction with nonlinear
  reconstruction using neural networks","For turbulent reacting flows, identification of low-dimensional
representations of the thermo-chemical state space is vitally important,
primarily to significantly reduce the computational cost of device-scale
simulations. Principal component analysis (PCA), and its variants, is a widely
employed class of methods. Recently, an alternative technique that focuses on
higher-order statistical interactions, co-kurtosis PCA (CoK-PCA), has been
shown to effectively provide a low-dimensional representation by capturing the
stiff chemical dynamics associated with spatiotemporally localized reaction
zones. While its effectiveness has only been demonstrated based on a priori
analysis with linear reconstruction, in this work, we employ nonlinear
techniques to reconstruct the full thermo-chemical state and evaluate the
efficacy of CoK-PCA compared to PCA. Specifically, we combine a CoK-PCA/PCA
based dimensionality reduction (encoding) with an artificial neural network
(ANN) based reconstruction (decoding) and examine a priori the reconstruction
errors of the thermo-chemical state. In addition, we evaluate the errors in
species production rates and heat release rates that are nonlinear functions of
the reconstructed state as a measure of the overall accuracy of the
dimensionality reduction technique. We employ four datasets to assess
CoK-PCA/PCA coupled with ANN-based reconstruction: a homogeneous reactor for
autoignition of an ethylene/air mixture that has conventional single-stage
ignition kinetics, a dimethyl ether (DME)/air mixture which has two-stage
ignition kinetics, a one-dimensional freely propagating premixed ethylene/air
laminar flame, and a two-dimensional homogeneous charge compression ignition of
ethanol. The analyses demonstrate the robustness of the CoK-PCA based
low-dimensional manifold with ANN reconstruction in accurately capturing the
data, specifically from the reaction zones.","Dibyajyoti Nayak, Anirudh Jonnalagadda, Uma Balakrishnan, Hemanth Kolla, Konduri Aditya",2023,http://arxiv.org/abs/2307.03289v1
Log-PCA versus Geodesic PCA of histograms in the Wasserstein space,"This paper is concerned by the statistical analysis of data sets whose
elements are random histograms. For the purpose of learning principal modes of
variation from such data, we consider the issue of computing the PCA of
histograms with respect to the 2-Wasserstein distance between probability
measures. To this end, we propose to compare the methods of log-PCA and
geodesic PCA in the Wasserstein space as introduced by Bigot et al. (2015) and
Seguy and Cuturi (2015). Geodesic PCA involves solving a non-convex
optimization problem. To solve it approximately, we propose a novel
forward-backward algorithm. This allows a detailed comparison between log-PCA
and geodesic PCA of one-dimensional histograms, which we carry out using
various data sets, and stress the benefits and drawbacks of each method. We
extend these results for two-dimensional data and compare both methods in that
setting.","Elsa Cazelles, Vivien Seguy, Jérémie Bigot, Marco Cuturi, Nicolas Papadakis",2017,http://arxiv.org/abs/1708.08143v1
Unsupervised and Supervised Principal Component Analysis: Tutorial,"This is a detailed tutorial paper which explains the Principal Component
Analysis (PCA), Supervised PCA (SPCA), kernel PCA, and kernel SPCA. We start
with projection, PCA with eigen-decomposition, PCA with one and multiple
projection directions, properties of the projection matrix, reconstruction
error minimization, and we connect to autoencoder. Then, PCA with singular
value decomposition, dual PCA, and kernel PCA are covered. SPCA using both
scoring and Hilbert-Schmidt independence criterion are explained. Kernel SPCA
using both direct and dual approaches are then introduced. We cover all cases
of projection and reconstruction of training and out-of-sample data. Finally,
some simulations are provided on Frey and AT&T face datasets for verifying the
theory in practice.","Benyamin Ghojogh, Mark Crowley",2019,http://arxiv.org/abs/1906.03148v2
"Empirical Bayes Covariance Decomposition, and a solution to the Multiple
  Tuning Problem in Sparse PCA","Sparse Principal Components Analysis (PCA) has been proposed as a way to
improve both interpretability and reliability of PCA. However, use of sparse
PCA in practice is hindered by the difficulty of tuning the multiple
hyperparameters that control the sparsity of different PCs (the ""multiple
tuning problem"", MTP). Here we present a solution to the MTP using Empirical
Bayes methods. We first introduce a general formulation for penalized PCA of a
data matrix $\mathbf{X}$, which includes some existing sparse PCA methods as
special cases. We show that this formulation also leads to a penalized
decomposition of the covariance (or Gram) matrix, $\mathbf{X}^T\mathbf{X}$. We
introduce empirical Bayes versions of these penalized problems, in which the
penalties are determined by prior distributions that are estimated from the
data by maximum likelihood rather than cross-validation. The resulting
""Empirical Bayes Covariance Decomposition"" provides a principled and efficient
solution to the MTP in sparse PCA, and one that can be immediately extended to
incorporate other structural assumptions (e.g. non-negative PCA). We illustrate
the effectiveness of this approach on both simulated and real data examples.","Joonsuk Kang, Matthew Stephens",2023,http://arxiv.org/abs/2312.03274v1
"On the asymptotic properties of product-PCA under the high-dimensional
  setting","Principal component analysis (PCA) is a widely used dimension reduction
method, but its performance is known to be non-robust to outliers. Recently,
product-PCA (PPCA) has been shown to possess the efficiency-loss free
ordering-robustness property: (i) in the absence of outliers, PPCA and PCA
share the same asymptotic distributions; (ii), in the presence of outliers,
PPCA is more ordering-robust than PCA in estimating the leading eigenspace.
PPCA is thus different from the conventional robust PCA methods, and may
deserve further investigations. In this article, we study the high-dimensional
statistical properties of the PPCA eigenvalues via the techniques of random
matrix theory. In particular, we derive the critical value for being distant
spiked eigenvalues, the limiting values of the sample spiked eigenvalues, and
the limiting spectral distribution of PPCA. Similar to the case of PCA, the
explicit forms of the asymptotic properties of PPCA become available under the
special case of the simple spiked model. These results enable us to more
clearly understand the superiorities of PPCA in comparison with PCA. Numerical
studies are conducted to verify our results.","Hung Hung, Chi-Chun Yeh, Su-Yun Huang",2024,http://arxiv.org/abs/2407.19725v2
Extensions of Scott's Graph Model and Kleene's Second Algebra,"We use a way to extend partial combinatory algebras (pcas) by forcing them to
represent certain functions. In the case of Scott's Graph model, equality is
computable relative to the complement function. However, the converse is not
true. This creates a hierarchy of pcas which relates to similar structures of
extensions on other pcas. We study one such structure on Kleene's second model
and one on a pca equivalent but not isomorphic to it. For the recursively
enumerable sub pca of the Graph model, results differ as we can compute the
(partial) complement function using the equality.","Jaap van Oosten, Niels Voorneveld",2016,http://arxiv.org/abs/1610.04050v1
"Shapley Values of Reconstruction Errors of PCA for Explaining Anomaly
  Detection","We present a method to compute the Shapley values of reconstruction errors of
principal component analysis (PCA), which is particularly useful in explaining
the results of anomaly detection based on PCA. Because features are usually
correlated when PCA-based anomaly detection is applied, care must be taken in
computing a value function for the Shapley values. We utilize the probabilistic
view of PCA, particularly its conditional distribution, to exactly compute a
value function for the Shapely values. We also present numerical examples,
which imply that the Shapley values are advantageous for explaining detected
anomalies than raw reconstruction errors of each feature.",Naoya Takeishi,2019,http://arxiv.org/abs/1909.03495v2
"2DR1-PCA and 2DL1-PCA: two variant 2DPCA algorithms based on none L2
  norm","In this paper, two novel methods: 2DR1-PCA and 2DL1-PCA are proposed for face
recognition. Compared to the traditional 2DPCA algorithm, 2DR1-PCA and 2DL1-PCA
are based on the R1 norm and L1 norm, respectively. The advantage of these
proposed methods is they are less sensitive to outliers. These proposed methods
are tested on the ORL, YALE and XM2VTS databases and the performance of the
related methods is compared experimentally.","Xing Liu, Xiao-Jun Wu, Zi-Qi Li",2019,http://arxiv.org/abs/1912.10768v1
"Fourier spectra for nonuniform phase-shifting algorithms based on
  principal component analysis","We develop an error-free, nonuniform phase-stepping algorithm (nPSA) based on
principal component analysis (PCA). PCA-based algorithms typically give
phase-demodulation errors when applied to nonuniform phase-shifted
interferograms. We present a straightforward way to correct those PCA
phase-demodulation errors. We give mathematical formulas to fully analyze
PCA-based nPSA (PCA-nPSA). These formulas give a) the PCA-nPSA frequency
transfer function (FTF), b) its corrected Lissajous figure, c) the corrected
PCA-nPSA formula, d) its harmonic robustness, and e) its signal-to-noise-ratio
(SNR). We show that the PCA-nPSA can be seen as a linear quadrature filter, and
as consequence, one can find its FTF. Using the FTF, we show why plain PCA
often fails to demodulate nonuniform phase-shifted fringes. Previous works on
PCA-nPSA (without FTF), give specific numerical/experimental fringe data to
""visually demonstrate"" that their new nPSA works better than competitors. This
often leads to biased/favorable fringe pattern selections which ""visually
demonstrate"" the superior performance of their new nPSA. This biasing is herein
totally avoided because we provide figures-of-merit formulas based on linear
systems and stochastic process theories. However, and for illustrative purposes
only, we provide specific fringe data phase-demodulation, including
comprehensive analysis and comparisons.","Manuel Servin, Moises Padilla, Guillermo Garnica, Gonzalo Paez",2019,http://arxiv.org/abs/1904.01071v1
"Supervised Discriminative Sparse PCA for Com-Characteristic Gene
  Selection and Tumor Classification on Multiview Biological Data","Principal Component Analysis (PCA) has been used to study the pathogenesis of
diseases. To enhance the interpretability of classical PCA, various improved
PCA methods have been proposed to date. Among these, a typical method is the
so-called sparse PCA, which focuses on seeking sparse loadings. However, the
performance of these methods is still far from satisfactory due to their
limitation of using unsupervised learning methods; moreover, the class
ambiguity within the sample is high. To overcome this problem, this study
developed a new PCA method, which is named the Supervised Discriminative Sparse
PCA (SDSPCA). The main innovation of this method is the incorporation of
discriminative information and sparsity into the PCA model. Specifically, in
contrast to the traditional sparse PCA, which imposes sparsity on the loadings,
here, sparse components are obtained to represent the data. Furthermore, via
linear transformation, the sparse components approximate the given label
information. On the one hand, sparse components improve interpretability over
traditional PCA, while on the other hand, they are have discriminative
abilities suitable for classification purposes. A simple algorithm is developed
and its convergence proof is provided. The SDSPCA has been applied to common
characteristic gene selection (com-characteristic gene) and tumor
classification on multi-view biological data. The sparsity and classification
performance of the SDSPCA are empirically verified via abundant, reasonable,
and effective experiments, and the obtained results demonstrate that SDSPCA
outperforms other state-of-the-art methods.","Chun-Mei Feng, Yong Xu, Jin-Xing Liu, Ying-Lian Gao, Chun-Hou Zheng",2019,http://arxiv.org/abs/1905.11837v1
"FAST-PCA: A Fast and Exact Algorithm for Distributed Principal Component
  Analysis","Principal Component Analysis (PCA) is a fundamental data preprocessing tool
in the world of machine learning. While PCA is often thought of as a
dimensionality reduction method, the purpose of PCA is actually two-fold:
dimension reduction and uncorrelated feature learning. Furthermore, the
enormity of the dimensions and sample size in the modern day datasets have
rendered the centralized PCA solutions unusable. In that vein, this paper
reconsiders the problem of PCA when data samples are distributed across nodes
in an arbitrarily connected network. While a few solutions for distributed PCA
exist, those either overlook the uncorrelated feature learning aspect of the
PCA, tend to have high communication overhead that makes them inefficient
and/or lack `exact' or `global' convergence guarantees. To overcome these
aforementioned issues, this paper proposes a distributed PCA algorithm termed
FAST-PCA (Fast and exAct diSTributed PCA). The proposed algorithm is efficient
in terms of communication and is proven to converge linearly and exactly to the
principal components, leading to dimension reduction as well as uncorrelated
features. The claims are further supported by experimental results.","Arpita Gang, Waheed U. Bajwa",2021,http://arxiv.org/abs/2108.12373v2
"Fair principal component analysis (PCA): minorization-maximization
  algorithms for Fair PCA, Fair Robust PCA and Fair Sparse PCA","In this paper we propose a new iterative algorithm to solve the fair PCA
(FPCA) problem. We start with the max-min fair PCA formulation originally
proposed in [1] and derive a simple and efficient iterative algorithm which is
based on the minorization-maximization (MM) approach. The proposed algorithm
relies on the relaxation of a semi-orthogonality constraint which is proved to
be tight at every iteration of the algorithm. The vanilla version of the
proposed algorithm requires solving a semi-definite program (SDP) at every
iteration, which can be further simplified to a quadratic program by
formulating the dual of the surrogate maximization problem. We also propose two
important reformulations of the fair PCA problem: a) fair robust PCA -- which
can handle outliers in the data, and b) fair sparse PCA -- which can enforce
sparsity on the estimated fair principal components. The proposed algorithms
are computationally efficient and monotonically increase their respective
design objectives at every iteration. An added feature of the proposed
algorithms is that they do not require the selection of any hyperparameter
(except for the fair sparse PCA case where a penalty parameter that controls
the sparsity has to be chosen by the user). We numerically compare the
performance of the proposed methods with two of the state-of-the-art approaches
on synthetic data sets and a real-life data set.","Prabhu Babu, Petre Stoica",2023,http://arxiv.org/abs/2305.05963v1
"A Deep-Learning-Based Geological Parameterization for History Matching
  Complex Models","A new low-dimensional parameterization based on principal component analysis
(PCA) and convolutional neural networks (CNN) is developed to represent complex
geological models. The CNN-PCA method is inspired by recent developments in
computer vision using deep learning. CNN-PCA can be viewed as a generalization
of an existing optimization-based PCA (O-PCA) method. Both CNN-PCA and O-PCA
entail post-processing a PCA model to better honor complex geological features.
In CNN-PCA, rather than use a histogram-based regularization as in O-PCA, a new
regularization involving a set of metrics for multipoint statistics is
introduced. The metrics are based on summary statistics of the nonlinear filter
responses of geological models to a pre-trained deep CNN. In addition, in the
CNN-PCA formulation presented here, a convolutional neural network is trained
as an explicit transform function that can post-process PCA models quickly.
CNN-PCA is shown to provide both unconditional and conditional realizations
that honor the geological features present in reference SGeMS geostatistical
realizations for a binary channelized system. Flow statistics obtained through
simulation of random CNN-PCA models closely match results for random SGeMS
models for a demanding case in which O-PCA models lead to significant
discrepancies. Results for history matching are also presented. In this
assessment CNN-PCA is applied with derivative-free optimization, and a subspace
randomized maximum likelihood method is used to provide multiple posterior
models. Data assimilation and significant uncertainty reduction are achieved
for existing wells, and physically reasonable predictions are also obtained for
new wells. Finally, the CNN-PCA method is extended to a more complex
non-stationary bimodal deltaic fan system, and is shown to provide high-quality
realizations for this challenging example.","Yimin Liu, Wenyue Sun, Louis J. Durlofsky",2018,http://arxiv.org/abs/1807.02716v1
Semi-Orthogonal Multilinear PCA with Relaxed Start,"Principal component analysis (PCA) is an unsupervised method for learning
low-dimensional features with orthogonal projections. Multilinear PCA methods
extend PCA to deal with multidimensional data (tensors) directly via
tensor-to-tensor projection or tensor-to-vector projection (TVP). However,
under the TVP setting, it is difficult to develop an effective multilinear PCA
method with the orthogonality constraint. This paper tackles this problem by
proposing a novel Semi-Orthogonal Multilinear PCA (SO-MPCA) approach. SO-MPCA
learns low-dimensional features directly from tensors via TVP by imposing the
orthogonality constraint in only one mode. This formulation results in more
captured variance and more learned features than full orthogonality. For better
generalization, we further introduce a relaxed start (RS) strategy to get
SO-MPCA-RS by fixing the starting projection vectors, which increases the bias
and reduces the variance of the learning model. Experiments on both face (2D)
and gait (3D) data demonstrate that SO-MPCA-RS outperforms other competing
algorithms on the whole, and the relaxed start strategy is also effective for
other TVP-based PCA methods.","Qiquan Shi, Haiping Lu",2015,http://arxiv.org/abs/1504.08142v2
"Sparse Generalized Principal Component Analysis for Large-scale
  Applications beyond Gaussianity","Principal Component Analysis (PCA) is a dimension reduction technique. It
produces inconsistent estimators when the dimensionality is moderate to high,
which is often the problem in modern large-scale applications where algorithm
scalability and model interpretability are difficult to achieve, not to mention
the prevalence of missing values. While existing sparse PCA methods alleviate
inconsistency, they are constrained to the Gaussian assumption of classical PCA
and fail to address algorithm scalability issues. We generalize sparse PCA to
the broad exponential family distributions under high-dimensional setup, with
built-in treatment for missing values. Meanwhile we propose a family of
iterative sparse generalized PCA (SG-PCA) algorithms such that despite the
non-convexity and non-smoothness of the optimization task, the loss function
decreases in every iteration. In terms of ease and intuitive parameter tuning,
our sparsity-inducing regularization is far superior to the popular Lasso.
Furthermore, to promote overall scalability, accelerated gradient is integrated
for fast convergence, while a progressive screening technique gradually
squeezes out nuisance dimensions of a large-scale problem for feasible
optimization. High-dimensional simulation and real data experiments demonstrate
the efficiency and efficacy of SG-PCA.","Qiaoya Zhang, Yiyuan She",2015,http://arxiv.org/abs/1512.03883v2
"On the performance overhead tradeoff of distributed principal component
  analysis via data partitioning","Principal component analysis (PCA) is not only a fundamental dimension
reduction method, but is also a widely used network anomaly detection
technique. Traditionally, PCA is performed in a centralized manner, which has
poor scalability for large distributed systems, on account of the large network
bandwidth cost required to gather the distributed state at a fusion center.
Consequently, several recent works have proposed various distributed PCA
algorithms aiming to reduce the communication overhead incurred by PCA without
losing its inferential power. This paper evaluates the tradeoff between
communication cost and solution quality of two distributed PCA algorithms on a
real domain name system (DNS) query dataset from a large network. We also apply
the distributed PCA algorithm in the area of network anomaly detection and
demonstrate that the detection accuracy of both distributed PCA-based methods
has little degradation in quality, yet achieves significant savings in
communication bandwidth.","Ni An, Steven Weber",2015,http://arxiv.org/abs/1512.05172v3
Towards a Theoretical Analysis of PCA for Heteroscedastic Data,"Principal Component Analysis (PCA) is a method for estimating a subspace
given noisy samples. It is useful in a variety of problems ranging from
dimensionality reduction to anomaly detection and the visualization of high
dimensional data. PCA performs well in the presence of moderate noise and even
with missing data, but is also sensitive to outliers. PCA is also known to have
a phase transition when noise is independent and identically distributed;
recovery of the subspace sharply declines at a threshold noise variance.
Effective use of PCA requires a rigorous understanding of these behaviors. This
paper provides a step towards an analysis of PCA for samples with
heteroscedastic noise, that is, samples that have non-uniform noise variances
and so are no longer identically distributed. In particular, we provide a
simple asymptotic prediction of the recovery of a one-dimensional subspace from
noisy heteroscedastic samples. The prediction enables: a) easy and efficient
calculation of the asymptotic performance, and b) qualitative reasoning to
understand how PCA is impacted by heteroscedasticity (such as outliers).","David Hong, Laura Balzano, Jeffrey A. Fessler",2016,http://arxiv.org/abs/1610.03595v1
Principal Component Analysis: A Natural Approach to Data Exploration,"Principal component analysis (PCA) is often used for analyzing data in the
most diverse areas. In this work, we report an integrated approach to several
theoretical and practical aspects of PCA. We start by providing, in an
intuitive and accessible manner, the basic principles underlying PCA and its
applications. Next, we present a systematic, though no exclusive, survey of
some representative works illustrating the potential of PCA applications to a
wide range of areas. An experimental investigation of the ability of PCA for
variance explanation and dimensionality reduction is also developed, which
confirms the efficacy of PCA and also shows that standardizing or not the
original data can have important effects on the obtained results. Overall, we
believe the several covered issues can assist researchers from the most diverse
areas in using and interpreting PCA.","Felipe L. Gewers, Gustavo R. Ferreira, Henrique F. de Arruda, Filipi N. Silva, Cesar H. Comin, Diego R. Amancio, Luciano da F. Costa",2018,http://arxiv.org/abs/1804.02502v2
PCA by Optimisation of Symmetric Functions has no Spurious Local Optima,"Principal Component Analysis (PCA) finds the best linear representation of
data, and is an indispensable tool in many learning and inference tasks.
Classically, principal components of a dataset are interpreted as the
directions that preserve most of its ""energy"", an interpretation that is
theoretically underpinned by the celebrated Eckart-Young-Mirsky Theorem.
  This paper introduces many other ways of performing PCA, with various
geometric interpretations, and proves that the corresponding family of
non-convex programs have no spurious local optima, while possessing only strict
saddle points. These programs therefore loosely behave like convex problems and
can be efficiently solved to global optimality, for example, with certain
variants of the stochastic gradient descent.
  Beyond providing new geometric interpretations and enhancing our theoretical
understanding of PCA, our findings might pave the way for entirely new
approaches to structured dimensionality reduction, such as sparse PCA and
nonnegative matrix factorisation. More specifically, we study an unconstrained
formulation of PCA using determinant optimisation that might provide an elegant
alternative to the deflating scheme commonly used in sparse PCA.","Raphael A. Hauser, Armin Eftekhari",2018,http://arxiv.org/abs/1805.07459v3
Discriminative Principal Component Analysis: A REVERSE THINKING,"In this paper, we propose a novel approach named by Discriminative Principal
Component Analysis which is abbreviated as Discriminative PCA in order to
enhance separability of PCA by Linear Discriminant Analysis (LDA). The proposed
method performs feature extraction by determining a linear projection that
captures the most scattered discriminative information. The most innovation of
Discriminative PCA is performing PCA on discriminative matrix rather than
original sample matrix. For calculating the required discriminative matrix
under low complexity, we exploit LDA on a converted matrix to obtain
within-class matrix and between-class matrix thereof. During the computation
process, we utilise direct linear discriminant analysis (DLDA) to solve the
encountered SSS problem. For evaluating the performances of Discriminative PCA
in face recognition, we analytically compare it with DLAD and PCA on four well
known facial databases, they are PIE, FERET, YALE and ORL respectively. Results
in accuracy and running time obtained by nearest neighbour classifier are
compared when different number of training images per person used. Not only the
superiority and outstanding performance of Discriminative PCA showed in
recognition rate, but also the comparable results of running time.",Hanli Qiao,2019,http://arxiv.org/abs/1903.04963v1
Empirical Bayes PCA in high dimensions,"When the dimension of data is comparable to or larger than the number of data
samples, Principal Components Analysis (PCA) may exhibit problematic
high-dimensional noise. In this work, we propose an Empirical Bayes PCA method
that reduces this noise by estimating a joint prior distribution for the
principal components. EB-PCA is based on the classical Kiefer-Wolfowitz
nonparametric MLE for empirical Bayes estimation, distributional results
derived from random matrix theory for the sample PCs, and iterative refinement
using an Approximate Message Passing (AMP) algorithm. In theoretical ""spiked""
models, EB-PCA achieves Bayes-optimal estimation accuracy in the same settings
as an oracle Bayes AMP procedure that knows the true priors. Empirically,
EB-PCA significantly improves over PCA when there is strong prior structure,
both in simulation and on quantitative benchmarks constructed from the 1000
Genomes Project and the International HapMap Project. An illustration is
presented for analysis of gene expression data obtained by single-cell RNA-seq.","Xinyi Zhong, Chang Su, Zhou Fan",2020,http://arxiv.org/abs/2012.11676v3
Improved sparse PCA method for face and image recognition,"Face recognition is the very significant field in pattern recognition area.
It has multiple applications in military and finance, to name a few. In this
paper, the combination of the sparse PCA with the nearest-neighbor method (and
with the kernel ridge regression method) will be proposed and will be applied
to solve the face recognition problem. Experimental results illustrate that the
accuracy of the combination of the sparse PCA method (using the proximal
gradient method and the FISTA method) and one specific classification system
may be lower than the accuracy of the combination of the PCA method and one
specific classification system but sometimes the combination of the sparse PCA
method (using the proximal gradient method or the FISTA method) and one
specific classification system leads to better accuracy. Moreover, we recognize
that the process computing the sparse PCA algorithm using the FISTA method is
always faster than the process computing the sparse PCA algorithm using the
proximal gradient method.","Loc Hoang Tran, Tuan Tran, An Mai",2021,http://arxiv.org/abs/2112.00207v1
Ensemble Principal Component Analysis,"Efficient representations of data are essential for processing, exploration,
and human understanding, and Principal Component Analysis (PCA) is one of the
most common dimensionality reduction techniques used for the analysis of large,
multivariate datasets today. Two well-known limitations of the method include
sensitivity to outliers and noise and no clear methodology for the uncertainty
quantification of the principle components or their associated explained
variances. Whereas previous work has focused on each of these problems
individually, we propose a scalable method called Ensemble PCA (EPCA) that
addresses them simultaneously for data which has an inherently low-rank
structure. EPCA combines boostrapped PCA with k-means cluster analysis to
handle challenges associated with sign-ambiguity and the re-ordering of
components in the PCA subsamples. EPCA provides a noise-resistant extension of
PCA that lends itself naturally to uncertainty quantification. We test EPCA on
data corrupted with white noise, sparse noise, and outliers against both
classical PCA and Robust PCA (RPCA) and show that EPCA performs competitively
across different noise scenarios, with a clear advantage on datasets containing
outliers and orders of magnitude reduction in computational cost compared to
RPCA.","Olga Dorabiala, Aleksandr Aravkin, J. Nathan Kutz",2023,http://arxiv.org/abs/2311.01826v1
"Implications of Computer Vision Driven Assistive Technologies Towards
  Individuals with Visual Impairment","Computer vision based technology is becoming ubiquitous in society. One
application area that has seen an increase in computer vision is assistive
technologies, specifically for those with visual impairment. Research has shown
the ability of computer vision models to achieve tasks such provide scene
captions, detect objects and recognize faces. Although assisting individuals
with visual impairment with these tasks increases their independence and
autonomy, concerns over bias, privacy and potential usefulness arise. This
paper addresses the positive and negative implications computer vision based
assistive technologies have on individuals with visual impairment, as well as
considerations for computer vision researchers and developers in order to
mitigate the amount of negative implications.","Linda Wang, Alexander Wong",2019,http://arxiv.org/abs/1905.07844v1
"Multiband NFC for High-Throughput Wireless Computer Vision Sensor
  Network","Vision sensors lie in the heart of computer vision. In many computer vision
applications, such as AR/VR, non-contacting near-field communication (NFC) with
high throughput is required to transfer information to algorithms. In this
work, we proposed a novel NFC system which utilizes multiple frequency bands to
achieve high throughput.","F. Li, J. Du",2017,http://arxiv.org/abs/1707.03720v1
Deep Learning vs. Traditional Computer Vision,"Deep Learning has pushed the limits of what was possible in the domain of
Digital Image Processing. However, that is not to say that the traditional
computer vision techniques which had been undergoing progressive development in
years prior to the rise of DL have become obsolete. This paper will analyse the
benefits and drawbacks of each approach. The aim of this paper is to promote a
discussion on whether knowledge of classical computer vision techniques should
be maintained. The paper will also explore how the two sides of computer vision
can be combined. Several recent hybrid methodologies are reviewed which have
demonstrated the ability to improve computer vision performance and to tackle
problems not suited to Deep Learning. For example, combining traditional
computer vision techniques with Deep Learning has been popular in emerging
domains such as Panoramic Vision and 3D vision for which Deep Learning models
have not yet been fully optimised","Niall O' Mahony, Sean Campbell, Anderson Carvalho, Suman Harapanahalli, Gustavo Velasco-Hernandez, Lenka Krpalkova, Daniel Riordan, Joseph Walsh",2019,http://arxiv.org/abs/1910.13796v1
Enhancing camera surveillance using computer vision: a research note,"$\mathbf{Purpose}$ - The growth of police operated surveillance cameras has
out-paced the ability of humans to monitor them effectively. Computer vision is
a possible solution. An ongoing research project on the application of computer
vision within a municipal police department is described. The paper aims to
discuss these issues.
  $\mathbf{Design/methodology/approach}$ - Following the demystification of
computer vision technology, its potential for police agencies is developed
within a focus on computer vision as a solution for two common surveillance
camera tasks (live monitoring of multiple surveillance cameras and summarizing
archived video files). Three unaddressed research questions (can specialized
computer vision applications for law enforcement be developed at this time, how
will computer vision be utilized within existing public safety camera
monitoring rooms, and what are the system-wide impacts of a computer vision
capability on local criminal justice systems) are considered.
  $\mathbf{Findings}$ - Despite computer vision becoming accessible to law
enforcement agencies the impact of computer vision has not been discussed or
adequately researched. There is little knowledge of computer vision or its
potential in the field.
  $\mathbf{Originality/value}$ - This paper introduces and discusses computer
vision from a law enforcement perspective and will be valuable to police
personnel tasked with monitoring large camera networks and considering computer
vision as a system upgrade.","Haroon Idrees, Mubarak Shah, Ray Surette",2018,http://arxiv.org/abs/1808.03998v1
"Are object detection assessment criteria ready for maritime computer
  vision?","Maritime vessels equipped with visible and infrared cameras can complement
other conventional sensors for object detection. However, application of
computer vision techniques in maritime domain received attention only recently.
The maritime environment offers its own unique requirements and challenges.
Assessment of the quality of detections is a fundamental need in computer
vision. However, the conventional assessment metrics suitable for usual object
detection are deficient in the maritime setting. Thus, a large body of related
work in computer vision appears inapplicable to the maritime setting at the
first sight. We discuss the problem of defining assessment metrics suitable for
maritime computer vision. We consider new bottom edge proximity metrics as
assessment metrics for maritime computer vision. These metrics indicate that
existing computer vision approaches are indeed promising for maritime computer
vision and can play a foundational role in the emerging field of maritime
computer vision.","Dilip K. Prasad, Huixu Dong, Deepu Rajan, Chai Quek",2018,http://arxiv.org/abs/1809.04659v2
BMVC 2019: Workshop on Interpretable and Explainable Machine Vision,"Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable
Machine Vision, Cardiff, UK, September 12, 2019.",Alun Preece,2019,http://arxiv.org/abs/1909.07245v1
"SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for
  Large-scale Vision-Language Models","Large-scale Vision-Language Models (LVLMs) have significantly advanced with
text-aligned vision inputs. They have made remarkable progress in computer
vision tasks by aligning text modality with vision inputs. There are also
endeavors to incorporate multi-vision sensors beyond RGB, including thermal,
depth, and medical X-ray images. However, we observe that current LVLMs view
images taken from multi-vision sensors as if they were in the same RGB domain
without considering the physical characteristics of multi-vision sensors. They
fail to convey the fundamental multi-vision sensor information from the dataset
and the corresponding contextual knowledge properly. Consequently, alignment
between the information from the actual physical environment and the text is
not achieved correctly, making it difficult to answer complex sensor-related
questions that consider the physical environment. In this paper, we aim to
establish a multi-vision Sensor Perception And Reasoning benchmarK called SPARK
that can reduce the fundamental multi-vision sensor information gap between
images and multi-vision sensors. We generated 6,248 vision-language test
samples to investigate multi-vision sensory perception and multi-vision sensory
reasoning on physical sensor knowledge proficiency across different formats,
covering different types of sensor-related questions. We utilized these samples
to assess ten leading LVLMs. The results showed that most models displayed
deficiencies in multi-vision sensory reasoning to varying extents. Codes and
data are available at https://github.com/top-yun/SPARK","Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee, Yong Man Ro",2024,http://arxiv.org/abs/2408.12114v3
"Vision Transformers in Medical Computer Vision -- A Contemplative
  Retrospection","Recent escalation in the field of computer vision underpins a huddle of
algorithms with the magnificent potential to unravel the information contained
within images. These computer vision algorithms are being practised in medical
image analysis and are transfiguring the perception and interpretation of
Imaging data. Among these algorithms, Vision Transformers are evolved as one of
the most contemporary and dominant architectures that are being used in the
field of computer vision. These are immensely utilized by a plenty of
researchers to perform new as well as former experiments. Here, in this article
we investigate the intersection of Vision Transformers and Medical images and
proffered an overview of various ViTs based frameworks that are being used by
different researchers in order to decipher the obstacles in Medical Computer
Vision. We surveyed the application of Vision transformers in different areas
of medical computer vision such as image-based disease classification,
anatomical structure segmentation, registration, region-based lesion Detection,
captioning, report generation, reconstruction using multiple medical imaging
modalities that greatly assist in medical diagnosis and hence treatment
process. Along with this, we also demystify several imaging modalities used in
Medical Computer Vision. Moreover, to get more insight and deeper
understanding, self-attention mechanism of transformers is also explained
briefly. Conclusively, we also put some light on available data sets, adopted
methodology, their performance measures, challenges and their solutions in form
of discussion. We hope that this review article will open future directions for
researchers in medical computer vision.","Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar, Huma Ameer, Muhammad Ali, Muhammad Moazam Fraz",2022,http://arxiv.org/abs/2203.15269v1
Adapting Computer Vision Algorithms for Omnidirectional Video,"Omnidirectional (360{\deg}) video has got quite popular because it provides a
highly immersive viewing experience. For computer vision algorithms, it poses
several challenges, like the special (equirectangular) projection commonly
employed and the huge image size. In this work, we give a high-level overview
of these challenges and outline strategies how to adapt computer vision
algorithm for the specifics of omnidirectional video.",Hannes Fassold,2019,http://arxiv.org/abs/1907.09233v1
Real-time Tracking Based on Neuromrophic Vision,"Real-time tracking is an important problem in computer vision in which most
methods are based on the conventional cameras. Neuromorphic vision is a concept
defined by incorporating neuromorphic vision sensors such as silicon retinas in
vision processing system. With the development of the silicon technology,
asynchronous event-based silicon retinas that mimic neuro-biological
architectures has been developed in recent years. In this work, we combine the
vision tracking algorithm of computer vision with the information encoding
mechanism of event-based sensors which is inspired from the neural rate coding
mechanism. The real-time tracking of single object with the advantage of high
speed of 100 time bins per second is successfully realized. Our method
demonstrates that the computer vision methods could be used for the
neuromorphic vision processing and we can realize fast real-time tracking using
neuromorphic vision sensors compare to the conventional camera.","Hongmin Li, Pei Jing, Guoqi Li",2015,http://arxiv.org/abs/1510.05275v1
Reconfiguring the Imaging Pipeline for Computer Vision,"Advancements in deep learning have ignited an explosion of research on
efficient hardware for embedded computer vision. Hardware vision acceleration,
however, does not address the cost of capturing and processing the image data
that feeds these algorithms. We examine the role of the image signal processing
(ISP) pipeline in computer vision to identify opportunities to reduce
computation and save energy. The key insight is that imaging pipelines should
be designed to be configurable: to switch between a traditional photography
mode and a low-power vision mode that produces lower-quality image data
suitable only for computer vision. We use eight computer vision algorithms and
a reversible pipeline simulation tool to study the imaging system's impact on
vision performance. For both CNN-based and classical vision algorithms, we
observe that only two ISP stages, demosaicing and gamma compression, are
critical for task performance. We propose a new image sensor design that can
compensate for skipping these stages. The sensor design features an adjustable
resolution and tunable analog-to-digital converters (ADCs). Our proposed
imaging system's vision mode disables the ISP entirely and configures the
sensor to produce subsampled, lower-precision image data. This vision mode can
save ~75% of the average energy of a baseline photography mode while having
only a small impact on vision task accuracy.","Mark Buckler, Suren Jayasuriya, Adrian Sampson",2017,http://arxiv.org/abs/1705.04352v3
"Integration and Performance Analysis of Artificial Intelligence and
  Computer Vision Based on Deep Learning Algorithms","This paper focuses on the analysis of the application effectiveness of the
integration of deep learning and computer vision technologies. Deep learning
achieves a historic breakthrough by constructing hierarchical neural networks,
enabling end-to-end feature learning and semantic understanding of images. The
successful experiences in the field of computer vision provide strong support
for training deep learning algorithms. The tight integration of these two
fields has given rise to a new generation of advanced computer vision systems,
significantly surpassing traditional methods in tasks such as machine vision
image classification and object detection. In this paper, typical image
classification cases are combined to analyze the superior performance of deep
neural network models while also pointing out their limitations in
generalization and interpretability, proposing directions for future
improvements. Overall, the efficient integration and development trend of deep
learning with massive visual data will continue to drive technological
breakthroughs and application expansion in the field of computer vision, making
it possible to build truly intelligent machine vision systems. This deepening
fusion paradigm will powerfully promote unprecedented tasks and functions in
computer vision, providing stronger development momentum for related
disciplines and industries.","Bo Liu, Liqiang Yu, Chang Che, Qunwei Lin, Hao Hu, Xinyu Zhao",2023,http://arxiv.org/abs/2312.12872v1
Scaling Up Computer Vision Neural Networks Using Fast Fourier Transform,"Deep Learning-based Computer Vision field has recently been trying to explore
larger kernels for convolution to effectively scale up Convolutional Neural
Networks. Simultaneously, new paradigm of models such as Vision Transformers
find it difficult to scale up to larger higher resolution images due to their
quadratic complexity in terms of input sequence. In this report, Fast Fourier
Transform is utilised in various ways to provide some solutions to these
issues.",Siddharth Agrawal,2023,http://arxiv.org/abs/2302.12185v1
Are Vision-Language Models Truly Understanding Multi-vision Sensor?,"Large-scale Vision-Language Models (VLMs) have advanced by aligning vision
inputs with text, significantly improving performance in computer vision tasks.
Moreover, for VLMs to be effectively utilized in real-world applications, an
understanding of diverse multi-vision sensor data, such as thermal, depth, and
X-ray information, is essential. However, we find that current VLMs process
multi-vision sensor images without deep understanding of sensor information,
disregarding each sensor's unique physical properties. This limitation
restricts their capacity to interpret and respond to complex questions
requiring multi-vision sensor reasoning. To address this, we propose a novel
Multi-vision Sensor Perception and Reasoning (MS-PR) benchmark, assessing VLMs
on their capacity for sensor-specific reasoning. Moreover, we introduce Diverse
Negative Attributes (DNA) optimization to enable VLMs to perform deep reasoning
on multi-vision sensor tasks, helping to bridge the core information gap
between images and sensor data. Extensive experimental results validate that
the proposed DNA method can significantly improve the multi-vision sensor
reasoning for VLMs.","Sangyun Chung, Youngjoon Yu, Youngchae Chee, Se Yeon Kim, Byung-Kwan Lee, Yong Man Ro",2024,http://arxiv.org/abs/2412.20750v1
Ethics and Creativity in Computer Vision,"This paper offers a retrospective of what we learnt from organizing the
workshop *Ethical Considerations in Creative applications of Computer Vision*
at CVPR 2021 conference and, prior to that, a series of workshops on *Computer
Vision for Fashion, Art and Design* at ECCV 2018, ICCV 2019, and CVPR 2020. We
hope this reflection will bring artists and machine learning researchers into
conversation around the ethical and social dimensions of creative applications
of computer vision.","Negar Rostamzadeh, Emily Denton, Linda Petrini",2021,http://arxiv.org/abs/2112.03111v1
Nomic Embed Vision: Expanding the Latent Space,"This technical report describes the training of nomic-embed-vision, a highly
performant, open-code, open-weights image embedding model that shares the same
latent space as nomic-embed-text. Together, nomic-embed-vision and
nomic-embed-text form the first unified latent space to achieve high
performance across vision, language, and multimodal tasks.","Zach Nussbaum, Brandon Duderstadt, Andriy Mulyar",2024,http://arxiv.org/abs/2406.18587v1
"Computer Vision and Abnormal Patient Gait Assessment a Comparison of
  Machine Learning Models","Abnormal gait, its associated falls and complications have high patient
morbidity, mortality. Computer vision detects, predicts patient gait
abnormalities, assesses fall risk and serves as clinical decision support tool
for physicians. This paper performs a systematic review of how computer vision,
machine learning models perform an abnormal patient's gait assessment. Computer
vision is beneficial in gait analysis, it helps capture the patient posture.
Several literature suggests the use of different machine learning algorithms
such as SVM, ANN, K-Star, Random Forest, KNN, among others to perform the
classification on the features extracted to study patient gait abnormalities.","Jasmin Hundall, Benson A. Babu",2020,http://arxiv.org/abs/2004.02810v1
Tuning computer vision models with task rewards,"Misalignment between model predictions and intended usage can be detrimental
for the deployment of computer vision models. The issue is exacerbated when the
task involves complex structured outputs, as it becomes harder to design
procedures which address this misalignment. In natural language processing,
this is often addressed using reinforcement learning techniques that align
models with a task reward. We adopt this approach and show its surprising
effectiveness across multiple computer vision tasks, such as object detection,
panoptic segmentation, colorization and image captioning. We believe this
approach has the potential to be widely useful for better aligning models with
a diverse range of computer vision tasks.","André Susano Pinto, Alexander Kolesnikov, Yuge Shi, Lucas Beyer, Xiaohua Zhai",2023,http://arxiv.org/abs/2302.08242v1
"The possibility of making \$138,000 from shredded banknote pieces using
  computer vision","Every country must dispose of old banknotes. At the Hong Kong Monetary
Authority visitor center, visitors can buy a paperweight souvenir full of
shredded banknotes. Even though the shredded banknotes are small, by using
computer vision, it is possible to reconstruct the whole banknote like a jigsaw
puzzle. Each paperweight souvenir costs \$100 HKD, and it is claimed to contain
shredded banknotes equivalent to 138 complete \$1000 HKD banknotes. In theory,
\$138,000 HKD can be recovered by using computer vision. This paper discusses
the technique of collecting shredded banknote pieces and applying a computer
vision program.",Chung To Kong,2023,http://arxiv.org/abs/2401.06133v1
Computer Stereo Vision for Autonomous Driving,"As an important component of autonomous systems, autonomous car perception
has had a big leap with recent advances in parallel computing architectures.
With the use of tiny but full-feature embedded supercomputers, computer stereo
vision has been prevalently applied in autonomous cars for depth perception.
The two key aspects of computer stereo vision are speed and accuracy. They are
both desirable but conflicting properties, as the algorithms with better
disparity accuracy usually have higher computational complexity. Therefore, the
main aim of developing a computer stereo vision algorithm for resource-limited
hardware is to improve the trade-off between speed and accuracy. In this
chapter, we introduce both the hardware and software aspects of computer stereo
vision for autonomous car systems. Then, we discuss four autonomous car
perception tasks, including 1) visual feature detection, description and
matching, 2) 3D information acquisition, 3) object detection/recognition and 4)
semantic image segmentation. The principles of computer stereo vision and
parallel computing on multi-threading CPU and GPU architectures are then
detailed.","Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas",2020,http://arxiv.org/abs/2012.03194v2
LM4LV: A Frozen Large Language Model for Low-level Vision Tasks,"The success of large language models (LLMs) has fostered a new research trend
of multi-modality large language models (MLLMs), which changes the paradigm of
various fields in computer vision. Though MLLMs have shown promising results in
numerous high-level vision and vision-language tasks such as VQA and
text-to-image, no works have demonstrated how low-level vision tasks can
benefit from MLLMs. We find that most current MLLMs are blind to low-level
features due to their design of vision modules, thus are inherently incapable
for solving low-level vision tasks. In this work, we purpose $\textbf{LM4LV}$,
a framework that enables a FROZEN LLM to solve a range of low-level vision
tasks without any multi-modal data or prior. This showcases the LLM's strong
potential in low-level vision and bridges the gap between MLLMs and low-level
vision tasks. We hope this work can inspire new perspectives on LLMs and deeper
understanding of their mechanisms. Code is available at
https://github.com/bytetriper/LM4LV.","Boyang Zheng, Jinjin Gu, Shijun Li, Chao Dong",2024,http://arxiv.org/abs/2405.15734v2
"A survey of the Vision Transformers and their CNN-Transformer based
  Variants","Vision transformers have become popular as a possible substitute to
convolutional neural networks (CNNs) for a variety of computer vision
applications. These transformers, with their ability to focus on global
relationships in images, offer large learning capacity. However, they may
suffer from limited generalization as they do not tend to model local
correlation in images. Recently, in vision transformers hybridization of both
the convolution operation and self-attention mechanism has emerged, to exploit
both the local and global image representations. These hybrid vision
transformers, also referred to as CNN-Transformer architectures, have
demonstrated remarkable results in vision applications. Given the rapidly
growing number of hybrid vision transformers, it has become necessary to
provide a taxonomy and explanation of these hybrid architectures. This survey
presents a taxonomy of the recent vision transformer architectures and more
specifically that of the hybrid vision transformers. Additionally, the key
features of these architectures such as the attention mechanisms, positional
embeddings, multi-scale processing, and convolution are also discussed. In
contrast to the previous survey papers that are primarily focused on individual
vision transformer architectures or CNNs, this survey uniquely emphasizes the
emerging trend of hybrid vision transformers. By showcasing the potential of
hybrid vision transformers to deliver exceptional performance across a range of
computer vision tasks, this survey sheds light on the future directions of this
rapidly evolving architecture.","Asifullah Khan, Zunaira Rauf, Anabia Sohail, Abdul Rehman, Hifsa Asif, Aqsa Asif, Umair Farooq",2023,http://arxiv.org/abs/2305.09880v4
A Survey on Deep Learning Methods for Robot Vision,"Deep learning has allowed a paradigm shift in pattern recognition, from using
hand-crafted features together with statistical classifiers to using
general-purpose learning procedures for learning data-driven representations,
features, and classifiers together. The application of this new paradigm has
been particularly successful in computer vision, in which the development of
deep learning methods for vision applications has become a hot research topic.
Given that deep learning has already attracted the attention of the robot
vision community, the main purpose of this survey is to address the use of deep
learning in robot vision. To achieve this, a comprehensive overview of deep
learning and its usage in computer vision is given, that includes a description
of the most frequently used neural models and their main application areas.
Then, the standard methodology and tools used for designing deep-learning based
vision systems are presented. Afterwards, a review of the principal work using
deep learning in robot vision is presented, as well as current and future
trends related to the use of deep learning in robotics. This survey is intended
to be a guide for the developers of robot vision systems.","Javier Ruiz-del-Solar, Patricio Loncomilla, Naiomi Soto",2018,http://arxiv.org/abs/1803.10862v1
Snapshot of Algebraic Vision,"In this survey article, we present interactions between algebraic geometry
and computer vision, which have recently come under the header of algebraic
vision. The subject has given new insights in multiple view geometry and its
application to 3D scene reconstruction and carried a host of novel problems and
ideas back into algebraic geometry.","Joe Kileel, Kathlén Kohn",2022,http://arxiv.org/abs/2210.11443v2
CloudCV: Large Scale Distributed Computer Vision as a Cloud Service,"We are witnessing a proliferation of massive visual data. Unfortunately
scaling existing computer vision algorithms to large datasets leaves
researchers repeatedly solving the same algorithmic, logistical, and
infrastructural problems. Our goal is to democratize computer vision; one
should not have to be a computer vision, big data and distributed computing
expert to have access to state-of-the-art distributed computer vision
algorithms. We present CloudCV, a comprehensive system to provide access to
state-of-the-art distributed computer vision algorithms as a cloud service
through a Web Interface and APIs.","Harsh Agrawal, Clint Solomon Mathialagan, Yash Goyal, Neelima Chavali, Prakriti Banik, Akrit Mohapatra, Ahmed Osman, Dhruv Batra",2015,http://arxiv.org/abs/1506.04130v3
Teaching Computer Vision for Ecology,"Computer vision can accelerate ecology research by automating the analysis of
raw imagery from sensors like camera traps, drones, and satellites. However,
computer vision is an emerging discipline that is rarely taught to ecologists.
This work discusses our experience teaching a diverse group of ecologists to
prototype and evaluate computer vision systems in the context of an intensive
hands-on summer workshop. We explain the workshop structure, discuss common
challenges, and propose best practices. This document is intended for computer
scientists who teach computer vision across disciplines, but it may also be
useful to ecologists or other domain experts who are learning to use computer
vision themselves.","Elijah Cole, Suzanne Stathatos, Björn Lütjens, Tarun Sharma, Justin Kay, Jason Parham, Benjamin Kellenberger, Sara Beery",2023,http://arxiv.org/abs/2301.02211v1
Negative Results in Computer Vision: A Perspective,"A negative result is when the outcome of an experiment or a model is not what
is expected or when a hypothesis does not hold. Despite being often overlooked
in the scientific community, negative results are results and they carry value.
While this topic has been extensively discussed in other fields such as social
sciences and biosciences, less attention has been paid to it in the computer
vision community. The unique characteristics of computer vision, particularly
its experimental aspect, call for a special treatment of this matter. In this
paper, I will address what makes negative results important, how they should be
disseminated and incentivized, and what lessons can be learned from cognitive
vision research in this regard. Further, I will discuss issues such as computer
vision and human vision interaction, experimental design and statistical
hypothesis testing, explanatory versus predictive modeling, performance
evaluation, model comparison, as well as computer vision research culture.",Ali Borji,2017,http://arxiv.org/abs/1705.04402v3
Quantifying Visual Image Quality: A Bayesian View,"Image quality assessment (IQA) models aim to establish a quantitative
relationship between visual images and their perceptual quality by human
observers. IQA modeling plays a special bridging role between vision science
and engineering practice, both as a test-bed for vision theories and
computational biovision models, and as a powerful tool that could potentially
make profound impact on a broad range of image processing, computer vision, and
computer graphics applications, for design, optimization, and evaluation
purposes. IQA research has enjoyed an accelerated growth in the past two
decades. Here we present an overview of IQA methods from a Bayesian
perspective, with the goals of unifying a wide spectrum of IQA approaches under
a common framework and providing useful references to fundamental concepts
accessible to vision scientists and image processing practitioners. We discuss
the implications of the successes and limitations of modern IQA methods for
biological vision and the prospect for vision science to inform the design of
future artificial vision systems.","Zhengfang Duanmu, Wentao Liu, Zhongling Wang, Zhou Wang",2021,http://arxiv.org/abs/2102.00195v2
Vision Language Transformers: A Survey,"Vision language tasks, such as answering questions about or generating
captions that describe an image, are difficult tasks for computers to perform.
A relatively recent body of research has adapted the pretrained transformer
architecture introduced in \citet{vaswani2017attention} to vision language
modeling. Transformer models have greatly improved performance and versatility
over previous vision language models. They do so by pretraining models on a
large generic datasets and transferring their learning to new tasks with minor
changes in architecture and parameter values. This type of transfer learning
has become the standard modeling practice in both natural language processing
and computer vision. Vision language transformers offer the promise of
producing similar advancements in tasks which require both vision and language.
In this paper, we provide a broad synthesis of the currently available research
on vision language transformer models and offer some analysis of their
strengths, limitations and some open questions that remain.","Clayton Fields, Casey Kennington",2023,http://arxiv.org/abs/2307.03254v1
DAMamba: Vision State Space Model with Dynamic Adaptive Scan,"State space models (SSMs) have recently garnered significant attention in
computer vision. However, due to the unique characteristics of image data,
adapting SSMs from natural language processing to computer vision has not
outperformed the state-of-the-art convolutional neural networks (CNNs) and
Vision Transformers (ViTs). Existing vision SSMs primarily leverage manually
designed scans to flatten image patches into sequences locally or globally.
This approach disrupts the original semantic spatial adjacency of the image and
lacks flexibility, making it difficult to capture complex image structures. To
address this limitation, we propose Dynamic Adaptive Scan (DAS), a data-driven
method that adaptively allocates scanning orders and regions. This enables more
flexible modeling capabilities while maintaining linear computational
complexity and global modeling capacity. Based on DAS, we further propose the
vision backbone DAMamba, which significantly outperforms current
state-of-the-art vision Mamba models in vision tasks such as image
classification, object detection, instance segmentation, and semantic
segmentation. Notably, it surpasses some of the latest state-of-the-art CNNs
and ViTs. Code will be available at https://github.com/ltzovo/DAMamba.","Tanzhe Li, Caoshuo Li, Jiayi Lyu, Hongjuan Pei, Baochang Zhang, Taisong Jin, Rongrong Ji",2025,http://arxiv.org/abs/2502.12627v1
"LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data
  Lottery Tickets","Vision transformers have revolutionized computer vision, but their
computational demands present challenges for training and deployment. This
paper introduces LOTUS (LOttery Transformers with Ultra Sparsity), a novel
method that leverages data lottery ticket selection and sparsity pruning to
accelerate vision transformer training while maintaining accuracy. Our approach
focuses on identifying and utilizing the most informative data subsets and
eliminating redundant model parameters to optimize the training process.
Through extensive experiments, we demonstrate the effectiveness of LOTUS in
achieving rapid convergence and high accuracy with significantly reduced
computational requirements. This work highlights the potential of combining
data selection and sparsity techniques for efficient vision transformer
training, opening doors for further research and development in this area.",Ojasw Upadhyay,2024,http://arxiv.org/abs/2405.00906v1
Leveraging Vision Reconstruction Pipelines for Satellite Imagery,"Reconstructing 3D geometry from satellite imagery is an important topic of
research. However, disparities exist between how this 3D reconstruction problem
is handled in the remote sensing context and how multi-view reconstruction
pipelines have been developed in the computer vision community. In this paper,
we explore whether state-of-the-art reconstruction pipelines from the vision
community can be applied to the satellite imagery. Along the way, we address
several challenges adapting vision-based structure from motion and multi-view
stereo methods. We show that vision pipelines can offer competitive speed and
accuracy in the satellite context.","Kai Zhang, Jin Sun, Noah Snavely",2019,http://arxiv.org/abs/1910.02989v2
"Agriculture-Vision Challenge 2022 -- The Runner-Up Solution for
  Agricultural Pattern Recognition via Transformer-based Models","The Agriculture-Vision Challenge in CVPR is one of the most famous and
competitive challenges for global researchers to break the boundary between
computer vision and agriculture sectors, aiming at agricultural pattern
recognition from aerial images. In this paper, we propose our solution to the
third Agriculture-Vision Challenge in CVPR 2022. We leverage a data
pre-processing scheme and several Transformer-based models as well as data
augmentation techniques to achieve a mIoU of 0.582, accomplishing the 2nd place
in this challenge.","Zhicheng Yang, Jui-Hsin Lai, Jun Zhou, Hang Zhou, Chen Du, Zhongcheng Lai",2022,http://arxiv.org/abs/2206.11920v1
Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models,"Vision-language alignment in Large Vision-Language Models (LVLMs)
successfully enables LLMs to understand visual input. However, we find that
existing vision-language alignment methods fail to transfer the existing safety
mechanism for text in LLMs to vision, which leads to vulnerabilities in toxic
image. To explore the cause of this problem, we give the insightful explanation
of where and how the safety mechanism of LVLMs operates and conduct comparative
analysis between text and vision. We find that the hidden states at the
specific transformer layers play a crucial role in the successful activation of
safety mechanism, while the vision-language alignment at hidden states level in
current methods is insufficient. This results in a semantic shift for input
images compared to text in hidden states, therefore misleads the safety
mechanism. To address this, we propose a novel Text-Guided vision-language
Alignment method (TGA) for LVLMs. TGA retrieves the texts related to input
vision and uses them to guide the projection of vision into the hidden states
space in LLMs. Experiments show that TGA not only successfully transfers the
safety mechanism for text in basic LLMs to vision in vision-language alignment
for LVLMs without any safety fine-tuning on the visual modality but also
maintains the general performance on various vision tasks (Safe and Good).","Shicheng Xu, Liang Pang, Yunchang Zhu, Huawei Shen, Xueqi Cheng",2024,http://arxiv.org/abs/2410.12662v2
"VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths
  Vision Computation","A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is
that while increasing the number of vision tokens generally enhances visual
understanding, it also significantly raises memory and computational costs,
especially in long-term, dense video frame streaming scenarios. Although
learnable approaches like Q-Former and Perceiver Resampler have been developed
to reduce the vision token burden, they overlook the context causally modeled
by LLMs (i.e., key-value cache), potentially leading to missed visual cues when
addressing user queries. In this paper, we introduce a novel approach to reduce
vision compute by leveraging redundant vision tokens ""skipping layers"" rather
than decreasing the number of vision tokens. Our method, VideoLLM-MoD, is
inspired by mixture-of-depths LLMs and addresses the challenge of numerous
vision tokens in long-term or streaming video. Specifically, for each
transformer layer, we learn to skip the computation for a high proportion
(e.g., 80\%) of vision tokens, passing them directly to the next layer. This
approach significantly enhances model efficiency, achieving approximately
\textasciitilde42\% time and \textasciitilde30\% memory savings for the entire
training. Moreover, our method reduces the computation in the context and avoid
decreasing the vision tokens, thus preserving or even improving performance
compared to the vanilla model. We conduct extensive experiments to demonstrate
the effectiveness of VideoLLM-MoD, showing its state-of-the-art results on
multiple benchmarks, including narration, forecasting, and summarization tasks
in COIN, Ego4D, and Ego-Exo4D datasets.","Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, Mike Zheng Shou",2024,http://arxiv.org/abs/2408.16730v1
"Towards Point Cloud Compression for Machine Perception: A Simple and
  Strong Baseline by Learning the Octree Depth Level Predictor","Point cloud compression has garnered significant interest in computer vision.
However, existing algorithms primarily cater to human vision, while most point
cloud data is utilized for machine vision tasks. To address this, we propose a
point cloud compression framework that simultaneously handles both human and
machine vision tasks. Our framework learns a scalable bit-stream, using only
subsets for different machine vision tasks to save bit-rate, while employing
the entire bit-stream for human vision tasks. Building on mainstream
octree-based frameworks like VoxelContext-Net, OctAttention, and G-PCC, we
introduce a new octree depth-level predictor. This predictor adaptively
determines the optimal depth level for each octree constructed from a point
cloud, controlling the bit-rate for machine vision tasks. For simpler tasks
(\textit{e.g.}, classification) or objects/scenarios, we use fewer depth levels
with fewer bits, saving bit-rate. Conversely, for more complex tasks
(\textit{e.g}., segmentation) or objects/scenarios, we use deeper depth levels
with more bits to enhance performance. Experimental results on various datasets
(\textit{e.g}., ModelNet10, ModelNet40, ShapeNet, ScanNet, and KITTI) show that
our point cloud compression approach improves performance for machine vision
tasks without compromising human vision quality.","Lei Liu, Zhihao Hu, Zhenghao Chen",2024,http://arxiv.org/abs/2406.00791v1
"From Structured to Unstructured:A Comparative Analysis of Computer
  Vision and Graph Models in solving Mesh-based PDEs","This article investigates the application of computer vision and graph-based
models in solving mesh-based partial differential equations within
high-performance computing environments. Focusing on structured, graded
structured, and unstructured meshes, the study compares the performance and
computational efficiency of three computer vision-based models against three
graph-based models across three data\-sets. The research aims to identify the
most suitable models for different mesh topographies, particularly highlighting
the exploration of graded meshes, a less studied area. Results demonstrate that
computer vision-based models, notably U-Net, outperform the graph models in
prediction performance and efficiency in two (structured and graded) out of
three mesh topographies. The study also reveals the unexpected effectiveness of
computer vision-based models in handling unstructured meshes, suggesting a
potential shift in methodological approaches for data-driven partial
differential equation learning. The article underscores deep learning as a
viable and potentially sustainable way to enhance traditional high-performance
computing methods, advocating for informed model selection based on the
topography of the mesh.","Jens Decke, Olaf Wünsch, Bernhard Sick, Christian Gruhl",2024,http://arxiv.org/abs/2406.00081v1
"ToVE: Efficient Vision-Language Learning via Knowledge Transfer from
  Vision Experts","Vision-language (VL) learning requires extensive visual perception
capabilities, such as fine-grained object recognition and spatial perception.
Recent works typically rely on training huge models on massive datasets to
develop these capabilities. As a more efficient alternative, this paper
proposes a new framework that Transfers the knowledge from a hub of Vision
Experts (ToVE) for efficient VL learning, leveraging pre-trained vision expert
models to promote visual perception capability. Specifically, building on a
frozen CLIP encoder that provides vision tokens for image-conditioned language
generation, ToVE introduces a hub of multiple vision experts and a token-aware
gating network that dynamically routes expert knowledge to vision tokens. In
the transfer phase, we propose a ""residual knowledge transfer"" strategy, which
not only preserves the generalizability of the vision tokens but also allows
detachment of low-contributing experts to improve inference efficiency.
Further, we explore to merge these expert knowledge to a single CLIP encoder,
creating a knowledge-merged CLIP that produces more informative vision tokens
without expert inference during deployment. Experiment results across various
VL tasks demonstrate that the proposed ToVE achieves competitive performance
with two orders of magnitude fewer training data.","Yuanchen Wu, Junlong Du, Ke Yan, Shouhong Ding, Xiaoqiang Li",2025,http://arxiv.org/abs/2504.00691v1
WiCV 2019: The Sixth Women In Computer Vision Workshop,"In this paper we present the Women in Computer Vision Workshop - WiCV 2019,
organized in conjunction with CVPR 2019. This event is meant for increasing the
visibility and inclusion of women researchers in the computer vision field.
Computer vision and machine learning have made incredible progress over the
past years, but the number of female researchers is still low both in academia
and in industry. WiCV is organized especially for the following reason: to
raise visibility of female researchers, to increase collaborations between
them, and to provide mentorship to female junior researchers in the field. In
this paper, we present a report of trends over the past years, along with a
summary of statistics regarding presenters, attendees, and sponsorship for the
current workshop.","Irene Amerini, Elena Balashova, Sayna Ebrahimi, Kathryn Leonard, Arsha Nagrani, Amaia Salvador",2019,http://arxiv.org/abs/1909.10225v1
Does Image Anonymization Impact Computer Vision Training?,"Image anonymization is widely adapted in practice to comply with privacy
regulations in many regions. However, anonymization often degrades the quality
of the data, reducing its utility for computer vision development. In this
paper, we investigate the impact of image anonymization for training computer
vision models on key computer vision tasks (detection, instance segmentation,
and pose estimation). Specifically, we benchmark the recognition drop on common
detection datasets, where we evaluate both traditional and realistic
anonymization for faces and full bodies. Our comprehensive experiments reflect
that traditional image anonymization substantially impacts final model
performance, particularly when anonymizing the full body. Furthermore, we find
that realistic anonymization can mitigate this decrease in performance, where
our experiments reflect a minimal performance drop for face anonymization. Our
study demonstrates that realistic anonymization can enable privacy-preserving
computer vision development with minimal performance degradation across a range
of important computer vision benchmarks.","Håkon Hukkelås, Frank Lindseth",2023,http://arxiv.org/abs/2306.05135v1
Deep Learning--Based Scene Simplification for Bionic Vision,"Retinal degenerative diseases cause profound visual impairment in more than
10 million people worldwide, and retinal prostheses are being developed to
restore vision to these individuals. Analogous to cochlear implants, these
devices electrically stimulate surviving retinal cells to evoke visual percepts
(phosphenes). However, the quality of current prosthetic vision is still
rudimentary. Rather than aiming to restore ""natural"" vision, there is potential
merit in borrowing state-of-the-art computer vision algorithms as image
processing techniques to maximize the usefulness of prosthetic vision. Here we
combine deep learning--based scene simplification strategies with a
psychophysically validated computational model of the retina to generate
realistic predictions of simulated prosthetic vision, and measure their ability
to support scene understanding of sighted subjects (virtual patients) in a
variety of outdoor scenarios. We show that object segmentation may better
support scene understanding than models based on visual saliency and monocular
depth estimation. In addition, we highlight the importance of basing
theoretical predictions on biologically realistic models of phosphene shape.
Overall, this work has the potential to drastically improve the utility of
prosthetic vision for people blinded from retinal degenerative diseases.","Nicole Han, Sudhanshu Srivastava, Aiwen Xu, Devi Klein, Michael Beyeler",2021,http://arxiv.org/abs/2102.00297v1
DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition,"Recently, Vision Graph Neural Network (ViG) has gained considerable attention
in computer vision. Despite its groundbreaking innovation, Vision Graph Neural
Network encounters key issues including the quadratic computational complexity
caused by its K-Nearest Neighbor (KNN) graph construction and the limitation of
pairwise relations of normal graphs. To address the aforementioned challenges,
we propose a novel vision architecture, termed Dilated Vision HyperGraph Neural
Network (DVHGNN), which is designed to leverage multi-scale hypergraph to
efficiently capture high-order correlations among objects. Specifically, the
proposed method tailors Clustering and Dilated HyperGraph Construction (DHGC)
to adaptively capture multi-scale dependencies among the data samples.
Furthermore, a dynamic hypergraph convolution mechanism is proposed to
facilitate adaptive feature exchange and fusion at the hypergraph level.
Extensive qualitative and quantitative evaluations of the benchmark image
datasets demonstrate that the proposed DVHGNN significantly outperforms the
state-of-the-art vision backbones. For instance, our DVHGNN-S achieves an
impressive top-1 accuracy of 83.1% on ImageNet-1K, surpassing ViG-S by +1.0%
and ViHGNN-S by +0.6%.","Caoshuo Li, Tanzhe Li, Xiaobin Hu, Donghao Luo, Taisong Jin",2025,http://arxiv.org/abs/2503.14867v1
Universal Object Detection with Large Vision Model,"Over the past few years, there has been growing interest in developing a
broad, universal, and general-purpose computer vision system. Such systems have
the potential to address a wide range of vision tasks simultaneously, without
being limited to specific problems or data domains. This universality is
crucial for practical, real-world computer vision applications. In this study,
our focus is on a specific challenge: the large-scale, multi-domain universal
object detection problem, which contributes to the broader goal of achieving a
universal vision system. This problem presents several intricate challenges,
including cross-dataset category label duplication, label conflicts, and the
necessity to handle hierarchical taxonomies. To address these challenges, we
introduce our approach to label handling, hierarchy-aware loss design, and
resource-efficient model training utilizing a pre-trained large vision model.
Our method has demonstrated remarkable performance, securing a prestigious
second-place ranking in the object detection track of the Robust Vision
Challenge 2022 (RVC 2022) on a million-scale cross-dataset object detection
benchmark. We believe that our comprehensive study will serve as a valuable
reference and offer an alternative approach for addressing similar challenges
within the computer vision community. The source code for our work is openly
available at https://github.com/linfeng93/Large-UniDet.","Feng Lin, Wenze Hu, Yaowei Wang, Yonghong Tian, Guangming Lu, Fanglin Chen, Yong Xu, Xiaoyu Wang",2022,http://arxiv.org/abs/2212.09408v3
"A Comparative Study of Confidence Calibration in Deep Learning: From
  Computer Vision to Medical Imaging","Although deep learning prediction models have been successful in the
discrimination of different classes, they can often suffer from poor
calibration across challenging domains including healthcare. Moreover, the
long-tail distribution poses great challenges in deep learning classification
problems including clinical disease prediction. There are approaches proposed
recently to calibrate deep prediction in computer vision, but there are no
studies found to demonstrate how the representative models work in different
challenging contexts. In this paper, we bridge the confidence calibration from
computer vision to medical imaging with a comparative study of four high-impact
calibration models. Our studies are conducted in different contexts (natural
image classification and lung cancer risk estimation) including in balanced vs.
imbalanced training sets and in computer vision vs. medical imaging. Our
results support key findings: (1) We achieve new conclusions which are not
studied under different learning contexts, e.g., combining two calibration
models that both mitigate the overconfident prediction can lead to
under-confident prediction, and simpler calibration models from the computer
vision domain tend to be more generalizable to medical imaging. (2) We
highlight the gap between general computer vision tasks and medical imaging
prediction, e.g., calibration methods ideal for general computer vision tasks
may in fact damage the calibration of medical imaging prediction. (3) We also
reinforce previous conclusions in natural image classification settings. We
believe that this study has merits to guide readers to choose calibration
models and understand gaps between general computer vision and medical imaging
domains.","Riqiang Gao, Thomas Li, Yucheng Tang, Zhoubing Xu, Michael Kammer, Sanja L. Antic, Kim Sandler, Fabien Moldonado, Thomas A. Lasko, Bennett Landman",2022,http://arxiv.org/abs/2206.08833v1
"Situated Cameras, Situated Knowledges: Towards an Egocentric
  Epistemology for Computer Vision","In her influential 1988 paper, Situated Knowledges, Donna Haraway uses vision
and perspective as a metaphor to discuss scientific knowledge. Today,
egocentric computer vision discusses many of the same issues, except in a
literal vision context. In this short position paper, we collapse that
metaphor, and explore the interactions between feminist epistemology and
egocentric CV as ""Egocentric Epistemology."" Using this framework, we argue for
the use of qualitative, human-centric methods as a complement to performance
benchmarks, to center both the literal and metaphorical perspective of human
crowd workers in CV.","Samuel Goree, David Crandall",2023,http://arxiv.org/abs/2307.00064v1
Some Insights into Lifelong Reinforcement Learning Systems,"A lifelong reinforcement learning system is a learning system that has the
ability to learn through trail-and-error interaction with the environment over
its lifetime. In this paper, I give some arguments to show that the traditional
reinforcement learning paradigm fails to model this type of learning system.
Some insights into lifelong reinforcement learning are provided, along with a
simplistic prototype lifelong reinforcement learning system.",Changjian Li,2020,http://arxiv.org/abs/2001.09608v1
"Counterexample-Guided Repair of Reinforcement Learning Systems Using
  Safety Critics","Naively trained Deep Reinforcement Learning agents may fail to satisfy vital
safety constraints. To avoid costly retraining, we may desire to repair a
previously trained reinforcement learning agent to obviate unsafe behaviour. We
devise a counterexample-guided repair algorithm for repairing reinforcement
learning systems leveraging safety critics. The algorithm jointly repairs a
reinforcement learning agent and a safety critic using gradient-based
constrained optimisation.","David Boetius, Stefan Leue",2024,http://arxiv.org/abs/2405.15430v1
Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey,"Deep reinforcement learning augments the reinforcement learning framework and
utilizes the powerful representation of deep neural networks. Recent works have
demonstrated the remarkable successes of deep reinforcement learning in various
domains including finance, medicine, healthcare, video games, robotics, and
computer vision. In this work, we provide a detailed review of recent and
state-of-the-art research advances of deep reinforcement learning in computer
vision. We start with comprehending the theories of deep learning,
reinforcement learning, and deep reinforcement learning. We then propose a
categorization of deep reinforcement learning methodologies and discuss their
advantages and limitations. In particular, we divide deep reinforcement
learning into seven main categories according to their applications in computer
vision, i.e. (i)landmark localization (ii) object detection; (iii) object
tracking; (iv) registration on both 2D image and 3D image volumetric data (v)
image segmentation; (vi) videos analysis; and (vii) other applications. Each of
these categories is further analyzed with reinforcement learning techniques,
network design, and performance. Moreover, we provide a comprehensive analysis
of the existing publicly available datasets and examine source code
availability. Finally, we present some open issues and discuss future research
directions on deep reinforcement learning in computer vision","Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki, Khoa Luu, Marios Savvides",2021,http://arxiv.org/abs/2108.11510v1
Causal Reinforcement Learning: A Survey,"Reinforcement learning is an essential paradigm for solving sequential
decision problems under uncertainty. Despite many remarkable achievements in
recent decades, applying reinforcement learning methods in the real world
remains challenging. One of the main obstacles is that reinforcement learning
agents lack a fundamental understanding of the world and must therefore learn
from scratch through numerous trial-and-error interactions. They may also face
challenges in providing explanations for their decisions and generalizing the
acquired knowledge. Causality, however, offers a notable advantage as it can
formalize knowledge in a systematic manner and leverage invariance for
effective knowledge transfer. This has led to the emergence of causal
reinforcement learning, a subfield of reinforcement learning that seeks to
enhance existing algorithms by incorporating causal relationships into the
learning process. In this survey, we comprehensively review the literature on
causal reinforcement learning. We first introduce the basic concepts of
causality and reinforcement learning, and then explain how causality can
address core challenges in non-causal reinforcement learning. We categorize and
systematically review existing causal reinforcement learning approaches based
on their target problems and methodologies. Finally, we outline open issues and
future directions in this emerging field.","Zhihong Deng, Jing Jiang, Guodong Long, Chengqi Zhang",2023,http://arxiv.org/abs/2307.01452v2
"Distributed Deep Reinforcement Learning: A Survey and A Multi-Player
  Multi-Agent Learning Toolbox","With the breakthrough of AlphaGo, deep reinforcement learning becomes a
recognized technique for solving sequential decision-making problems. Despite
its reputation, data inefficiency caused by its trial and error learning
mechanism makes deep reinforcement learning hard to be practical in a wide
range of areas. Plenty of methods have been developed for sample efficient deep
reinforcement learning, such as environment modeling, experience transfer, and
distributed modifications, amongst which, distributed deep reinforcement
learning has shown its potential in various applications, such as
human-computer gaming, and intelligent transportation. In this paper, we
conclude the state of this exciting field, by comparing the classical
distributed deep reinforcement learning methods, and studying important
components to achieve efficient distributed learning, covering single player
single agent distributed deep reinforcement learning to the most complex
multiple players multiple agents distributed deep reinforcement learning.
Furthermore, we review recently released toolboxes that help to realize
distributed deep reinforcement learning without many modifications of their
non-distributed versions. By analyzing their strengths and weaknesses, a
multi-player multi-agent distributed deep reinforcement learning toolbox is
developed and released, which is further validated on Wargame, a complex
environment, showing usability of the proposed toolbox for multiple players and
multiple agents distributed deep reinforcement learning under complex games.
Finally, we try to point out challenges and future trends, hoping this brief
review can provide a guide or a spark for researchers who are interested in
distributed deep reinforcement learning.","Qiyue Yin, Tongtong Yu, Shengqi Shen, Jun Yang, Meijing Zhao, Kaiqi Huang, Bin Liang, Liang Wang",2022,http://arxiv.org/abs/2212.00253v1
Transfer Learning in Deep Reinforcement Learning: A Survey,"Reinforcement learning is a learning paradigm for solving sequential
decision-making problems. Recent years have witnessed remarkable progress in
reinforcement learning upon the fast development of deep neural networks. Along
with the promising prospects of reinforcement learning in numerous domains such
as robotics and game-playing, transfer learning has arisen to tackle various
challenges faced by reinforcement learning, by transferring knowledge from
external expertise to facilitate the efficiency and effectiveness of the
learning process. In this survey, we systematically investigate the recent
progress of transfer learning approaches in the context of deep reinforcement
learning. Specifically, we provide a framework for categorizing the
state-of-the-art transfer learning approaches, under which we analyze their
goals, methodologies, compatible reinforcement learning backbones, and
practical applications. We also draw connections between transfer learning and
other relevant topics from the reinforcement learning perspective and explore
their potential challenges that await future research progress.","Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain, Jiayu Zhou",2020,http://arxiv.org/abs/2009.07888v7
"Memory-two strategies forming symmetric mutual reinforcement learning
  equilibrium in repeated prisoners' dilemma game","We investigate symmetric equilibria of mutual reinforcement learning when
both players alternately learn the optimal memory-two strategies against the
opponent in the repeated prisoners' dilemma game. We provide a necessary
condition for memory-two deterministic strategies to form symmetric equilibria.
We then provide three examples of memory-two deterministic strategies which
form symmetric mutual reinforcement learning equilibria. We also prove that
mutual reinforcement learning equilibria formed by memory-two strategies are
also mutual reinforcement learning equilibria when both players use
reinforcement learning of memory-$n$ strategies with $n>2$.",Masahiko Ueda,2021,http://arxiv.org/abs/2108.03258v2
Implementing Online Reinforcement Learning with Temporal Neural Networks,"A Temporal Neural Network (TNN) architecture for implementing efficient
online reinforcement learning is proposed and studied via simulation. The
proposed T-learning system is composed of a frontend TNN that implements online
unsupervised clustering and a backend TNN that implements online reinforcement
learning. The reinforcement learning paradigm employs biologically plausible
neo-Hebbian three-factor learning rules. As a working example, a prototype
implementation of the cart-pole problem (balancing an inverted pendulum) is
studied via simulation.",James E. Smith,2022,http://arxiv.org/abs/2204.05437v1
Deep Reinforcement Learning for Conversational AI,"Deep reinforcement learning is revolutionizing the artificial intelligence
field. Currently, it serves as a good starting point for constructing
intelligent autonomous systems which offer a better knowledge of the visual
world. It is possible to scale deep reinforcement learning with the use of deep
learning and do amazing tasks such as use of pixels in playing video games. In
this paper, key concepts of deep reinforcement learning including reward
function, differences between reinforcement learning and supervised learning
and models for implementation of reinforcement are discussed. Key challenges
related to the implementation of reinforcement learning in conversational AI
domain are identified as well as discussed in detail. Various conversational
models which are based on deep reinforcement learning (as well as deep
learning) are also discussed. In summary, this paper discusses key aspects of
deep reinforcement learning which are crucial for designing an efficient
conversational AI.","Mahipal Jadeja, Neelanshi Varia, Agam Shah",2017,http://arxiv.org/abs/1709.05067v1
"On the Opportunities and Challenges of Offline Reinforcement Learning
  for Recommender Systems","Reinforcement learning serves as a potent tool for modeling dynamic user
interests within recommender systems, garnering increasing research attention
of late. However, a significant drawback persists: its poor data efficiency,
stemming from its interactive nature. The training of reinforcement
learning-based recommender systems demands expensive online interactions to
amass adequate trajectories, essential for agents to learn user preferences.
This inefficiency renders reinforcement learning-based recommender systems a
formidable undertaking, necessitating the exploration of potential solutions.
Recent strides in offline reinforcement learning present a new perspective.
Offline reinforcement learning empowers agents to glean insights from offline
datasets and deploy learned policies in online settings. Given that recommender
systems possess extensive offline datasets, the framework of offline
reinforcement learning aligns seamlessly. Despite being a burgeoning field,
works centered on recommender systems utilizing offline reinforcement learning
remain limited. This survey aims to introduce and delve into offline
reinforcement learning within recommender systems, offering an inclusive review
of existing literature in this domain. Furthermore, we strive to underscore
prevalent challenges, opportunities, and future pathways, poised to propel
research in this evolving field.","Xiaocong Chen, Siyu Wang, Julian McAuley, Dietmar Jannach, Lina Yao",2023,http://arxiv.org/abs/2308.11336v1
A Survey Analyzing Generalization in Deep Reinforcement Learning,"Reinforcement learning research obtained significant success and attention
with the utilization of deep neural networks to solve problems in high
dimensional state or action spaces. While deep reinforcement learning policies
are currently being deployed in many different fields from medical applications
to large language models, there are still ongoing questions the field is trying
to answer on the generalization capabilities of deep reinforcement learning
policies. In this paper, we will formalize and analyze generalization in deep
reinforcement learning. We will explain the fundamental reasons why deep
reinforcement learning policies encounter overfitting problems that limit their
generalization capabilities. Furthermore, we will categorize and explain the
manifold solution approaches to increase generalization, and overcome
overfitting in deep reinforcement learning policies. From exploration to
adversarial analysis and from regularization to robustness our paper provides
an analysis on a wide range of subfields within deep reinforcement learning
with a broad scope and in-depth view. We believe our study can provide a
compact guideline for the current advancements in deep reinforcement learning,
and help to construct robust deep neural policies with higher generalization
skills.",Ezgi Korkmaz,2024,http://arxiv.org/abs/2401.02349v2
Reinforcement Teaching,"Machine learning algorithms learn to solve a task, but are unable to improve
their ability to learn. Meta-learning methods learn about machine learning
algorithms and improve them so that they learn more quickly. However, existing
meta-learning methods are either hand-crafted to improve one specific component
of an algorithm or only work with differentiable algorithms. We develop a
unifying meta-learning framework, called Reinforcement Teaching, to improve the
learning process of \emph{any} algorithm. Under Reinforcement Teaching, a
teaching policy is learned, through reinforcement, to improve a student's
learning algorithm. To learn an effective teaching policy, we introduce the
parametric-behavior embedder that learns a representation of the student's
learnable parameters from its input/output behavior. We further use learning
progress to shape the teacher's reward, allowing it to more quickly maximize
the student's performance. To demonstrate the generality of Reinforcement
Teaching, we conduct experiments in which a teacher learns to significantly
improve both reinforcement and supervised learning algorithms. Reinforcement
Teaching outperforms previous work using heuristic reward functions and state
representations, as well as other parameter representations.","Calarina Muslimani, Alex Lewandowski, Dale Schuurmans, Matthew E. Taylor, Jun Luo",2022,http://arxiv.org/abs/2204.11897v3
Generative Adversarial Imitation Learning,"Consider learning a policy from example expert behavior, without interaction
with the expert or access to reinforcement signal. One approach is to recover
the expert's cost function with inverse reinforcement learning, then extract a
policy from that cost function with reinforcement learning. This approach is
indirect and can be slow. We propose a new general framework for directly
extracting a policy from data, as if it were obtained by reinforcement learning
following inverse reinforcement learning. We show that a certain instantiation
of our framework draws an analogy between imitation learning and generative
adversarial networks, from which we derive a model-free imitation learning
algorithm that obtains significant performance gains over existing model-free
methods in imitating complex behaviors in large, high-dimensional environments.","Jonathan Ho, Stefano Ermon",2016,http://arxiv.org/abs/1606.03476v1
Two-Memory Reinforcement Learning,"While deep reinforcement learning has shown important empirical success, it
tends to learn relatively slow due to slow propagation of rewards information
and slow update of parametric neural networks. Non-parametric episodic memory,
on the other hand, provides a faster learning alternative that does not require
representation learning and uses maximum episodic return as state-action values
for action selection. Episodic memory and reinforcement learning both have
their own strengths and weaknesses. Notably, humans can leverage multiple
memory systems concurrently during learning and benefit from all of them. In
this work, we propose a method called Two-Memory reinforcement learning agent
(2M) that combines episodic memory and reinforcement learning to distill both
of their strengths. The 2M agent exploits the speed of the episodic memory part
and the optimality and the generalization capacity of the reinforcement
learning part to complement each other. Our experiments demonstrate that the 2M
agent is more data efficient and outperforms both pure episodic memory and pure
reinforcement learning, as well as a state-of-the-art memory-augmented RL
agent. Moreover, the proposed approach provides a general framework that can be
used to combine any episodic memory agent with other off-policy reinforcement
learning algorithms.","Zhao Yang, Thomas. M. Moerland, Mike Preuss, Aske Plaat",2023,http://arxiv.org/abs/2304.10098v2
Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning,"Reinforcement learning, evolutionary algorithms and imitation learning are
three principal methods to deal with continuous control tasks. Reinforcement
learning is sample efficient, yet sensitive to hyper-parameters setting and
needs efficient exploration; Evolutionary algorithms are stable, but with low
sample efficiency; Imitation learning is both sample efficient and stable,
however it requires the guidance of expert data. In this paper, we propose
Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning,
a scalable framework that combines advantages of the three methods mentioned
above. The core of this framework is a dual-actors and single critic
reinforcement learning agent. This agent can recruit high-fitness actors from
the population of evolutionary algorithms, which instructs itself to learn from
experience replay buffer. At the same time, low-fitness actors in the
evolutionary population can imitate behavior patterns of the reinforcement
learning agent and improve their adaptability. Reinforcement and imitation
learners in this framework can be replaced with any off-policy actor-critic
reinforcement learner or data-driven imitation learner. We evaluate RIM on a
series of benchmarks for continuous control tasks in Mujoco. The experimental
results show that RIM outperforms prior evolutionary or reinforcement learning
methods. The performance of RIM's components is significantly better than
components of previous evolutionary reinforcement learning algorithm, and the
recruitment using soft update enables reinforcement learning agent to learn
faster than that using hard update.","Shuai Lü, Shuai Han, Wenbo Zhou, Junwei Zhang",2019,http://arxiv.org/abs/1912.06310v1
"Accelerate Reinforcement Learning with PID Controllers in the Pendulum
  Simulations","We propose a Proportional Integral Derivative (PID) controller-based coaching
scheme to expedite reinforcement learning (RL).",Liping Bai,2022,http://arxiv.org/abs/2210.00770v1
"Dex: Incremental Learning for Complex Environments in Deep Reinforcement
  Learning","This paper introduces Dex, a reinforcement learning environment toolkit
specialized for training and evaluation of continual learning methods as well
as general reinforcement learning problems. We also present the novel continual
learning method of incremental learning, where a challenging environment is
solved using optimal weight initialization learned from first solving a similar
easier environment. We show that incremental learning can produce vastly
superior results than standard methods by providing a strong baseline method
across ten Dex environments. We finally develop a saliency method for
qualitative analysis of reinforcement learning, which shows the impact
incremental learning has on network attention.","Nick Erickson, Qi Zhao",2017,http://arxiv.org/abs/1706.05749v1
Augmented Q Imitation Learning (AQIL),"The study of unsupervised learning can be generally divided into two
categories: imitation learning and reinforcement learning. In imitation
learning the machine learns by mimicking the behavior of an expert system
whereas in reinforcement learning the machine learns via direct environment
feedback. Traditional deep reinforcement learning takes a significant time
before the machine starts to converge to an optimal policy. This paper proposes
Augmented Q-Imitation-Learning, a method by which deep reinforcement learning
convergence can be accelerated by applying Q-imitation-learning as the initial
training process in traditional Deep Q-learning.","Xiao Lei Zhang, Anish Agarwal",2020,http://arxiv.org/abs/2004.00993v2
Interpretable Reinforcement Learning with Ensemble Methods,"We propose to use boosted regression trees as a way to compute
human-interpretable solutions to reinforcement learning problems. Boosting
combines several regression trees to improve their accuracy without
significantly reducing their inherent interpretability. Prior work has focused
independently on reinforcement learning and on interpretable machine learning,
but there has been little progress in interpretable reinforcement learning. Our
experimental results show that boosted regression trees compute solutions that
are both interpretable and match the quality of leading reinforcement learning
methods.","Alexander Brown, Marek Petrik",2018,http://arxiv.org/abs/1809.06995v1
Unsupervised Meta-Learning for Reinforcement Learning,"Meta-learning algorithms use past experience to learn to quickly solve new
tasks. In the context of reinforcement learning, meta-learning algorithms
acquire reinforcement learning procedures to solve new problems more
efficiently by utilizing experience from prior tasks. The performance of
meta-learning algorithms depends on the tasks available for meta-training: in
the same way that supervised learning generalizes best to test points drawn
from the same distribution as the training points, meta-learning methods
generalize best to tasks from the same distribution as the meta-training tasks.
In effect, meta-reinforcement learning offloads the design burden from
algorithm design to task design. If we can automate the process of task design
as well, we can devise a meta-learning algorithm that is truly automated. In
this work, we take a step in this direction, proposing a family of unsupervised
meta-learning algorithms for reinforcement learning. We motivate and describe a
general recipe for unsupervised meta-reinforcement learning, and present an
instantiation of this approach. Our conceptual and theoretical contributions
consist of formulating the unsupervised meta-reinforcement learning problem and
describing how task proposals based on mutual information can be used to train
optimal meta-learners. Our experimental results indicate that unsupervised
meta-reinforcement learning effectively acquires accelerated reinforcement
learning procedures without the need for manual task design and these
procedures exceed the performance of learning from scratch.","Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, Sergey Levine",2018,http://arxiv.org/abs/1806.04640v3
Lineage Evolution Reinforcement Learning,"We propose a general agent population learning system, and on this basis, we
propose lineage evolution reinforcement learning algorithm. Lineage evolution
reinforcement learning is a kind of derivative algorithm which accords with the
general agent population learning system. We take the agents in DQN and its
related variants as the basic agents in the population, and add the selection,
mutation and crossover modules in the genetic algorithm to the reinforcement
learning algorithm. In the process of agent evolution, we refer to the
characteristics of natural genetic behavior, add lineage factor to ensure the
retention of potential performance of agent, and comprehensively consider the
current performance and lineage value when evaluating the performance of agent.
Without changing the parameters of the original reinforcement learning
algorithm, lineage evolution reinforcement learning can optimize different
reinforcement learning algorithms. Our experiments show that the idea of
evolution with lineage improves the performance of original reinforcement
learning algorithm in some games in Atari 2600.","Zeyu Zhang, Guisheng Yin",2020,http://arxiv.org/abs/2010.14616v1
"Robust Reinforcement Learning with Distributional Risk-averse
  formulation","Robust Reinforcement Learning tries to make predictions more robust to
changes in the dynamics or rewards of the system. This problem is particularly
important when the dynamics and rewards of the environment are estimated from
the data. In this paper, we approximate the Robust Reinforcement Learning
constrained with a $\Phi$-divergence using an approximate Risk-Averse
formulation. We show that the classical Reinforcement Learning formulation can
be robustified using standard deviation penalization of the objective. Two
algorithms based on Distributional Reinforcement Learning, one for discrete and
one for continuous action spaces are proposed and tested in a classical Gym
environment to demonstrate the robustness of the algorithms.","Pierre Clavier, Stéphanie Allassonière, Erwan Le Pennec",2022,http://arxiv.org/abs/2206.06841v1
A survey of benchmarking frameworks for reinforcement learning,"Reinforcement learning has recently experienced increased prominence in the
machine learning community. There are many approaches to solving reinforcement
learning problems with new techniques developed constantly. When solving
problems using reinforcement learning, there are various difficult challenges
to overcome. To ensure progress in the field, benchmarks are important for
testing new algorithms and comparing with other approaches. The reproducibility
of results for fair comparison is therefore vital in ensuring that improvements
are accurately judged. This paper provides an overview of different
contributions to reinforcement learning benchmarking and discusses how they can
assist researchers to address the challenges facing reinforcement learning. The
contributions discussed are the most used and recent in the literature. The
paper discusses the contributions in terms of implementation, tasks and
provided algorithm implementations with benchmarks. The survey aims to bring
attention to the wide range of reinforcement learning benchmarking tasks
available and to encourage research to take place in a standardised manner.
Additionally, this survey acts as an overview for researchers not familiar with
the different tasks that can be used to develop and test new reinforcement
learning algorithms.","Belinda Stapelberg, Katherine M. Malan",2020,http://arxiv.org/abs/2011.13577v1
"Distilling Neuron Spike with High Temperature in Reinforcement Learning
  Agents","Spiking neural network (SNN), compared with depth neural network (DNN), has
faster processing speed, lower energy consumption and more biological
interpretability, which is expected to approach Strong AI. Reinforcement
learning is similar to learning in biology. It is of great significance to
study the combination of SNN and RL. We propose the reinforcement learning
method of spike distillation network (SDN) with STBP. This method uses
distillation to effectively avoid the weakness of STBP, which can achieve SOTA
performance in classification, and can obtain a smaller, faster convergence and
lower power consumption SNN reinforcement learning model. Experiments show that
our method can converge faster than traditional SNN reinforcement learning and
DNN reinforcement learning methods, about 1000 epochs faster, and obtain SNN
200 times smaller than DNN. We also deploy SDN to the PKU nc64c chip, which
proves that SDN has lower power consumption than DNN, and the power consumption
of SDN is more than 600 times lower than DNN on large-scale devices. SDN
provides a new way of SNN reinforcement learning, and can achieve SOTA
performance, which proves the possibility of further development of SNN
reinforcement learning.","Ling Zhang, Jian Cao, Yuan Zhang, Bohan Zhou, Shuo Feng",2021,http://arxiv.org/abs/2108.10078v1
A Survey on Offline Model-Based Reinforcement Learning,"Model-based approaches are becoming increasingly popular in the field of
offline reinforcement learning, with high potential in real-world applications
due to the model's capability of thoroughly utilizing the large historical
datasets available with supervised learning techniques. This paper presents a
literature review of recent work in offline model-based reinforcement learning,
a field that utilizes model-based approaches in offline reinforcement learning.
The survey provides a brief overview of the concepts and recent developments in
both offline reinforcement learning and model-based reinforcement learning, and
discuss the intersection of the two fields. We then presents key relevant
papers in the field of offline model-based reinforcement learning and discuss
their methods, particularly their approaches in solving the issue of
distributional shift, the main problem faced by all current offline model-based
reinforcement learning methods. We further discuss key challenges faced by the
field, and suggest possible directions for future work.",Haoyang He,2023,http://arxiv.org/abs/2305.03360v1
"Integration of Imitation Learning using GAIL and Reinforcement Learning
  using Task-achievement Rewards via Probabilistic Graphical Model","Integration of reinforcement learning and imitation learning is an important
problem that has been studied for a long time in the field of intelligent
robotics. Reinforcement learning optimizes policies to maximize the cumulative
reward, whereas imitation learning attempts to extract general knowledge about
the trajectories demonstrated by experts, i.e., demonstrators. Because each of
them has their own drawbacks, methods combining them and compensating for each
set of drawbacks have been explored thus far. However, many of the methods are
heuristic and do not have a solid theoretical basis. In this paper, we present
a new theory for integrating reinforcement and imitation learning by extending
the probabilistic generative model framework for reinforcement learning, {\it
plan by inference}. We develop a new probabilistic graphical model for
reinforcement learning with multiple types of rewards and a probabilistic
graphical model for Markov decision processes with multiple optimality
emissions (pMDP-MO). Furthermore, we demonstrate that the integrated learning
method of reinforcement learning and imitation learning can be formulated as a
probabilistic inference of policies on pMDP-MO by considering the output of the
discriminator in generative adversarial imitation learning as an additional
optimal emission observation. We adapt the generative adversarial imitation
learning and task-achievement reward to our proposed framework, achieving
significantly better performance than agents trained with reinforcement
learning or imitation learning alone. Experiments demonstrate that our
framework successfully integrates imitation and reinforcement learning even
when the number of demonstrators is only a few.","Akira Kinose, Tadahiro Taniguchi",2019,http://arxiv.org/abs/1907.02140v2
A Brief Survey of Deep Reinforcement Learning,"Deep reinforcement learning is poised to revolutionise the field of AI and
represents a step towards building autonomous systems with a higher level
understanding of the visual world. Currently, deep learning is enabling
reinforcement learning to scale to problems that were previously intractable,
such as learning to play video games directly from pixels. Deep reinforcement
learning algorithms are also applied to robotics, allowing control policies for
robots to be learned directly from camera inputs in the real world. In this
survey, we begin with an introduction to the general field of reinforcement
learning, then progress to the main streams of value-based and policy-based
methods. Our survey will cover central algorithms in deep reinforcement
learning, including the deep $Q$-network, trust region policy optimisation, and
asynchronous advantage actor-critic. In parallel, we highlight the unique
advantages of deep neural networks, focusing on visual understanding via
reinforcement learning. To conclude, we describe several current areas of
research within the field.","Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath",2017,http://arxiv.org/abs/1708.05866v2
Generalization and Regularization in DQN,"Deep reinforcement learning algorithms have shown an impressive ability to
learn complex control policies in high-dimensional tasks. However, despite the
ever-increasing performance on popular benchmarks, policies learned by deep
reinforcement learning algorithms can struggle to generalize when evaluated in
remarkably similar environments. In this paper we propose a protocol to
evaluate generalization in reinforcement learning through different modes of
Atari 2600 games. With that protocol we assess the generalization capabilities
of DQN, one of the most traditional deep reinforcement learning algorithms, and
we provide evidence suggesting that DQN overspecializes to the training
environment. We then comprehensively evaluate the impact of dropout and
$\ell_2$ regularization, as well as the impact of reusing learned
representations to improve the generalization capabilities of DQN. Despite
regularization being largely underutilized in deep reinforcement learning, we
show that it can, in fact, help DQN learn more general features. These features
can be reused and fine-tuned on similar tasks, considerably improving DQN's
sample efficiency.","Jesse Farebrother, Marlos C. Machado, Michael Bowling",2018,http://arxiv.org/abs/1810.00123v3
Rating-based Reinforcement Learning,"This paper develops a novel rating-based reinforcement learning approach that
uses human ratings to obtain human guidance in reinforcement learning.
Different from the existing preference-based and ranking-based reinforcement
learning paradigms, based on human relative preferences over sample pairs, the
proposed rating-based reinforcement learning approach is based on human
evaluation of individual trajectories without relative comparisons between
sample pairs. The rating-based reinforcement learning approach builds on a new
prediction model for human ratings and a novel multi-class loss function. We
conduct several experimental studies based on synthetic ratings and real human
ratings to evaluate the effectiveness and benefits of the new rating-based
reinforcement learning approach.","Devin White, Mingkang Wu, Ellen Novoseller, Vernon J. Lawhern, Nicholas Waytowich, Yongcan Cao",2023,http://arxiv.org/abs/2307.16348v2
Placement Optimization with Deep Reinforcement Learning,"Placement Optimization is an important problem in systems and chip design,
which consists of mapping the nodes of a graph onto a limited set of resources
to optimize for an objective, subject to constraints. In this paper, we start
by motivating reinforcement learning as a solution to the placement problem. We
then give an overview of what deep reinforcement learning is. We next formulate
the placement problem as a reinforcement learning problem and show how this
problem can be solved with policy gradient optimization. Finally, we describe
lessons we have learned from training deep reinforcement learning policies
across a variety of placement optimization problems.","Anna Goldie, Azalia Mirhoseini",2020,http://arxiv.org/abs/2003.08445v1
Equivariant Reinforcement Learning for Quadrotor UAV,"This paper presents an equivariant reinforcement learning framework for
quadrotor unmanned aerial vehicles. Successful training of reinforcement
learning often requires numerous interactions with the environments, which
hinders its applicability especially when the available computational resources
are limited, or when there is no reliable simulation model. We identified an
equivariance property of the quadrotor dynamics such that the dimension of the
state required in the training is reduced by one, thereby improving the
sampling efficiency of reinforcement learning substantially. This is
illustrated by numerical examples with popular reinforcement learning
techniques of TD3 and SAC.","Beomyeol Yu, Taeyoung Lee",2022,http://arxiv.org/abs/2206.01233v2
Explaining Reinforcement Learning with Shapley Values,"For reinforcement learning systems to be widely adopted, their users must
understand and trust them. We present a theoretical analysis of explaining
reinforcement learning using Shapley values, following a principled approach
from game theory for identifying the contribution of individual players to the
outcome of a cooperative game. We call this general framework Shapley Values
for Explaining Reinforcement Learning (SVERL). Our analysis exposes the
limitations of earlier uses of Shapley values in reinforcement learning. We
then develop an approach that uses Shapley values to explain agent performance.
In a variety of domains, SVERL produces meaningful explanations that match and
supplement human intuition.","Daniel Beechey, Thomas M. S. Smith, Özgür Şimşek",2023,http://arxiv.org/abs/2306.05810v1
Diverse Policies Converge in Reward-free Markov Decision Processe,"Reinforcement learning has achieved great success in many decision-making
tasks, and traditional reinforcement learning algorithms are mainly designed
for obtaining a single optimal solution. However, recent works show the
importance of developing diverse policies, which makes it an emerging research
topic. Despite the variety of diversity reinforcement learning algorithms that
have emerged, none of them theoretically answer the question of how the
algorithm converges and how efficient the algorithm is. In this paper, we
provide a unified diversity reinforcement learning framework and investigate
the convergence of training diverse policies. Under such a framework, we also
propose a provably efficient diversity reinforcement learning algorithm.
Finally, we verify the effectiveness of our method through numerical
experiments.","Fanqi Lin, Shiyu Huang, Weiwei Tu",2023,http://arxiv.org/abs/2308.11924v1
"PGN: A perturbation generation network against deep reinforcement
  learning","Deep reinforcement learning has advanced greatly and applied in many areas.
In this paper, we explore the vulnerability of deep reinforcement learning by
proposing a novel generative model for creating effective adversarial examples
to attack the agent. Our proposed model can achieve both targeted attacks and
untargeted attacks. Considering the specificity of deep reinforcement learning,
we propose the action consistency ratio as a measure of stealthiness, and a new
measurement index of effectiveness and stealthiness. Experiment results show
that our method can ensure the effectiveness and stealthiness of attack
compared with other algorithms. Moreover, our methods are considerably faster
and thus can achieve rapid and efficient verification of the vulnerability of
deep reinforcement learning.","Xiangjuan Li, Feifan Li, Yang Li, Quan Pan",2023,http://arxiv.org/abs/2312.12904v1
Modern Deep Reinforcement Learning Algorithms,"Recent advances in Reinforcement Learning, grounded on combining classical
theoretical results with Deep Learning paradigm, led to breakthroughs in many
artificial intelligence tasks and gave birth to Deep Reinforcement Learning
(DRL) as a field of research. In this work latest DRL algorithms are reviewed
with a focus on their theoretical justification, practical limitations and
observed empirical properties.","Sergey Ivanov, Alexander D'yakonov",2019,http://arxiv.org/abs/1906.10025v2
"Applications of Deep Reinforcement Learning in Communications and
  Networking: A Survey","This paper presents a comprehensive literature review on applications of deep
reinforcement learning in communications and networking. Modern networks, e.g.,
Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become
more decentralized and autonomous. In such networks, network entities need to
make decisions locally to maximize the network performance under uncertainty of
network environment. Reinforcement learning has been efficiently used to enable
the network entities to obtain the optimal policy including, e.g., decisions or
actions, given their states when the state and action spaces are small.
However, in complex and large-scale networks, the state and action spaces are
usually large, and the reinforcement learning may not be able to find the
optimal policy in reasonable time. Therefore, deep reinforcement learning, a
combination of reinforcement learning with deep learning, has been developed to
overcome the shortcomings. In this survey, we first give a tutorial of deep
reinforcement learning from fundamental concepts to advanced models. Then, we
review deep reinforcement learning approaches proposed to address emerging
issues in communications and networking. The issues include dynamic network
access, data rate control, wireless caching, data offloading, network security,
and connectivity preservation which are all important to next generation
networks such as 5G and beyond. Furthermore, we present applications of deep
reinforcement learning for traffic routing, resource sharing, and data
collection. Finally, we highlight important challenges, open issues, and future
research directions of applying deep reinforcement learning.","Nguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang, Ying-Chang Liang, Dong In Kim",2018,http://arxiv.org/abs/1810.07862v1
Data Valuation for Offline Reinforcement Learning,"The success of deep reinforcement learning (DRL) hinges on the availability
of training data, which is typically obtained via a large number of environment
interactions. In many real-world scenarios, costs and risks are associated with
gathering these data. The field of offline reinforcement learning addresses
these issues through outsourcing the collection of data to a domain expert or a
carefully monitored program and subsequently searching for a batch-constrained
optimal policy. With the emergence of data markets, an alternative to
constructing a dataset in-house is to purchase external data. However, while
state-of-the-art offline reinforcement learning approaches have shown a lot of
promise, they currently rely on carefully constructed datasets that are well
aligned with the intended target domains. This raises questions regarding the
transferability and robustness of an offline reinforcement learning agent
trained on externally acquired data. In this paper, we empirically evaluate the
ability of the current state-of-the-art offline reinforcement learning
approaches to coping with the source-target domain mismatch within two MuJoCo
environments, finding that current state-of-the-art offline reinforcement
learning algorithms underperform in the target domain. To address this, we
propose data valuation for offline reinforcement learning (DVORL), which allows
us to identify relevant and high-quality transitions, improving the performance
and transferability of policies learned by offline reinforcement learning
algorithms. The results show that our method outperforms offline reinforcement
learning baselines on two MuJoCo environments.","Amir Abolfazli, Gregory Palmer, Daniel Kudenko",2022,http://arxiv.org/abs/2205.09550v1
"A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a
  Platform","With the development of industry, drones are appearing in various field. In
recent years, deep reinforcement learning has made impressive gains in games,
and we are committed to applying deep reinforcement learning algorithms to the
field of robotics, moving reinforcement learning algorithms from game scenarios
to real-world application scenarios. We are inspired by the LunarLander of
OpenAI Gym, we decided to make a bold attempt in the field of reinforcement
learning to control drones. At present, there is still a lack of work applying
reinforcement learning algorithms to robot control, the physical simulation
platform related to robot control is only suitable for the verification of
classical algorithms, and is not suitable for accessing reinforcement learning
algorithms for the training. In this paper, we will face this problem, bridging
the gap between physical simulation platforms and intelligent agent, connecting
intelligent agents to a physical simulation platform, allowing agents to learn
and complete drone flight tasks in a simulator that approximates the real
world. We proposed a reinforcement learning framework based on Gazebo that is a
kind of physical simulation platform (ROS-RL), and used three continuous action
space reinforcement learning algorithms in the framework to dealing with the
problem of autonomous landing of drones. Experiments show the effectiveness of
the algorithm, the task of autonomous landing of drones based on reinforcement
learning achieved full success.","Z. Jiang, G. Song",2022,http://arxiv.org/abs/2209.02954v1
"Tackling Error Propagation through Reinforcement Learning: A Case of
  Greedy Dependency Parsing","Error propagation is a common problem in NLP. Reinforcement learning explores
erroneous states during training and can therefore be more robust when mistakes
are made early in a process. In this paper, we apply reinforcement learning to
greedy dependency parsing which is known to suffer from error propagation.
Reinforcement learning improves accuracy of both labeled and unlabeled
dependencies of the Stanford Neural Dependency Parser, a high performance
greedy parser, while maintaining its efficiency. We investigate the portion of
errors which are the result of error propagation and confirm that reinforcement
learning reduces the occurrence of error propagation.","Minh Le, Antske Fokkens",2017,http://arxiv.org/abs/1702.06794v1
"Active Reinforcement Learning -- A Roadmap Towards Curious Classifier
  Systems for Self-Adaptation","Intelligent systems have the ability to improve their behaviour over time
taking observations, experiences or explicit feedback into account. Traditional
approaches separate the learning problem and make isolated use of techniques
from different field of machine learning such as reinforcement learning, active
learning, anomaly detection or transfer learning, for instance. In this
context, the fundamental reinforcement learning approaches come with several
drawbacks that hinder their application to real-world systems: trial-and-error,
purely reactive behaviour or isolated problem handling. The idea of this
article is to present a concept for alleviating these drawbacks by setting up a
research agenda towards what we call ""active reinforcement learning"" in
intelligent systems.","Simon Reichhuber, Sven Tomforde",2022,http://arxiv.org/abs/2201.03947v1
Exploration by Distributional Reinforcement Learning,"We propose a framework based on distributional reinforcement learning and
recent attempts to combine Bayesian parameter updates with deep reinforcement
learning. We show that our proposed framework conceptually unifies multiple
previous methods in exploration. We also derive a practical algorithm that
achieves efficient exploration on challenging control tasks.","Yunhao Tang, Shipra Agrawal",2018,http://arxiv.org/abs/1805.01907v2
Implicit Policy for Reinforcement Learning,"We introduce Implicit Policy, a general class of expressive policies that can
flexibly represent complex action distributions in reinforcement learning, with
efficient algorithms to compute entropy regularized policy gradients. We
empirically show that, despite its simplicity in implementation, entropy
regularization combined with a rich policy class can attain desirable
properties displayed under maximum entropy reinforcement learning framework,
such as robustness and multi-modality.","Yunhao Tang, Shipra Agrawal",2018,http://arxiv.org/abs/1806.06798v2
Derivative-Free Reinforcement Learning: A Review,"Reinforcement learning is about learning agent models that make the best
sequential decisions in unknown environments. In an unknown environment, the
agent needs to explore the environment while exploiting the collected
information, which usually forms a sophisticated problem to solve.
Derivative-free optimization, meanwhile, is capable of solving sophisticated
problems. It commonly uses a sampling-and-updating framework to iteratively
improve the solution, where exploration and exploitation are also needed to be
well balanced. Therefore, derivative-free optimization deals with a similar
core issue as reinforcement learning, and has been introduced in reinforcement
learning approaches, under the names of learning classifier systems and
neuroevolution/evolutionary reinforcement learning. Although such methods have
been developed for decades, recently, derivative-free reinforcement learning
exhibits attracting increasing attention. However, recent survey on this topic
is still lacking. In this article, we summarize methods of derivative-free
reinforcement learning to date, and organize the methods in aspects including
parameter updating, model selection, exploration, and parallel/distributed
methods. Moreover, we discuss some current limitations and possible future
directions, hoping that this article could bring more attentions to this topic
and serve as a catalyst for developing novel and efficient approaches.","Hong Qian, Yang Yu",2021,http://arxiv.org/abs/2102.05710v1
Multi-Task Federated Reinforcement Learning with Adversaries,"Reinforcement learning algorithms, just like any other Machine learning
algorithm pose a serious threat from adversaries. The adversaries can
manipulate the learning algorithm resulting in non-optimal policies. In this
paper, we analyze the Multi-task Federated Reinforcement Learning algorithms,
where multiple collaborative agents in various environments are trying to
maximize the sum of discounted return, in the presence of adversarial agents.
We argue that the common attack methods are not guaranteed to carry out a
successful attack on Multi-task Federated Reinforcement Learning and propose an
adaptive attack method with better attack performance. Furthermore, we modify
the conventional federated reinforcement learning algorithm to address the
issue of adversaries that works equally well with and without the adversaries.
Experimentation on different small to mid-size reinforcement learning problems
show that the proposed attack method outperforms other general attack methods
and the proposed modification to federated reinforcement learning algorithm was
able to achieve near-optimal policies in the presence of adversarial agents.","Aqeel Anwar, Arijit Raychowdhury",2021,http://arxiv.org/abs/2103.06473v1
"rSoccer: A Framework for Studying Reinforcement Learning in Small and
  Very Small Size Robot Soccer","Reinforcement learning is an active research area with a vast number of
applications in robotics, and the RoboCup competition is an interesting
environment for studying and evaluating reinforcement learning methods. A known
difficulty in applying reinforcement learning to robotics is the high number of
experience samples required, being the use of simulated environments for
training the agents followed by transfer learning to real-world (sim-to-real) a
viable path. This article introduces an open-source simulator for the IEEE Very
Small Size Soccer and the Small Size League optimized for reinforcement
learning experiments. We also propose a framework for creating OpenAI Gym
environments with a set of benchmarks tasks for evaluating single-agent and
multi-agent robot soccer skills. We then demonstrate the learning capabilities
of two state-of-the-art reinforcement learning methods as well as their
limitations in certain scenarios introduced in this framework. We believe this
will make it easier for more teams to compete in these categories using
end-to-end reinforcement learning approaches and further develop this research
area.","Felipe B. Martins, Mateus G. Machado, Hansenclever F. Bassani, Pedro H. M. Braga, Edna S. Barros",2021,http://arxiv.org/abs/2106.12895v1
"Tracking the Race Between Deep Reinforcement Learning and Imitation
  Learning -- Extended Version","Learning-based approaches for solving large sequential decision making
problems have become popular in recent years. The resulting agents perform
differently and their characteristics depend on those of the underlying
learning approach. Here, we consider a benchmark planning problem from the
reinforcement learning domain, the Racetrack, to investigate the properties of
agents derived from different deep (reinforcement) learning approaches. We
compare the performance of deep supervised learning, in particular imitation
learning, to reinforcement learning for the Racetrack model. We find that
imitation learning yields agents that follow more risky paths. In contrast, the
decisions of deep reinforcement learning are more foresighted, i.e., avoid
states in which fatal decisions are more likely. Our evaluations show that for
this sequential decision making problem, deep reinforcement learning performs
best in many aspects even though for imitation learning optimal decisions are
considered.","Timo P. Gros, Daniel Höller, Jörg Hoffmann, Verena Wolf",2020,http://arxiv.org/abs/2008.00766v1
Reinforcement Learning and Video Games,"Reinforcement learning has exceeded human-level performance in game playing
AI with deep learning methods according to the experiments from DeepMind on Go
and Atari games. Deep learning solves high dimension input problems which stop
the development of reinforcement for many years. This study uses both two
techniques to create several agents with different algorithms that successfully
learn to play T-rex Runner. Deep Q network algorithm and three types of
improvements are implemented to train the agent. The results from some of them
are far from satisfactory but others are better than human experts. Batch
normalization is a method to solve internal covariate shift problems in deep
neural network. The positive influence of this on reinforcement learning has
also been proved in this study.",Yue Zheng,2019,http://arxiv.org/abs/1909.04751v1
Distributional Reinforcement Learning with Ensembles,"It is well known that ensemble methods often provide enhanced performance in
reinforcement learning. In this paper, we explore this concept further by using
group-aided training within the distributional reinforcement learning paradigm.
Specifically, we propose an extension to categorical reinforcement learning,
where distributional learning targets are implicitly based on the total
information gathered by an ensemble. We empirically show that this may lead to
much more robust initial learning, a stronger individual performance level, and
good efficiency on a per-sample basis.","Björn Lindenberg, Jonas Nordqvist, Karl-Olof Lindahl",2020,http://arxiv.org/abs/2003.10903v2
"Poisoning Deep Reinforcement Learning Agents with In-Distribution
  Triggers","In this paper, we propose a new data poisoning attack and apply it to deep
reinforcement learning agents. Our attack centers on what we call
in-distribution triggers, which are triggers native to the data distributions
the model will be trained on and deployed in. We outline a simple procedure for
embedding these, and other, triggers in deep reinforcement learning agents
following a multi-task learning paradigm, and demonstrate in three common
reinforcement learning environments. We believe that this work has important
implications for the security of deep learning models.","Chace Ashcraft, Kiran Karra",2021,http://arxiv.org/abs/2106.07798v1
A Survey of Exploration Methods in Reinforcement Learning,"Exploration is an essential component of reinforcement learning algorithms,
where agents need to learn how to predict and control unknown and often
stochastic environments. Reinforcement learning agents depend crucially on
exploration to obtain informative data for the learning process as the lack of
enough information could hinder effective learning. In this article, we provide
a survey of modern exploration methods in (Sequential) reinforcement learning,
as well as a taxonomy of exploration methods.","Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, Doina Precup",2021,http://arxiv.org/abs/2109.00157v2
